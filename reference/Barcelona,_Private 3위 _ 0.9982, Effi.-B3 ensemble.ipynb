{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 작물 병해 분류 AI 경진대회"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- Training dataset이 250장으로 매우 적었습니다.\n",
    "- Train:200, Val:50 장으로 학습시켜서 5Fold로 제출해도 0.991이라는 Public Score가 나왔으나 조금 더 안정적인 학습을 위해 Pseudo Labeling 방법을 사용하였습니다.\n",
    "- RegNetY-064를 기반으로 Pseudo Labeling을 반복적으로 수행하였고,\n",
    "- RegNetY-064를 통해 Pseudo label set으로 학습한 5Fold 앙상블 (RegNetY-064, EfficientNet-B3) 2개 모델 제출했습니다.\n",
    "- Private에서 최종적으로 선택되어진 모델은 5Fold EfficientNet-B3 앙상블 모델입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 배운점\n",
    "- 평가 Measure가 F1-Score였는데 Class의 Imbalance에는 크게 신경을 쓰지 않았습니다.\n",
    "- 1등, 2등 하신 분들의 Solution을 보면 Pseudo labeling 기법은 동일하게 사용하였지만 Class별 Sampling or Loss 가중치를 다르게 준 것을 확인했습니다.\n",
    "- 그래서, Public에서 1등이었으나 Private에서는 아쉽게 3등이 되지 않았나 생각합니다.\n",
    "- F1-Score가 Metric일 경우 Class Imbalance를 고려하여 모델링을 해야겠다는 생각을 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0-1. Requirements\n",
    "Ubuntu 18.04, Cuda 11.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- opencv-python  \n",
    "- numpy  \n",
    "- pandas\n",
    "- timm\n",
    "- torch==1.8.0 torchvision 0.9.0 with cuda 11.1\n",
    "- natsort\n",
    "- scikit-learn\n",
    "- pillow\n",
    "- torch_optimizer\n",
    "- tqdm\n",
    "- easydict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0-2 Directory 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- data\n",
    "    - train.csv\n",
    "    - test.csv\n",
    "    - sample_submission.csv\n",
    "    - train_imgs\n",
    "        - 10000.jpg\n",
    "        - 10001.jpg\n",
    "        - ...\n",
    "    - test_imgs\n",
    "        - 20000.jpg\n",
    "        - 20001.jpg\n",
    "        - ...\n",
    "- notebook\n",
    "    - 3rd place solution.ipynb\n",
    "    - results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0-3. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import easydict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from os.path import join as opj\n",
    "from ptflops import get_model_complexity_info\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from PIL import Image\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_optimizer as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, grad_scaler\n",
    "from torchvision import transforms\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0-4. Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 및 학습의 Hyper-parameter를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict(\n",
    "    {'exp_num':'0',\n",
    "     'experiment':'Base',\n",
    "     'tag':'Default',\n",
    "\n",
    "     # Path settings\n",
    "     'data_path':'../data',\n",
    "     'fold':0,\n",
    "     'Kfold':5,\n",
    "     'model_path':'results/',\n",
    "\n",
    "     # Model parameter settings\n",
    "     'encoder_name':'regnety_064',\n",
    "     'drop_path_rate':0.2,\n",
    "     \n",
    "     # Training parameter settings\n",
    "     ## Base Parameter\n",
    "     'img_size':352,\n",
    "     'batch_size':16,\n",
    "     'epochs':60,\n",
    "     'optimizer':'Lamb',\n",
    "     'initial_lr':5e-6,\n",
    "     'weight_decay':1e-3,\n",
    "\n",
    "     ## Augmentation\n",
    "     'aug_ver':2,\n",
    "\n",
    "     ## Scheduler\n",
    "     'scheduler':'cycle',\n",
    "     'warm_epoch':5,\n",
    "     ### OnecycleLR\n",
    "     'max_lr':1e-3,\n",
    "\n",
    "     ## etc.\n",
    "     'patience':15,\n",
    "     'clipping':None,\n",
    "\n",
    "     # Hardware settings\n",
    "     'amp':True,\n",
    "     'multi_gpu':False,\n",
    "     'logging':False,\n",
    "     'num_workers':4,\n",
    "     'seed':42\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Make dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>img_path</th>\n",
       "      <th>disease</th>\n",
       "      <th>disease_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>train_imgs/10000.jpg</td>\n",
       "      <td>시설포도노균병</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>train_imgs/10001.jpg</td>\n",
       "      <td>시설포도노균병</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10002</td>\n",
       "      <td>train_imgs/10002.jpg</td>\n",
       "      <td>시설포도노균병반응</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10003</td>\n",
       "      <td>train_imgs/10003.jpg</td>\n",
       "      <td>축과병</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10004</td>\n",
       "      <td>train_imgs/10004.jpg</td>\n",
       "      <td>시설포도노균병</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>10245</td>\n",
       "      <td>train_imgs/10245.jpg</td>\n",
       "      <td>시설포도노균병반응</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>10246</td>\n",
       "      <td>train_imgs/10246.jpg</td>\n",
       "      <td>시설포도탄저병반응</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>10247</td>\n",
       "      <td>train_imgs/10247.jpg</td>\n",
       "      <td>시설포도노균병</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>10248</td>\n",
       "      <td>train_imgs/10248.jpg</td>\n",
       "      <td>시설포도노균병반응</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>10249</td>\n",
       "      <td>train_imgs/10249.jpg</td>\n",
       "      <td>시설포도노균병반응</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       uid              img_path    disease  disease_code\n",
       "0    10000  train_imgs/10000.jpg    시설포도노균병             1\n",
       "1    10001  train_imgs/10001.jpg    시설포도노균병             1\n",
       "2    10002  train_imgs/10002.jpg  시설포도노균병반응             2\n",
       "3    10003  train_imgs/10003.jpg        축과병             4\n",
       "4    10004  train_imgs/10004.jpg    시설포도노균병             1\n",
       "..     ...                   ...        ...           ...\n",
       "245  10245  train_imgs/10245.jpg  시설포도노균병반응             2\n",
       "246  10246  train_imgs/10246.jpg  시설포도탄저병반응             6\n",
       "247  10247  train_imgs/10247.jpg    시설포도노균병             1\n",
       "248  10248  train_imgs/10248.jpg  시설포도노균병반응             2\n",
       "249  10249  train_imgs/10249.jpg  시설포도노균병반응             2\n",
       "\n",
       "[250 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/train.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 이미지의 Resolution이 커서 Load시 병목으로 인하여 적당한 사이즈로 Resize하여 저장\n",
    "# Pseudo Labeling을 위해 Train/Test Image를 모두 한폴더에 미리 저장 (불필요한 저장용량 증가 방지)\n",
    "\n",
    "path = '../data/total_imgs_1024'\n",
    "\n",
    "# Make Train\n",
    "df = pd.read_csv('../data/train.csv')\n",
    "os.makedirs(path, exist_ok=True)\n",
    "for img in df['img_path']:\n",
    "    name = os.path.basename(img)\n",
    "    img = cv2.imread(opj('../data/', img))\n",
    "    img = cv2.resize(img, dsize=(1024, 1024))\n",
    "    img = cv2.imwrite(opj(path, name), img)\n",
    "\n",
    "# Make Test\n",
    "df = pd.read_csv('../data/test.csv')\n",
    "for img in df['img_path']:\n",
    "    name = os.path.basename(img)\n",
    "    img = cv2.imread(opj('../data/', img))\n",
    "    img = cv2.resize(img, dsize=(1024, 1024))\n",
    "    img = cv2.imwrite(opj(path, name), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Dataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.img_path = df['img_path'].values\n",
    "        self.target = df['disease_code'].values\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f'Dataset size:{len(self.img_path)}')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(opj('../data/', self.img_path[idx])).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        target = self.target[idx]\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "class Test_dataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.img_path = df['img_path'].values\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f'Test Dataset size:{len(self.img_path)}')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(opj('../data/', self.img_path[idx])).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "def get_loader(df, phase: str, batch_size, shuffle,\n",
    "               num_workers, transform):\n",
    "    if phase == 'test':\n",
    "        dataset = Test_dataset(df, transform)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True)\n",
    "    else:\n",
    "        dataset = Train_Dataset(df, transform)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True,\n",
    "                                 drop_last=False)\n",
    "    return data_loader\n",
    "\n",
    "def get_train_augmentation(img_size, ver):\n",
    "    if ver==1: # for valid, test\n",
    "        transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "                ])\n",
    "\n",
    "    if ver==2:\n",
    "        transform = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomAffine((-20, 20)),\n",
    "                transforms.RandomRotation(90),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "            \n",
    "    return transform                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network\n",
    "pytorch image models(timm) 라이브러리를 활용하여 Generalization Performance에 강점을 가지는 RegNet을 Base 모델로 사용하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.encoder = timm.create_model(args.encoder_name, pretrained=True, drop_path_rate=args.drop_path_rate)\n",
    "\n",
    "        if 'regnet' in args.encoder_name:\n",
    "            num_head = self.encoder.head.fc.in_features\n",
    "            self.encoder.head.fc = nn.Linear(num_head, 7)\n",
    "\n",
    "        elif 'efficient' in args.encoder_name:\n",
    "            num_head = self.encoder.classifier.in_features\n",
    "            self.encoder.classifier = nn.Linear(num_head, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "class Network_test(nn.Module):   # Without drop_path_rate\n",
    "    def __init__(self, encoder_name):\n",
    "        super().__init__()\n",
    "        self.encoder = timm.create_model(encoder_name, pretrained=False)\n",
    "\n",
    "        if 'regnet' in encoder_name:\n",
    "            num_head = self.encoder.head.fc.in_features\n",
    "            self.encoder.head.fc = nn.Linear(num_head, 7)\n",
    "\n",
    "        elif 'efficient' in encoder_name:\n",
    "            num_head = self.encoder.classifier.in_features\n",
    "            self.encoder.classifier = nn.Linear(num_head, 7)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils for training and Logging\n",
    "Logging과 AvgMeter를 통해 실험 기록을 log파일로 남도록 저장하였습니다.  \n",
    "추가로 실험마다 비교를 쉽게 하기위해 Neptune을 활용하였는데 코드에서는 제거하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup Learning rate scheduler\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "class WarmUpLR(_LRScheduler):\n",
    "    \"\"\"warmup_training learning rate scheduler\n",
    "    Args:\n",
    "        optimizer: optimzier(e.g. SGD)\n",
    "        total_iters: totoal_iters of warmup phase\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, total_iters, last_epoch=-1):\n",
    "        \n",
    "        self.total_iters = total_iters\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"we will use the first m batches, and set the learning\n",
    "        rate to base_lr * m / total_iters\n",
    "        \"\"\"\n",
    "        return [base_lr * self.last_epoch / (self.total_iters + 1e-8) for base_lr in self.base_lrs]\n",
    "\n",
    "# Logging\n",
    "def get_root_logger(logger_name='basicsr',\n",
    "                    log_level=logging.INFO,\n",
    "                    log_file=None):\n",
    "\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    # if the logger has been initialized, just return it\n",
    "    if logger.hasHandlers():\n",
    "        return logger\n",
    "\n",
    "    format_str = '%(asctime)s %(levelname)s: %(message)s'\n",
    "    logging.basicConfig(format=format_str, level=log_level)\n",
    "\n",
    "    if log_file is not None:\n",
    "        file_handler = logging.FileHandler(log_file, 'w')\n",
    "        file_handler.setFormatter(logging.Formatter(format_str))\n",
    "        file_handler.setLevel(log_level)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "class AvgMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.losses = []\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        self.losses.append(val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer\n",
    "모델의 학습(training function)과 검증(Validation)을 위한 Class입니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, args, save_path):\n",
    "        '''\n",
    "        args: arguments\n",
    "        save_path: Model 가중치 저장 경로\n",
    "        '''\n",
    "        super(Trainer, self).__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Logging\n",
    "        log_file = os.path.join(save_path, 'log.log')\n",
    "        self.logger = get_root_logger(logger_name='IR', log_level=logging.INFO, log_file=log_file)\n",
    "        self.logger.info(args)\n",
    "        self.logger.info(args.tag)\n",
    "\n",
    "        # Train, Valid Set load\n",
    "        ############################################################################\n",
    "\n",
    "        original_train_length = 250\n",
    "        if args.phase == 0 :\n",
    "            df_train = pd.read_csv(opj(args.data_path, 'train.csv'))\n",
    "        else :\n",
    "            df_train = pd.read_csv(opj(args.data_path, f'train_pseudo{(args.phase)-1}.csv'))\n",
    "\n",
    "        df_train['fold'] = -1\n",
    "        kf = StratifiedKFold(n_splits=args.Kfold, shuffle=True, random_state=args.seed)\n",
    "\n",
    "        # 기존 Train set 250장을 Train 200장과 Valid 50장으로 나누고, 추가적인 Pseudo set이 들어온 경우 Train 200장 + pseudo set 개수가 되도록 코드 작성\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(range(original_train_length), y=df_train['disease_code'].values[:original_train_length])):\n",
    "            df_train.loc[val_idx, 'fold'] = fold\n",
    "        val_idx = list(df_train[df_train['fold'] == int(args.fold)].index)\n",
    "\n",
    "        df_val = df_train[df_train['fold'] == args.fold].reset_index(drop=True)\n",
    "        df_train = df_train[df_train['fold'] != args.fold].reset_index(drop=True)\n",
    "\n",
    "        # Augmentation\n",
    "        self.train_transform = get_train_augmentation(img_size=args.img_size, ver=args.aug_ver)\n",
    "        self.test_transform = get_train_augmentation(img_size=args.img_size, ver=1)\n",
    "\n",
    "        # TrainLoader\n",
    "        self.train_loader = get_loader(df_train, phase='train', batch_size=args.batch_size, shuffle=True,\n",
    "                                       num_workers=args.num_workers, transform=self.train_transform)\n",
    "        self.val_loader = get_loader(df_val, phase='train', batch_size=args.batch_size, shuffle=False,\n",
    "                                       num_workers=args.num_workers, transform=self.test_transform)\n",
    "\n",
    "        # Network\n",
    "        self.model = Network(args).to(self.device)\n",
    "        macs, params = get_model_complexity_info(self.model, (3, args.img_size, args.img_size), as_strings=True,\n",
    "                                                 print_per_layer_stat=False, verbose=False)\n",
    "        self.logger.info('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "        self.logger.info('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
    "\n",
    "        # Loss\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        # self.criterion = LabelSmoothingLoss(classes=7, smoothing=0.1)\n",
    "        \n",
    "        # Optimizer & Scheduler\n",
    "        self.optimizer = optim.Lamb(self.model.parameters(), lr=args.initial_lr, weight_decay=args.weight_decay)\n",
    "        \n",
    "        iter_per_epoch = len(self.train_loader)\n",
    "        self.warmup_scheduler = WarmUpLR(self.optimizer, iter_per_epoch * args.warm_epoch)\n",
    "        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, max_lr=args.max_lr, steps_per_epoch=iter_per_epoch, epochs=args.epochs)\n",
    "\n",
    "        if args.multi_gpu:\n",
    "            self.model = nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "        # Train / Validate\n",
    "        best_loss = np.inf\n",
    "        best_epoch = 0\n",
    "        early_stopping = 0\n",
    "        start = time.time()\n",
    "        for epoch in range(1, args.epochs+1):\n",
    "            self.epoch = epoch\n",
    "\n",
    "            if args.scheduler == 'cos':\n",
    "                if epoch > args.warm_epoch:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            # Training\n",
    "            train_loss, train_acc, train_f1 = self.training(args)\n",
    "\n",
    "            # Validation\n",
    "            val_loss, val_acc, val_f1 = self.validate()\n",
    "\n",
    "            # Save models\n",
    "            if val_loss < best_loss:\n",
    "                # Model weight in Multi_GPU or Single GPU\n",
    "                state_dict= self.model.module.state_dict() if args.multi_gpu else self.model.state_dict()\n",
    "\n",
    "                early_stopping = 0\n",
    "                best_epoch = epoch\n",
    "                best_loss = val_loss\n",
    "                best_acc = val_acc\n",
    "                best_f1 = val_f1\n",
    "\n",
    "                torch.save({'epoch':epoch,\n",
    "                            'state_dict':state_dict,\n",
    "                            'optimizer': self.optimizer.state_dict(),\n",
    "                            'scheduler': self.scheduler.state_dict(),\n",
    "                    }, os.path.join(save_path, 'best_model.pth'))\n",
    "                self.logger.info(f'-----------------SAVE:{best_epoch}epoch----------------')\n",
    "            else:\n",
    "                early_stopping += 1\n",
    "\n",
    "            # Early Stopping\n",
    "            if early_stopping == args.patience:\n",
    "                break\n",
    "\n",
    "        self.logger.info(f'\\nBest Val Epoch:{best_epoch} | Val Loss:{best_loss:.4f} | Val Acc:{best_acc:.4f} | Val F1:{best_f1:.4f}')\n",
    "        end = time.time()\n",
    "        self.logger.info(f'Total Process time:{(end - start) / 60:.3f}Minute')\n",
    "\n",
    "    # Training\n",
    "    def training(self, args):\n",
    "        self.model.train()\n",
    "        train_loss = AvgMeter()\n",
    "        train_acc = 0\n",
    "        preds_list = []\n",
    "        targets_list = []\n",
    "\n",
    "        scaler = grad_scaler.GradScaler()\n",
    "        for i, (images, targets) in enumerate(tqdm(self.train_loader)):\n",
    "            images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
    "            targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
    "            \n",
    "            if self.epoch <= args.warm_epoch:\n",
    "                self.warmup_scheduler.step()\n",
    "\n",
    "            self.model.zero_grad(set_to_none=True)\n",
    "            if args.amp:\n",
    "                with autocast():\n",
    "                    preds = self.model(images)\n",
    "                    loss = self.criterion(preds, targets)\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                # Gradient Clipping\n",
    "                if args.clipping is not None:\n",
    "                    scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
    "\n",
    "                scaler.step(self.optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            else:\n",
    "                preds = self.model(images)\n",
    "                loss = self.criterion(preds, targets)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
    "                self.optimizer.step()\n",
    "\n",
    "            if args.scheduler == 'cycle':\n",
    "                if self.epoch > args.warm_epoch:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            # Metric\n",
    "            train_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
    "            preds_list.extend(preds.argmax(dim=1).cpu().detach().numpy())\n",
    "            targets_list.extend(targets.cpu().detach().numpy())\n",
    "            # log\n",
    "            train_loss.update(loss.item(), n=images.size(0))\n",
    "\n",
    "        train_acc /= len(self.train_loader.dataset)\n",
    "        train_f1 = f1_score(np.array(targets_list), np.array(preds_list), average='macro')\n",
    "\n",
    "        self.logger.info(f'Epoch:[{self.epoch:03d}/{args.epochs:03d}]')\n",
    "        self.logger.info(f'Train Loss:{train_loss.avg:.3f} | Acc:{train_acc:.4f} | F1:{train_f1:.4f}')\n",
    "        return train_loss.avg, train_acc, train_f1\n",
    "            \n",
    "    # Validation or Dev\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = AvgMeter()\n",
    "            val_acc = 0\n",
    "            preds_list = []\n",
    "            targets_list = []\n",
    "\n",
    "            for i, (images, targets) in enumerate(self.val_loader):\n",
    "                images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
    "                targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
    "\n",
    "                preds = self.model(images)\n",
    "                loss = self.criterion(preds, targets)\n",
    "\n",
    "                # Metric\n",
    "                val_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
    "                preds_list.extend(preds.argmax(dim=1).cpu().detach().numpy())\n",
    "                targets_list.extend(targets.cpu().detach().numpy())\n",
    "\n",
    "                # log\n",
    "                val_loss.update(loss.item(), n=images.size(0))\n",
    "            val_acc /= len(self.val_loader.dataset)\n",
    "            val_f1 = f1_score(np.array(targets_list), np.array(preds_list), average='macro')\n",
    "\n",
    "            self.logger.info(f'Valid Loss:{val_loss.avg:.3f} | Acc:{val_acc:.4f} | F1:{val_f1:.4f}')\n",
    "        return val_loss.avg, val_acc, val_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    # Random Seed\n",
    "    seed = args.seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    save_path = os.path.join(args.model_path, (args.exp_num).zfill(3))\n",
    "    # Create model directory\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    Trainer(args, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model을 받아서 예측 (ensemble 포함)\n",
    "def predict(model_list, test_loader, device):\n",
    "    preds_list = []\n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(test_loader):\n",
    "            images = torch.as_tensor(images, device=device, dtype=torch.float32)\n",
    "\n",
    "            ensemble = np.zeros((images.shape[0], 7), dtype=np.float32)\n",
    "            for model in model_list:\n",
    "                preds = model(images)\n",
    "                preds = torch.softmax(preds, dim=1)\n",
    "                preds = preds.detach().cpu().numpy()\n",
    "                ensemble += preds\n",
    "            preds = ensemble / len(model_list)\n",
    "            preds_list.extend(preds.tolist())\n",
    "    return np.array(preds_list)\n",
    "\n",
    "# 학습된 Weight을 Load하고 반환하는 함수\n",
    "def load_model(encoder_name, model_path):\n",
    "    model = Network_test(encoder_name).to(device)\n",
    "    model.load_state_dict(torch.load(opj('./results/', model_path, 'best_model.pth'))['state_dict'])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Pseudo label을 추가하여 학습하기 위한 DataFrame 생성\n",
    "def generate_df_pseudo(df_train, df_test, ensemble_pred, threshold, phase):\n",
    "    max_proba = ensemble_pred.max(axis=1)\n",
    "    pseudo_idx = np.where(max_proba > threshold)[0]\n",
    "    print('Add Number of images', len(pseudo_idx))\n",
    "\n",
    "    # Make pseudo dataframe\n",
    "    df_pseudo = df_train.drop(['uid', 'disease'], axis=1)\n",
    "    for i, img_path in enumerate(df_test['img_path']):\n",
    "        if i in pseudo_idx:\n",
    "            name = os.path.basename(img_path)\n",
    "            row = [opj('total_imgs_1024', name), ensemble_pred.argmax(axis=1)[i]]\n",
    "            df_pseudo = df_pseudo.append({'img_path':row[0],\n",
    "                                        'disease_code':row[1]\n",
    "                                        }, ignore_index=True)\n",
    "    df_pseudo.to_csv(f'../data/train_pseudo{phase}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Test dataset\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    submission = pd.read_csv('../data/sample_submission.csv')\n",
    "    df_train = pd.read_csv('../data/train.csv')\n",
    "    df_test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "    # Train, Test dataframe img_path 경로 바꿔주기\n",
    "    df_train['img_path'] = df_train['img_path'].apply(lambda x:x.replace('train_imgs', 'total_imgs_1024'))\n",
    "    df_test['img_path'] = df_test['img_path'].apply(lambda x:x.replace('test_imgs', 'total_imgs_1024'))\n",
    "\n",
    "    test_transform = get_train_augmentation(img_size=352, ver=1)\n",
    "    test_dataset = Test_dataset(df_test, test_transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "    encoder_list = ['regnety_064'] * 5\n",
    "    threshold = [0.95, 0.9, 0.9, 0.85, 0.85]   # Pseudo labeling을 할 때, threshold를 줄여가면서 test sample 사용 증가\n",
    "    for phase in range(0, 5):  # Pseudo labeling 방법 5번 반복\n",
    "        args.phase = phase\n",
    "        exp_num_list = []\n",
    "\n",
    "        if phase == 4:\n",
    "            args.encoder_name = 'efficientnet_b3'\n",
    "            encoder_list = ['efficientnet_b3']\n",
    "\n",
    "        for i in range(5):   # Train the 5Folds\n",
    "            args.fold = i\n",
    "            args.exp_num = str(i).zfill(3)\n",
    "            main(args)\n",
    "            exp_num_list.append(args.exp_num)\n",
    "\n",
    "        # 5Fold Ensemble Predict\n",
    "        model_list = [load_model(enc, path) for enc, path in zip(encoder_list, exp_num_list)]\n",
    "        ensemble_pred = predict(model_list, test_loader, device)\n",
    "        generate_df_pseudo(df_train, df_test, ensemble_pred, threshold[phase], phase)\n",
    "\n",
    "    submission.iloc[:, 1] = ensemble_pred.argmax(axis=1)\n",
    "    submission.to_csv('final_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65d15e24cdd096baf434faf42c81b7c04036b87ca24f1b4b7ae1f0e7508668de"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('torch18': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
