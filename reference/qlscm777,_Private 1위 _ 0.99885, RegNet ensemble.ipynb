{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "- RegNet040 5Fold ensemble\n",
    "- Pseudo labeling\n",
    "- 이전 대회에서 활용한 Framework을 기반으로 진행했습니다.\n",
    "\n",
    "## Total Process\n",
    "- Train Dastset이 많지 않기 때문에 Test Dataset에 대해 Pseudo labeling을 진행 후, Train Dataset에 추가하여 모델을 Training시켰습니다.\n",
    "- 총 6번의 Pseudo labeling을 진행 하였습니다.\n",
    "- 첫 Pseudo labeling은 최적화 과정을 거친 regnet 5fold ensemble(Public Score : 0.989)을 이용했습니다.\n",
    "- 이후 동일한 Model config에서 총 5번의 Pseudo label set update를 진행하였습니다.\n",
    "- 최종적으로 총 5번의 update가 된 Pseudo label set + Train Dataset에 대해 Training한 regnet 5fold ensemble을 제출 하였고 Private Score기준 1등을 달성했습니다.\n",
    "\n",
    "\n",
    "## Pseudo label set sampling\n",
    "- Pseudo label set의 신뢰성을 위해, 5fold ensembel logic에 대한 softmax결과값이 0.9보다 큰 sample로 한정했습니다.\n",
    "- Pseudo label set에서 비율이 많은 0번 class는 random sampling(n=500)했습니다.\n",
    "\n",
    "## Pseudo label set업데이트에 따른 Regnet 5fold ensemble Public Score\n",
    "- 0step(Only Trainset) : 0.989\n",
    "- 1step : 0.996\n",
    "- 4step : 0.998\n",
    "- 6step(Submission) : 0.999 / Private : 0.99885"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "Ubuntu 18.04, Cuda 11\n",
    "\n",
    "- opencv-python\n",
    "- numpy\n",
    "- pandas\n",
    "- timm\n",
    "- torch==1.8.0 torchvision 0.9.0 with cuda 11.1\n",
    "- natsort\n",
    "- scikit-learn\n",
    "- pillow\n",
    "- torch_optimizer\n",
    "- tqdm\n",
    "- ptflops\n",
    "- easydict\n",
    "- matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directory Structure\n",
    "\n",
    "- data\n",
    "    - train.csv\n",
    "    - test.csv\n",
    "    - sample_submission.csv\n",
    "    - train_imgs\n",
    "        - 10000.jpg\n",
    "        - ...\n",
    "        - 10249.jpg\n",
    "    - test_imgs\n",
    "        - 20000.jpg\n",
    "        - ...\n",
    "        - 24749.jpg\n",
    "- notebook\n",
    "    - 1st_place_solution.ipynb\n",
    "    - results\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import easydict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from os.path import join as opj\n",
    "from ptflops import get_model_complexity_info\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_optimizer as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, grad_scaler\n",
    "from torchvision import transforms\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict(\r\n",
    "    {'exp_num':'0',\r\n",
    "     \r\n",
    "     # Path settings\r\n",
    "     'data_path':'../data',\r\n",
    "     'Kfold':5,\r\n",
    "     'model_path':'results/',\r\n",
    "     'image_type':'train_1024', \r\n",
    "\r\n",
    "     # Model parameter settings\r\n",
    "     'encoder_name':'regnety_040',\r\n",
    "     'drop_path_rate':0.2,\r\n",
    "     \r\n",
    "     # Training parameter settings\r\n",
    "     ## Base Parameter\r\n",
    "     'img_size':288,\r\n",
    "     'batch_size':16,\r\n",
    "     'epochs':70,\r\n",
    "     'optimizer':'Lamb',\r\n",
    "     'initial_lr':5e-6,\r\n",
    "     'weight_decay':1e-3,\r\n",
    "\r\n",
    "     ## Augmentation\r\n",
    "     'aug_ver':2,\r\n",
    "\r\n",
    "     ## Scheduler (OnecycleLR)\r\n",
    "     'scheduler':'cycle',\r\n",
    "     'warm_epoch':5,\r\n",
    "     'max_lr':1e-3,\r\n",
    "\r\n",
    "     ### Cosine Annealing\r\n",
    "     'min_lr':5e-6,\r\n",
    "     'tmax':145,\r\n",
    "\r\n",
    "     ## etc.\r\n",
    "     'patience':15,\r\n",
    "     'clipping':None,\r\n",
    "\r\n",
    "     # Hardware settings\r\n",
    "     'amp':True,\r\n",
    "     'multi_gpu':False,\r\n",
    "     'logging':False,\r\n",
    "     'num_workers':4,\r\n",
    "     'seed':42\r\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils for training and Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup Learning rate scheduler\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "class WarmUpLR(_LRScheduler):\n",
    "    \"\"\"warmup_training learning rate scheduler\n",
    "    Args:\n",
    "        optimizer: optimzier(e.g. SGD)\n",
    "        total_iters: totoal_iters of warmup phase\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, total_iters, last_epoch=-1):\n",
    "        \n",
    "        self.total_iters = total_iters\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"we will use the first m batches, and set the learning\n",
    "        rate to base_lr * m / total_iters\n",
    "        \"\"\"\n",
    "        return [base_lr * self.last_epoch / (self.total_iters + 1e-8) for base_lr in self.base_lrs]\n",
    "\n",
    "# Logging\n",
    "def get_root_logger(logger_name='basicsr',\n",
    "                    log_level=logging.INFO,\n",
    "                    log_file=None):\n",
    "\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    # if the logger has been initialized, just return it\n",
    "    if logger.hasHandlers():\n",
    "        return logger\n",
    "\n",
    "    format_str = '%(asctime)s %(levelname)s: %(message)s'\n",
    "    logging.basicConfig(format=format_str, level=log_level)\n",
    "\n",
    "    if log_file is not None:\n",
    "        file_handler = logging.FileHandler(log_file, 'w')\n",
    "        file_handler.setFormatter(logging.Formatter(format_str))\n",
    "        file_handler.setLevel(log_level)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "class AvgMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.losses = []\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        self.losses.append(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "- 원본 이미지 사이즈가 매우 큰 점을 감안해 (1024,1024)로 resize하여 데이터를 새롭게 저장하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:14<00:00, 16.83it/s]\n",
      "100%|██████████| 4750/4750 [04:32<00:00, 17.46it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/train.csv')\n",
    "\n",
    "# Resize Train Images\n",
    "save_path = '../data/train_1024'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "for img in tqdm(df['img_path']):\n",
    "    name = os.path.basename(img)\n",
    "    img = cv2.imread(opj('../data/', img))\n",
    "    img = cv2.resize(img, dsize=(1024, 1024))\n",
    "    img = cv2.imwrite(opj(save_path, name), img)\n",
    "\n",
    "# Resize Test Images\n",
    "df = pd.read_csv('../data/test.csv')\n",
    "save_path = '../data/test_1024'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "for img in tqdm(df['img_path']):\n",
    "    name = os.path.basename(img)\n",
    "    img = cv2.imread(opj('../data/', img))\n",
    "    img = cv2.resize(img, dsize=(1024, 1024))\n",
    "    img = cv2.imwrite(opj(save_path, name), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset & Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Dataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.img_path = df['img_path'].values\n",
    "        self.target = df['disease_code'].values \n",
    "        self.transform = transform\n",
    "\n",
    "        print(f'Dataset size:{len(self.img_path)}')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(opj('../data/', self.img_path[idx])).astype(np.float32)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0\n",
    "        target = self.target[idx]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(torch.from_numpy(image.transpose(2,0,1)))\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "class Test_dataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.test_img_path = df['img_path'].values\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f'Test Dataset size:{len(self.test_img_path)}')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(opj('../data/', self.test_img_path[idx])).astype(np.float32)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(torch.from_numpy(image.transpose(2,0,1)))\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.test_img_path)\n",
    "\n",
    "def get_loader(df, phase: str, batch_size, shuffle,\n",
    "               num_workers, transform):\n",
    "    if phase == 'test':\n",
    "        dataset = Test_dataset(df, transform)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True)\n",
    "    else:\n",
    "        dataset = Train_Dataset(df, transform)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True,\n",
    "                                 drop_last=False)\n",
    "    return data_loader\n",
    "\n",
    "def get_train_augmentation(img_size, ver):\n",
    "    if ver==1: # for validset\n",
    "        transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "                ])\n",
    "\n",
    "    if ver == 2:\n",
    "        transform = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomAffine((20)),\n",
    "                transforms.RandomRotation(90),\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "    \n",
    "    \n",
    "    return transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.encoder = timm.create_model(args.encoder_name, pretrained=True,\n",
    "                                    drop_path_rate=args.drop_path_rate,\n",
    "                                    )\n",
    "        \n",
    "        num_head = self.encoder.head.fc.in_features\n",
    "        self.encoder.head.fc = nn.Linear(num_head, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "class Network_test(nn.Module):\n",
    "    def __init__(self, encoder_name):\n",
    "        super().__init__()\n",
    "        self.encoder = timm.create_model(encoder_name, pretrained=True,\n",
    "                                    drop_path_rate=0,\n",
    "                                    )\n",
    "        \n",
    "        num_head = self.encoder.head.fc.in_features\n",
    "        self.encoder.head.fc = nn.Linear(num_head, 7)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer for Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, args, save_path):\n",
    "        '''\n",
    "        args: arguments\n",
    "        save_path: Model 가중치 저장 경로\n",
    "        '''\n",
    "        super(Trainer, self).__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Logging\n",
    "        log_file = os.path.join(save_path, 'log.log')\n",
    "        self.logger = get_root_logger(logger_name='IR', log_level=logging.INFO, log_file=log_file)\n",
    "        self.logger.info(args)\n",
    "        # self.logger.info(args.tag)\n",
    "\n",
    "        # Train, Valid Set load\n",
    "        ############################################################################\n",
    "        if args.step == 0 :\n",
    "            df_train = pd.read_csv(opj(args.data_path, 'train.csv'))\n",
    "        else :\n",
    "            df_train = pd.read_csv(opj(args.data_path, f'train_{args.step}step.csv'))\n",
    "\n",
    "        if args.image_type is not None:\n",
    "            df_train['img_path'] = df_train['img_path'].apply(lambda x:x.replace('train_imgs', args.image_type))\n",
    "            df_train['img_path'] = df_train['img_path'].apply(lambda x:x.replace('test_imgs', 'test_1024'))\n",
    "\n",
    "        kf = StratifiedKFold(n_splits=args.Kfold, shuffle=True, random_state=args.seed)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(range(len(df_train)), y=df_train['disease_code'])):\n",
    "            df_train.loc[val_idx, 'fold'] = fold\n",
    "        val_idx = list(df_train[df_train['fold'] == int(args.fold)].index)\n",
    "\n",
    "        df_val = df_train[df_train['fold'] == args.fold].reset_index(drop=True)\n",
    "        df_train = df_train[df_train['fold'] != args.fold].reset_index(drop=True)\n",
    "\n",
    "        # Augmentation\n",
    "        self.train_transform = get_train_augmentation(img_size=args.img_size, ver=args.aug_ver)\n",
    "        self.test_transform = get_train_augmentation(img_size=args.img_size, ver=1)\n",
    "\n",
    "        # TrainLoader\n",
    "        self.train_loader = get_loader(df_train, phase='train', batch_size=args.batch_size, shuffle=True,\n",
    "                                       num_workers=args.num_workers, transform=self.train_transform)\n",
    "        self.val_loader = get_loader(df_val, phase='train', batch_size=args.batch_size, shuffle=False,\n",
    "                                       num_workers=args.num_workers, transform=self.test_transform)\n",
    "\n",
    "        # Network\n",
    "        self.model = Network(args).to(self.device)\n",
    "        macs, params = get_model_complexity_info(self.model, (3, args.img_size, args.img_size), as_strings=True,\n",
    "                                                 print_per_layer_stat=False, verbose=False)\n",
    "        self.logger.info('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "        self.logger.info('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
    "\n",
    "        # Loss\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Optimizer & Scheduler\n",
    "        self.optimizer = optim.Lamb(self.model.parameters(), lr=args.initial_lr, weight_decay=args.weight_decay)\n",
    "        \n",
    "        iter_per_epoch = len(self.train_loader)\n",
    "        self.warmup_scheduler = WarmUpLR(self.optimizer, iter_per_epoch * args.warm_epoch)\n",
    "\n",
    "        if args.scheduler == 'step':\n",
    "            self.scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=args.milestone, gamma=args.lr_factor, verbose=True)\n",
    "        elif args.scheduler == 'cos':\n",
    "            tmax = args.tmax # half-cycle \n",
    "            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max = tmax, eta_min=args.min_lr, verbose=True)\n",
    "        elif args.scheduler == 'cycle':\n",
    "            self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, max_lr=args.max_lr, steps_per_epoch=iter_per_epoch, epochs=args.epochs)\n",
    "\n",
    "        if args.multi_gpu:\n",
    "            self.model = nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "        # Train / Validate\n",
    "        best_loss = np.inf\n",
    "        best_acc = 0\n",
    "        best_epoch = 0\n",
    "        early_stopping = 0\n",
    "        start = time.time()\n",
    "        for epoch in range(1, args.epochs+1):\n",
    "            self.epoch = epoch\n",
    "\n",
    "            if args.scheduler == 'cos':\n",
    "                if epoch > args.warm_epoch:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            # Training\n",
    "            train_loss, train_acc, train_f1 = self.training(args)\n",
    "\n",
    "            # Model weight in Multi_GPU or Single GPU\n",
    "            state_dict= self.model.module.state_dict() if args.multi_gpu else self.model.state_dict()\n",
    "\n",
    "            # Validation\n",
    "            val_loss, val_acc, val_f1 = self.validate(args, phase='val')\n",
    "\n",
    "            # Save models\n",
    "            if val_loss < best_loss:\n",
    "                early_stopping = 0\n",
    "                best_epoch = epoch\n",
    "                best_loss = val_loss\n",
    "                best_acc = val_acc\n",
    "                best_f1 = val_f1\n",
    "\n",
    "                torch.save({'epoch':epoch,\n",
    "                            'state_dict':state_dict,\n",
    "                            'optimizer': self.optimizer.state_dict(),\n",
    "                            'scheduler': self.scheduler.state_dict(),\n",
    "                    }, os.path.join(save_path, 'best_model.pth'))\n",
    "                self.logger.info(f'-----------------SAVE:{best_epoch}epoch----------------')\n",
    "            else:\n",
    "                early_stopping += 1\n",
    "\n",
    "            # Early Stopping\n",
    "            if early_stopping == args.patience:\n",
    "                break\n",
    "\n",
    "        self.logger.info(f'\\nBest Val Epoch:{best_epoch} | Val Loss:{best_loss:.4f} | Val Acc:{best_acc:.4f} | Val F1:{best_f1:.4f}')\n",
    "        end = time.time()\n",
    "        self.logger.info(f'Total Process time:{(end - start) / 60:.3f}Minute')\n",
    "\n",
    "    # Training\n",
    "    def training(self, args):\n",
    "        self.model.train()\n",
    "        train_loss = AvgMeter()\n",
    "        train_acc = 0\n",
    "        preds_list = []\n",
    "        targets_list = []\n",
    "\n",
    "        scaler = grad_scaler.GradScaler()\n",
    "        for i, (images, targets) in enumerate(tqdm(self.train_loader)):\n",
    "            images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
    "            targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
    "            \n",
    "            if self.epoch <= args.warm_epoch:\n",
    "                self.warmup_scheduler.step()\n",
    "\n",
    "            self.model.zero_grad(set_to_none=True)\n",
    "            if args.amp:\n",
    "                with autocast():\n",
    "                    preds = self.model(images)\n",
    "                    loss = self.criterion(preds, targets)\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                # Gradient Clipping\n",
    "                if args.clipping is not None:\n",
    "                    scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
    "\n",
    "                scaler.step(self.optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            else:\n",
    "                preds = self.model(images)\n",
    "                loss = self.criterion(preds, targets)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
    "                self.optimizer.step()\n",
    "\n",
    "            if args.scheduler == 'cycle':\n",
    "                if self.epoch > args.warm_epoch:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            # Metric\n",
    "            train_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
    "            preds_list.extend(preds.argmax(dim=1).cpu().detach().numpy())\n",
    "            targets_list.extend(targets.cpu().detach().numpy())\n",
    "            # log\n",
    "            train_loss.update(loss.item(), n=images.size(0))\n",
    "\n",
    "        train_acc /= len(self.train_loader.dataset)\n",
    "        train_f1 = f1_score(np.array(targets_list), np.array(preds_list), average='macro')\n",
    "\n",
    "        self.logger.info(f'Epoch:[{self.epoch:03d}/{args.epochs:03d}]')\n",
    "        self.logger.info(f'Train Loss:{train_loss.avg:.3f} | Acc:{train_acc:.4f} | F1:{train_f1:.4f}')\n",
    "        return train_loss.avg, train_acc, train_f1\n",
    "            \n",
    "    # Validation or Dev\n",
    "    def validate(self, args, phase='val'):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = AvgMeter()\n",
    "            val_acc = 0\n",
    "            preds_list = []\n",
    "            targets_list = []\n",
    "\n",
    "            for i, (images, targets) in enumerate(self.val_loader):\n",
    "                images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
    "                targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
    "\n",
    "                preds = self.model(images)\n",
    "                loss = self.criterion(preds, targets)\n",
    "\n",
    "                # Metric\n",
    "                val_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
    "                preds_list.extend(preds.argmax(dim=1).cpu().detach().numpy())\n",
    "                targets_list.extend(targets.cpu().detach().numpy())\n",
    "\n",
    "                # log\n",
    "                val_loss.update(loss.item(), n=images.size(0))\n",
    "            val_acc /= len(self.val_loader.dataset)\n",
    "            val_f1 = f1_score(np.array(targets_list), np.array(preds_list), average='macro')\n",
    "\n",
    "            self.logger.info(f'{phase} Loss:{val_loss.avg:.3f} | Acc:{val_acc:.4f} | F1:{val_f1:.4f}')\n",
    "        return val_loss.avg, val_acc, val_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    print('<---- Training Params ---->')\n",
    "    \n",
    "    # Random Seed\n",
    "    seed = args.seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    save_path = os.path.join(args.model_path, (args.exp_num).zfill(3))\n",
    "    \n",
    "    # Create model directory\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    Trainer(args, save_path)\n",
    "\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference & Make pseudo label set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(encoder_name, test_loader, device, model_path):\n",
    "    model = Network_test(encoder_name).to(device)\n",
    "    model.load_state_dict(torch.load(opj(model_path, 'best_model.pth'))['state_dict'])\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(test_loader):\n",
    "            images = torch.as_tensor(images, device=device, dtype=torch.float32)\n",
    "            preds = model(images)\n",
    "            preds = torch.softmax(preds, dim=1)\n",
    "            preds_list.extend(preds.cpu().tolist())\n",
    "\n",
    "    return np.array(preds_list)\n",
    "\n",
    "def ensemble_5fold(model_path_list, test_loader, device):\n",
    "    predict_list = []\n",
    "    for model_path in model_path_list:\n",
    "        prediction = predict(encoder_name= 'regnety_040', test_loader = test_loader, device = device, model_path = model_path)\n",
    "        predict_list.append(prediction)\n",
    "    ensemble = (predict_list[0] + predict_list[1] + predict_list[2] + predict_list[3] + predict_list[4])/len(predict_list)\n",
    "\n",
    "    return ensemble\n",
    "\n",
    "\n",
    "def make_pseudo_df(train_df, test_df, ensemble, step, threshold = 0.9, z_sample = 500): \n",
    "    train_df_copy = train_df.copy()\n",
    "    test_df_copy = test_df.copy()\n",
    "\n",
    "    test_df_copy['disease'] = np.nan\n",
    "    test_df_copy['disease_code'] = ensemble.argmax(axis=1)\n",
    "    pseudo_test_df = test_df_copy.iloc[np.where(ensemble > threshold)[0]].reset_index(drop=True)\n",
    "    z_idx  = pseudo_test_df[pseudo_test_df['disease_code'] == 0].sample(n=z_sample, random_state=42).index.tolist()\n",
    "    ot_idx = pseudo_test_df[pseudo_test_df['disease_code'].isin([*range(1,8)])].index.tolist()\n",
    "    pseudo_test_df = pseudo_test_df.iloc[z_idx + ot_idx]\n",
    "\n",
    "    train_df_copy = train_df_copy.append(pseudo_test_df, ignore_index=True).reset_index(drop=True) # reset_index\n",
    "    # print(f'Make train_{step}step.csv')\n",
    "    train_df_copy.to_csv(f'../data/train_{step}step.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Inference\n",
    "- 5fold Training -> Inference & Ensemble -> Make or Update Pseudo label set -> Add Dataset(Trainset + Pseudo label set)\n",
    "- 다음과 과정을 반복하기 때문에 Training과 Inference를 동시에 진행했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-25 14:27:11,057 INFO: {'exp_num': '0', 'data_path': '../data', 'Kfold': 5, 'model_path': 'results/', 'image_type': 'train_1024', 'encoder_name': 'regnety_040', 'drop_path_rate': 0.2, 'img_size': 288, 'batch_size': 16, 'epochs': 15, 'optimizer': 'Lamb', 'initial_lr': 5e-06, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'cycle', 'warm_epoch': 2, 'max_lr': 0.001, 'min_lr': 5e-06, 'tmax': 145, 'patience': 15, 'clipping': None, 'amp': True, 'multi_gpu': False, 'logging': False, 'num_workers': 4, 'seed': 42, 'step': 5, 'fold': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset size:4750\n",
      "<---- Training Params ---->\n",
      "Dataset size:2853\n",
      "Dataset size:714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-25 14:27:11,287 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "2021-10-25 14:27:13,799 INFO: Computational complexity:       6.58 GMac\n",
      "2021-10-25 14:27:13,800 INFO: Number of parameters:           19.57 M \n",
      "100%|██████████| 179/179 [01:33<00:00,  1.92it/s]\n",
      "2021-10-25 14:28:46,849 INFO: Epoch:[001/015]\n",
      "2021-10-25 14:28:46,849 INFO: Train Loss:2.048 | Acc:0.1272 | F1:0.0922\n",
      "2021-10-25 14:28:50,790 INFO: val Loss:2.053 | Acc:0.1176 | F1:0.0722\n",
      "2021-10-25 14:28:51,304 INFO: -----------------SAVE:1epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.96it/s]\n",
      "2021-10-25 14:30:22,709 INFO: Epoch:[002/015]\n",
      "2021-10-25 14:30:22,709 INFO: Train Loss:2.016 | Acc:0.1385 | F1:0.1002\n",
      "2021-10-25 14:30:26,330 INFO: val Loss:2.009 | Acc:0.1429 | F1:0.0872\n",
      "2021-10-25 14:30:26,803 INFO: -----------------SAVE:2epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.95it/s]\n",
      "2021-10-25 14:31:58,418 INFO: Epoch:[003/015]\n",
      "2021-10-25 14:31:58,418 INFO: Train Loss:1.776 | Acc:0.3214 | F1:0.2990\n",
      "2021-10-25 14:32:01,990 INFO: val Loss:1.200 | Acc:0.7913 | F1:0.8133\n",
      "2021-10-25 14:32:02,480 INFO: -----------------SAVE:3epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.95it/s]\n",
      "2021-10-25 14:33:34,068 INFO: Epoch:[004/015]\n",
      "2021-10-25 14:33:34,068 INFO: Train Loss:1.007 | Acc:0.7774 | F1:0.7888\n",
      "2021-10-25 14:33:37,662 INFO: val Loss:0.168 | Acc:0.9790 | F1:0.9802\n",
      "2021-10-25 14:33:38,181 INFO: -----------------SAVE:4epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.96it/s]\n",
      "2021-10-25 14:35:09,476 INFO: Epoch:[005/015]\n",
      "2021-10-25 14:35:09,477 INFO: Train Loss:0.463 | Acc:0.9173 | F1:0.9218\n",
      "2021-10-25 14:35:13,177 INFO: val Loss:0.108 | Acc:0.9594 | F1:0.9693\n",
      "2021-10-25 14:35:13,658 INFO: -----------------SAVE:5epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.96it/s]\n",
      "2021-10-25 14:36:44,852 INFO: Epoch:[006/015]\n",
      "2021-10-25 14:36:44,852 INFO: Train Loss:0.362 | Acc:0.9320 | F1:0.9348\n",
      "2021-10-25 14:36:48,495 INFO: val Loss:0.072 | Acc:0.9860 | F1:0.9858\n",
      "2021-10-25 14:36:48,999 INFO: -----------------SAVE:6epoch----------------\n",
      "100%|██████████| 179/179 [01:32<00:00,  1.93it/s]\n",
      "2021-10-25 14:38:21,732 INFO: Epoch:[007/015]\n",
      "2021-10-25 14:38:21,732 INFO: Train Loss:0.256 | Acc:0.9457 | F1:0.9504\n",
      "2021-10-25 14:38:25,335 INFO: val Loss:0.047 | Acc:0.9860 | F1:0.9853\n",
      "2021-10-25 14:38:25,793 INFO: -----------------SAVE:7epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.95it/s]\n",
      "2021-10-25 14:39:57,437 INFO: Epoch:[008/015]\n",
      "2021-10-25 14:39:57,438 INFO: Train Loss:0.181 | Acc:0.9657 | F1:0.9668\n",
      "2021-10-25 14:40:01,068 INFO: val Loss:0.015 | Acc:0.9930 | F1:0.9948\n",
      "2021-10-25 14:40:01,520 INFO: -----------------SAVE:8epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.95it/s]\n",
      "2021-10-25 14:41:33,166 INFO: Epoch:[009/015]\n",
      "2021-10-25 14:41:33,166 INFO: Train Loss:0.133 | Acc:0.9751 | F1:0.9774\n",
      "2021-10-25 14:41:36,971 INFO: val Loss:0.011 | Acc:0.9944 | F1:0.9959\n",
      "2021-10-25 14:41:37,479 INFO: -----------------SAVE:9epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.96it/s]\n",
      "2021-10-25 14:43:08,634 INFO: Epoch:[010/015]\n",
      "2021-10-25 14:43:08,634 INFO: Train Loss:0.090 | Acc:0.9807 | F1:0.9821\n",
      "2021-10-25 14:43:12,331 INFO: val Loss:0.007 | Acc:0.9986 | F1:0.9990\n",
      "2021-10-25 14:43:12,767 INFO: -----------------SAVE:10epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.95it/s]\n",
      "2021-10-25 14:44:44,415 INFO: Epoch:[011/015]\n",
      "2021-10-25 14:44:44,416 INFO: Train Loss:0.078 | Acc:0.9825 | F1:0.9850\n",
      "2021-10-25 14:44:48,150 INFO: val Loss:0.015 | Acc:0.9958 | F1:0.9969\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.95it/s]\n",
      "2021-10-25 14:46:19,837 INFO: Epoch:[012/015]\n",
      "2021-10-25 14:46:19,837 INFO: Train Loss:0.075 | Acc:0.9825 | F1:0.9841\n",
      "2021-10-25 14:46:23,604 INFO: val Loss:0.010 | Acc:0.9972 | F1:0.9980\n",
      "100%|██████████| 179/179 [01:32<00:00,  1.94it/s]\n",
      "2021-10-25 14:47:55,682 INFO: Epoch:[013/015]\n",
      "2021-10-25 14:47:55,683 INFO: Train Loss:0.054 | Acc:0.9891 | F1:0.9896\n",
      "2021-10-25 14:47:59,445 INFO: val Loss:0.003 | Acc:0.9986 | F1:0.9990\n",
      "2021-10-25 14:47:59,932 INFO: -----------------SAVE:13epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.95it/s]\n",
      "2021-10-25 14:49:31,738 INFO: Epoch:[014/015]\n",
      "2021-10-25 14:49:31,739 INFO: Train Loss:0.043 | Acc:0.9944 | F1:0.9947\n",
      "2021-10-25 14:49:35,298 INFO: val Loss:0.006 | Acc:0.9986 | F1:0.9990\n",
      "100%|██████████| 179/179 [01:32<00:00,  1.94it/s]\n",
      "2021-10-25 14:51:07,637 INFO: Epoch:[015/015]\n",
      "2021-10-25 14:51:07,638 INFO: Train Loss:0.019 | Acc:0.9979 | F1:0.9976\n",
      "2021-10-25 14:51:11,400 INFO: val Loss:0.005 | Acc:0.9972 | F1:0.9979\n",
      "2021-10-25 14:51:11,401 INFO: \n",
      "Best Val Epoch:13 | Val Loss:0.0035 | Val Acc:0.9986 | Val F1:0.9990\n",
      "2021-10-25 14:51:11,401 INFO: Total Process time:23.960Minute\n",
      "2021-10-25 14:51:11,403 INFO: {'exp_num': '1', 'data_path': '../data', 'Kfold': 5, 'model_path': 'results/', 'image_type': 'train_1024', 'encoder_name': 'regnety_040', 'drop_path_rate': 0.2, 'img_size': 288, 'batch_size': 16, 'epochs': 15, 'optimizer': 'Lamb', 'initial_lr': 5e-06, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'cycle', 'warm_epoch': 2, 'max_lr': 0.001, 'min_lr': 5e-06, 'tmax': 145, 'patience': 15, 'clipping': None, 'amp': True, 'multi_gpu': False, 'logging': False, 'num_workers': 4, 'seed': 42, 'step': 5, 'fold': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "Dataset size:2853\n",
      "Dataset size:714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-25 14:51:11,627 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "2021-10-25 14:51:11,752 INFO: Computational complexity:       6.58 GMac\n",
      "2021-10-25 14:51:11,752 INFO: Number of parameters:           19.57 M \n",
      "100%|██████████| 179/179 [01:32<00:00,  1.94it/s]\n",
      "2021-10-25 14:52:43,803 INFO: Epoch:[001/015]\n",
      "2021-10-25 14:52:43,804 INFO: Train Loss:2.047 | Acc:0.1206 | F1:0.0903\n",
      "2021-10-25 14:52:47,285 INFO: val Loss:2.080 | Acc:0.1008 | F1:0.0595\n",
      "2021-10-25 14:52:47,745 INFO: -----------------SAVE:1epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.95it/s]\n",
      "2021-10-25 14:54:19,583 INFO: Epoch:[002/015]\n",
      "2021-10-25 14:54:19,583 INFO: Train Loss:2.016 | Acc:0.1462 | F1:0.1120\n",
      "2021-10-25 14:54:23,039 INFO: val Loss:2.024 | Acc:0.1092 | F1:0.0652\n",
      "2021-10-25 14:54:23,508 INFO: -----------------SAVE:2epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.96it/s]\n",
      "2021-10-25 14:55:55,029 INFO: Epoch:[003/015]\n",
      "2021-10-25 14:55:55,030 INFO: Train Loss:1.784 | Acc:0.3172 | F1:0.2948\n",
      "2021-10-25 14:55:58,549 INFO: val Loss:1.200 | Acc:0.7969 | F1:0.8080\n",
      "2021-10-25 14:55:59,011 INFO: -----------------SAVE:3epoch----------------\n",
      "100%|██████████| 179/179 [01:30<00:00,  1.99it/s]\n",
      "2021-10-25 14:57:29,023 INFO: Epoch:[004/015]\n",
      "2021-10-25 14:57:29,023 INFO: Train Loss:1.007 | Acc:0.7809 | F1:0.7913\n",
      "2021-10-25 14:57:32,596 INFO: val Loss:0.181 | Acc:0.9776 | F1:0.9809\n",
      "2021-10-25 14:57:33,056 INFO: -----------------SAVE:4epoch----------------\n",
      "100%|██████████| 179/179 [01:29<00:00,  1.99it/s]\n",
      "2021-10-25 14:59:02,796 INFO: Epoch:[005/015]\n",
      "2021-10-25 14:59:02,796 INFO: Train Loss:0.485 | Acc:0.9054 | F1:0.9103\n",
      "2021-10-25 14:59:06,274 INFO: val Loss:0.043 | Acc:0.9930 | F1:0.9921\n",
      "2021-10-25 14:59:06,729 INFO: -----------------SAVE:5epoch----------------\n",
      "100%|██████████| 179/179 [01:30<00:00,  1.97it/s]\n",
      "2021-10-25 15:00:37,559 INFO: Epoch:[006/015]\n",
      "2021-10-25 15:00:37,559 INFO: Train Loss:0.344 | Acc:0.9341 | F1:0.9378\n",
      "2021-10-25 15:00:41,143 INFO: val Loss:0.114 | Acc:0.9608 | F1:0.9577\n",
      "100%|██████████| 179/179 [01:32<00:00,  1.94it/s]\n",
      "2021-10-25 15:02:13,338 INFO: Epoch:[007/015]\n",
      "2021-10-25 15:02:13,338 INFO: Train Loss:0.267 | Acc:0.9460 | F1:0.9512\n",
      "2021-10-25 15:02:17,016 INFO: val Loss:0.022 | Acc:0.9972 | F1:0.9961\n",
      "2021-10-25 15:02:17,495 INFO: -----------------SAVE:7epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.95it/s]\n",
      "2021-10-25 15:03:49,283 INFO: Epoch:[008/015]\n",
      "2021-10-25 15:03:49,283 INFO: Train Loss:0.175 | Acc:0.9646 | F1:0.9694\n",
      "2021-10-25 15:03:52,781 INFO: val Loss:0.025 | Acc:0.9930 | F1:0.9930\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.95it/s]\n",
      "2021-10-25 15:05:24,630 INFO: Epoch:[009/015]\n",
      "2021-10-25 15:05:24,630 INFO: Train Loss:0.131 | Acc:0.9709 | F1:0.9763\n",
      "2021-10-25 15:05:28,116 INFO: val Loss:0.016 | Acc:0.9972 | F1:0.9963\n",
      "2021-10-25 15:05:28,594 INFO: -----------------SAVE:9epoch----------------\n",
      "100%|██████████| 179/179 [01:29<00:00,  1.99it/s]\n",
      "2021-10-25 15:06:58,566 INFO: Epoch:[010/015]\n",
      "2021-10-25 15:06:58,567 INFO: Train Loss:0.088 | Acc:0.9818 | F1:0.9845\n",
      "2021-10-25 15:07:02,007 INFO: val Loss:0.021 | Acc:0.9958 | F1:0.9951\n",
      "100%|██████████| 179/179 [01:30<00:00,  1.97it/s]\n",
      "2021-10-25 15:08:32,909 INFO: Epoch:[011/015]\n",
      "2021-10-25 15:08:32,910 INFO: Train Loss:0.076 | Acc:0.9814 | F1:0.9825\n",
      "2021-10-25 15:08:36,402 INFO: val Loss:0.024 | Acc:0.9930 | F1:0.9930\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.96it/s]\n",
      "2021-10-25 15:10:07,964 INFO: Epoch:[012/015]\n",
      "2021-10-25 15:10:07,964 INFO: Train Loss:0.059 | Acc:0.9877 | F1:0.9897\n",
      "2021-10-25 15:10:11,438 INFO: val Loss:0.026 | Acc:0.9972 | F1:0.9961\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.97it/s]\n",
      "2021-10-25 15:11:42,467 INFO: Epoch:[013/015]\n",
      "2021-10-25 15:11:42,467 INFO: Train Loss:0.048 | Acc:0.9909 | F1:0.9925\n",
      "2021-10-25 15:11:45,973 INFO: val Loss:0.016 | Acc:0.9958 | F1:0.9951\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.95it/s]\n",
      "2021-10-25 15:13:17,972 INFO: Epoch:[014/015]\n",
      "2021-10-25 15:13:17,972 INFO: Train Loss:0.042 | Acc:0.9909 | F1:0.9913\n",
      "2021-10-25 15:13:21,399 INFO: val Loss:0.017 | Acc:0.9958 | F1:0.9951\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.95it/s]\n",
      "2021-10-25 15:14:53,316 INFO: Epoch:[015/015]\n",
      "2021-10-25 15:14:53,316 INFO: Train Loss:0.023 | Acc:0.9965 | F1:0.9971\n",
      "2021-10-25 15:14:56,844 INFO: val Loss:0.015 | Acc:0.9972 | F1:0.9961\n",
      "2021-10-25 15:14:57,368 INFO: -----------------SAVE:15epoch----------------\n",
      "2021-10-25 15:14:57,369 INFO: \n",
      "Best Val Epoch:15 | Val Loss:0.0149 | Val Acc:0.9972 | Val F1:0.9961\n",
      "2021-10-25 15:14:57,369 INFO: Total Process time:23.760Minute\n",
      "2021-10-25 15:14:57,371 INFO: {'exp_num': '2', 'data_path': '../data', 'Kfold': 5, 'model_path': 'results/', 'image_type': 'train_1024', 'encoder_name': 'regnety_040', 'drop_path_rate': 0.2, 'img_size': 288, 'batch_size': 16, 'epochs': 15, 'optimizer': 'Lamb', 'initial_lr': 5e-06, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'cycle', 'warm_epoch': 2, 'max_lr': 0.001, 'min_lr': 5e-06, 'tmax': 145, 'patience': 15, 'clipping': None, 'amp': True, 'multi_gpu': False, 'logging': False, 'num_workers': 4, 'seed': 42, 'step': 5, 'fold': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "Dataset size:2854\n",
      "Dataset size:713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-25 15:14:57,596 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "2021-10-25 15:14:57,709 INFO: Computational complexity:       6.58 GMac\n",
      "2021-10-25 15:14:57,710 INFO: Number of parameters:           19.57 M \n",
      "100%|██████████| 179/179 [01:31<00:00,  1.95it/s]\n",
      "2021-10-25 15:16:29,526 INFO: Epoch:[001/015]\n",
      "2021-10-25 15:16:29,526 INFO: Train Loss:2.049 | Acc:0.1244 | F1:0.0957\n",
      "2021-10-25 15:16:33,417 INFO: val Loss:2.051 | Acc:0.1304 | F1:0.0834\n",
      "2021-10-25 15:16:33,884 INFO: -----------------SAVE:1epoch----------------\n",
      "100%|██████████| 179/179 [01:30<00:00,  1.97it/s]\n",
      "2021-10-25 15:18:04,684 INFO: Epoch:[002/015]\n",
      "2021-10-25 15:18:04,685 INFO: Train Loss:2.024 | Acc:0.1405 | F1:0.1016\n",
      "2021-10-25 15:18:08,284 INFO: val Loss:2.002 | Acc:0.1557 | F1:0.1014\n",
      "2021-10-25 15:18:08,730 INFO: -----------------SAVE:2epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.96it/s]\n",
      "2021-10-25 15:19:40,102 INFO: Epoch:[003/015]\n",
      "2021-10-25 15:19:40,102 INFO: Train Loss:1.785 | Acc:0.3304 | F1:0.3075\n",
      "2021-10-25 15:19:43,821 INFO: val Loss:1.200 | Acc:0.8233 | F1:0.8465\n",
      "2021-10-25 15:19:44,342 INFO: -----------------SAVE:3epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.96it/s]\n",
      "2021-10-25 15:21:15,890 INFO: Epoch:[004/015]\n",
      "2021-10-25 15:21:15,890 INFO: Train Loss:0.999 | Acc:0.7817 | F1:0.7963\n",
      "2021-10-25 15:21:19,504 INFO: val Loss:0.147 | Acc:0.9846 | F1:0.9886\n",
      "2021-10-25 15:21:19,976 INFO: -----------------SAVE:4epoch----------------\n",
      "100%|██████████| 179/179 [01:30<00:00,  1.97it/s]\n",
      "2021-10-25 15:22:50,766 INFO: Epoch:[005/015]\n",
      "2021-10-25 15:22:50,766 INFO: Train Loss:0.454 | Acc:0.9229 | F1:0.9269\n",
      "2021-10-25 15:22:54,475 INFO: val Loss:0.035 | Acc:0.9986 | F1:0.9990\n",
      "2021-10-25 15:22:54,954 INFO: -----------------SAVE:5epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.96it/s]\n",
      "2021-10-25 15:24:26,378 INFO: Epoch:[006/015]\n",
      "2021-10-25 15:24:26,378 INFO: Train Loss:0.364 | Acc:0.9264 | F1:0.9309\n",
      "2021-10-25 15:24:30,035 INFO: val Loss:0.028 | Acc:0.9930 | F1:0.9946\n",
      "2021-10-25 15:24:30,513 INFO: -----------------SAVE:6epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.95it/s]\n",
      "2021-10-25 15:26:02,308 INFO: Epoch:[007/015]\n",
      "2021-10-25 15:26:02,309 INFO: Train Loss:0.250 | Acc:0.9509 | F1:0.9581\n",
      "2021-10-25 15:26:05,951 INFO: val Loss:0.016 | Acc:0.9972 | F1:0.9969\n",
      "2021-10-25 15:26:06,467 INFO: -----------------SAVE:7epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.95it/s]\n",
      "2021-10-25 15:27:38,378 INFO: Epoch:[008/015]\n",
      "2021-10-25 15:27:38,379 INFO: Train Loss:0.168 | Acc:0.9674 | F1:0.9699\n",
      "2021-10-25 15:27:42,012 INFO: val Loss:0.023 | Acc:0.9930 | F1:0.9946\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.95it/s]\n",
      "2021-10-25 15:29:13,980 INFO: Epoch:[009/015]\n",
      "2021-10-25 15:29:13,980 INFO: Train Loss:0.132 | Acc:0.9709 | F1:0.9732\n",
      "2021-10-25 15:29:17,649 INFO: val Loss:0.017 | Acc:0.9944 | F1:0.9958\n",
      "100%|██████████| 179/179 [01:32<00:00,  1.93it/s]\n",
      "2021-10-25 15:30:50,163 INFO: Epoch:[010/015]\n",
      "2021-10-25 15:30:50,163 INFO: Train Loss:0.101 | Acc:0.9800 | F1:0.9832\n",
      "2021-10-25 15:30:53,836 INFO: val Loss:0.003 | Acc:1.0000 | F1:1.0000\n",
      "2021-10-25 15:30:54,299 INFO: -----------------SAVE:10epoch----------------\n",
      "100%|██████████| 179/179 [01:30<00:00,  1.97it/s]\n",
      "2021-10-25 15:32:25,261 INFO: Epoch:[011/015]\n",
      "2021-10-25 15:32:25,261 INFO: Train Loss:0.072 | Acc:0.9835 | F1:0.9854\n",
      "2021-10-25 15:32:28,920 INFO: val Loss:0.003 | Acc:1.0000 | F1:1.0000\n",
      "2021-10-25 15:32:29,416 INFO: -----------------SAVE:11epoch----------------\n",
      "100%|██████████| 179/179 [01:30<00:00,  1.97it/s]\n",
      "2021-10-25 15:34:00,285 INFO: Epoch:[012/015]\n",
      "2021-10-25 15:34:00,286 INFO: Train Loss:0.061 | Acc:0.9884 | F1:0.9895\n",
      "2021-10-25 15:34:03,991 INFO: val Loss:0.001 | Acc:1.0000 | F1:1.0000\n",
      "2021-10-25 15:34:04,509 INFO: -----------------SAVE:12epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.96it/s]\n",
      "2021-10-25 15:35:35,646 INFO: Epoch:[013/015]\n",
      "2021-10-25 15:35:35,647 INFO: Train Loss:0.059 | Acc:0.9898 | F1:0.9919\n",
      "2021-10-25 15:35:39,393 INFO: val Loss:0.002 | Acc:1.0000 | F1:1.0000\n",
      "100%|██████████| 179/179 [01:30<00:00,  1.97it/s]\n",
      "2021-10-25 15:37:10,210 INFO: Epoch:[014/015]\n",
      "2021-10-25 15:37:10,211 INFO: Train Loss:0.040 | Acc:0.9916 | F1:0.9936\n",
      "2021-10-25 15:37:13,810 INFO: val Loss:0.001 | Acc:1.0000 | F1:1.0000\n",
      "2021-10-25 15:37:14,290 INFO: -----------------SAVE:14epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.96it/s]\n",
      "2021-10-25 15:38:45,522 INFO: Epoch:[015/015]\n",
      "2021-10-25 15:38:45,522 INFO: Train Loss:0.025 | Acc:0.9965 | F1:0.9969\n",
      "2021-10-25 15:38:49,155 INFO: val Loss:0.000 | Acc:1.0000 | F1:1.0000\n",
      "2021-10-25 15:38:49,709 INFO: -----------------SAVE:15epoch----------------\n",
      "2021-10-25 15:38:49,710 INFO: \n",
      "Best Val Epoch:15 | Val Loss:0.0004 | Val Acc:1.0000 | Val F1:1.0000\n",
      "2021-10-25 15:38:49,710 INFO: Total Process time:23.867Minute\n",
      "2021-10-25 15:38:49,712 INFO: {'exp_num': '3', 'data_path': '../data', 'Kfold': 5, 'model_path': 'results/', 'image_type': 'train_1024', 'encoder_name': 'regnety_040', 'drop_path_rate': 0.2, 'img_size': 288, 'batch_size': 16, 'epochs': 15, 'optimizer': 'Lamb', 'initial_lr': 5e-06, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'cycle', 'warm_epoch': 2, 'max_lr': 0.001, 'min_lr': 5e-06, 'tmax': 145, 'patience': 15, 'clipping': None, 'amp': True, 'multi_gpu': False, 'logging': False, 'num_workers': 4, 'seed': 42, 'step': 5, 'fold': 3}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "Dataset size:2854\n",
      "Dataset size:713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-25 15:38:49,939 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "2021-10-25 15:38:50,048 INFO: Computational complexity:       6.58 GMac\n",
      "2021-10-25 15:38:50,049 INFO: Number of parameters:           19.57 M \n",
      "100%|██████████| 179/179 [01:31<00:00,  1.97it/s]\n",
      "2021-10-25 15:40:21,069 INFO: Epoch:[001/015]\n",
      "2021-10-25 15:40:21,070 INFO: Train Loss:2.041 | Acc:0.1345 | F1:0.1004\n",
      "2021-10-25 15:40:24,640 INFO: val Loss:2.065 | Acc:0.1038 | F1:0.0608\n",
      "2021-10-25 15:40:25,191 INFO: -----------------SAVE:1epoch----------------\n",
      "100%|██████████| 179/179 [01:30<00:00,  1.97it/s]\n",
      "2021-10-25 15:41:55,930 INFO: Epoch:[002/015]\n",
      "2021-10-25 15:41:55,930 INFO: Train Loss:2.021 | Acc:0.1517 | F1:0.1099\n",
      "2021-10-25 15:41:59,582 INFO: val Loss:2.022 | Acc:0.1206 | F1:0.0751\n",
      "2021-10-25 15:42:00,109 INFO: -----------------SAVE:2epoch----------------\n",
      "100%|██████████| 179/179 [01:30<00:00,  1.98it/s]\n",
      "2021-10-25 15:43:30,612 INFO: Epoch:[003/015]\n",
      "2021-10-25 15:43:30,613 INFO: Train Loss:1.771 | Acc:0.3364 | F1:0.3071\n",
      "2021-10-25 15:43:34,276 INFO: val Loss:1.183 | Acc:0.8345 | F1:0.8449\n",
      "2021-10-25 15:43:34,743 INFO: -----------------SAVE:3epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.96it/s]\n",
      "2021-10-25 15:45:06,024 INFO: Epoch:[004/015]\n",
      "2021-10-25 15:45:06,024 INFO: Train Loss:0.997 | Acc:0.7817 | F1:0.7961\n",
      "2021-10-25 15:45:09,556 INFO: val Loss:0.153 | Acc:0.9874 | F1:0.9908\n",
      "2021-10-25 15:45:10,116 INFO: -----------------SAVE:4epoch----------------\n",
      "100%|██████████| 179/179 [01:30<00:00,  1.97it/s]\n",
      "2021-10-25 15:46:41,057 INFO: Epoch:[005/015]\n",
      "2021-10-25 15:46:41,057 INFO: Train Loss:0.486 | Acc:0.9050 | F1:0.9114\n",
      "2021-10-25 15:46:44,658 INFO: val Loss:0.029 | Acc:0.9986 | F1:0.9990\n",
      "2021-10-25 15:46:45,135 INFO: -----------------SAVE:5epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.96it/s]\n",
      "2021-10-25 15:48:16,596 INFO: Epoch:[006/015]\n",
      "2021-10-25 15:48:16,596 INFO: Train Loss:0.364 | Acc:0.9317 | F1:0.9369\n",
      "2021-10-25 15:48:20,274 INFO: val Loss:0.046 | Acc:0.9888 | F1:0.9919\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.96it/s]\n",
      "2021-10-25 15:49:51,699 INFO: Epoch:[007/015]\n",
      "2021-10-25 15:49:51,699 INFO: Train Loss:0.270 | Acc:0.9457 | F1:0.9499\n",
      "2021-10-25 15:49:55,354 INFO: val Loss:0.029 | Acc:0.9930 | F1:0.9947\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.95it/s]\n",
      "2021-10-25 15:51:27,157 INFO: Epoch:[008/015]\n",
      "2021-10-25 15:51:27,158 INFO: Train Loss:0.176 | Acc:0.9685 | F1:0.9715\n",
      "2021-10-25 15:51:30,772 INFO: val Loss:0.028 | Acc:0.9902 | F1:0.9923\n",
      "2021-10-25 15:51:31,271 INFO: -----------------SAVE:8epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.96it/s]\n",
      "2021-10-25 15:53:02,387 INFO: Epoch:[009/015]\n",
      "2021-10-25 15:53:02,387 INFO: Train Loss:0.131 | Acc:0.9734 | F1:0.9762\n",
      "2021-10-25 15:53:06,069 INFO: val Loss:0.010 | Acc:0.9986 | F1:0.9989\n",
      "2021-10-25 15:53:06,550 INFO: -----------------SAVE:9epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.96it/s]\n",
      "2021-10-25 15:54:37,781 INFO: Epoch:[010/015]\n",
      "2021-10-25 15:54:37,781 INFO: Train Loss:0.108 | Acc:0.9793 | F1:0.9833\n",
      "2021-10-25 15:54:41,400 INFO: val Loss:0.003 | Acc:1.0000 | F1:1.0000\n",
      "2021-10-25 15:54:41,943 INFO: -----------------SAVE:10epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.95it/s]\n",
      "2021-10-25 15:56:13,710 INFO: Epoch:[011/015]\n",
      "2021-10-25 15:56:13,711 INFO: Train Loss:0.078 | Acc:0.9828 | F1:0.9864\n",
      "2021-10-25 15:56:17,331 INFO: val Loss:0.003 | Acc:1.0000 | F1:1.0000\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.95it/s]\n",
      "2021-10-25 15:57:49,215 INFO: Epoch:[012/015]\n",
      "2021-10-25 15:57:49,215 INFO: Train Loss:0.056 | Acc:0.9888 | F1:0.9908\n",
      "2021-10-25 15:57:52,827 INFO: val Loss:0.003 | Acc:0.9986 | F1:0.9990\n",
      "100%|██████████| 179/179 [01:30<00:00,  1.98it/s]\n",
      "2021-10-25 15:59:23,443 INFO: Epoch:[013/015]\n",
      "2021-10-25 15:59:23,443 INFO: Train Loss:0.056 | Acc:0.9884 | F1:0.9901\n",
      "2021-10-25 15:59:27,098 INFO: val Loss:0.002 | Acc:1.0000 | F1:1.0000\n",
      "2021-10-25 15:59:27,603 INFO: -----------------SAVE:13epoch----------------\n",
      "100%|██████████| 179/179 [01:30<00:00,  1.98it/s]\n",
      "2021-10-25 16:00:58,079 INFO: Epoch:[014/015]\n",
      "2021-10-25 16:00:58,079 INFO: Train Loss:0.038 | Acc:0.9912 | F1:0.9928\n",
      "2021-10-25 16:01:01,765 INFO: val Loss:0.003 | Acc:0.9972 | F1:0.9978\n",
      "100%|██████████| 179/179 [01:30<00:00,  1.98it/s]\n",
      "2021-10-25 16:02:32,259 INFO: Epoch:[015/015]\n",
      "2021-10-25 16:02:32,259 INFO: Train Loss:0.033 | Acc:0.9965 | F1:0.9971\n",
      "2021-10-25 16:02:35,941 INFO: val Loss:0.001 | Acc:1.0000 | F1:1.0000\n",
      "2021-10-25 16:02:36,448 INFO: -----------------SAVE:15epoch----------------\n",
      "2021-10-25 16:02:36,448 INFO: \n",
      "Best Val Epoch:15 | Val Loss:0.0006 | Val Acc:1.0000 | Val F1:1.0000\n",
      "2021-10-25 16:02:36,449 INFO: Total Process time:23.773Minute\n",
      "2021-10-25 16:02:36,451 INFO: {'exp_num': '4', 'data_path': '../data', 'Kfold': 5, 'model_path': 'results/', 'image_type': 'train_1024', 'encoder_name': 'regnety_040', 'drop_path_rate': 0.2, 'img_size': 288, 'batch_size': 16, 'epochs': 15, 'optimizer': 'Lamb', 'initial_lr': 5e-06, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'cycle', 'warm_epoch': 2, 'max_lr': 0.001, 'min_lr': 5e-06, 'tmax': 145, 'patience': 15, 'clipping': None, 'amp': True, 'multi_gpu': False, 'logging': False, 'num_workers': 4, 'seed': 42, 'step': 5, 'fold': 4}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "Dataset size:2854\n",
      "Dataset size:713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-25 16:02:36,678 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "2021-10-25 16:02:36,794 INFO: Computational complexity:       6.58 GMac\n",
      "2021-10-25 16:02:36,794 INFO: Number of parameters:           19.57 M \n",
      "100%|██████████| 179/179 [01:31<00:00,  1.96it/s]\n",
      "2021-10-25 16:04:07,952 INFO: Epoch:[001/015]\n",
      "2021-10-25 16:04:07,953 INFO: Train Loss:2.051 | Acc:0.1191 | F1:0.0861\n",
      "2021-10-25 16:04:11,624 INFO: val Loss:2.054 | Acc:0.1108 | F1:0.0682\n",
      "2021-10-25 16:04:12,202 INFO: -----------------SAVE:1epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.96it/s]\n",
      "2021-10-25 16:05:43,467 INFO: Epoch:[002/015]\n",
      "2021-10-25 16:05:43,467 INFO: Train Loss:2.026 | Acc:0.1352 | F1:0.0975\n",
      "2021-10-25 16:05:47,142 INFO: val Loss:1.995 | Acc:0.1318 | F1:0.0851\n",
      "2021-10-25 16:05:47,653 INFO: -----------------SAVE:2epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.96it/s]\n",
      "2021-10-25 16:07:18,920 INFO: Epoch:[003/015]\n",
      "2021-10-25 16:07:18,921 INFO: Train Loss:1.774 | Acc:0.3304 | F1:0.3161\n",
      "2021-10-25 16:07:22,598 INFO: val Loss:1.180 | Acc:0.8079 | F1:0.8114\n",
      "2021-10-25 16:07:23,105 INFO: -----------------SAVE:3epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.96it/s]\n",
      "2021-10-25 16:08:54,521 INFO: Epoch:[004/015]\n",
      "2021-10-25 16:08:54,521 INFO: Train Loss:1.000 | Acc:0.7761 | F1:0.7926\n",
      "2021-10-25 16:08:58,136 INFO: val Loss:0.171 | Acc:0.9790 | F1:0.9755\n",
      "2021-10-25 16:08:58,618 INFO: -----------------SAVE:4epoch----------------\n",
      "100%|██████████| 179/179 [01:30<00:00,  1.97it/s]\n",
      "2021-10-25 16:10:29,383 INFO: Epoch:[005/015]\n",
      "2021-10-25 16:10:29,384 INFO: Train Loss:0.470 | Acc:0.9177 | F1:0.9267\n",
      "2021-10-25 16:10:33,005 INFO: val Loss:0.038 | Acc:0.9902 | F1:0.9924\n",
      "2021-10-25 16:10:33,479 INFO: -----------------SAVE:5epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.96it/s]\n",
      "2021-10-25 16:12:04,679 INFO: Epoch:[006/015]\n",
      "2021-10-25 16:12:04,680 INFO: Train Loss:0.364 | Acc:0.9296 | F1:0.9361\n",
      "2021-10-25 16:12:08,251 INFO: val Loss:0.085 | Acc:0.9677 | F1:0.9767\n",
      "100%|██████████| 179/179 [01:30<00:00,  1.97it/s]\n",
      "2021-10-25 16:13:39,009 INFO: Epoch:[007/015]\n",
      "2021-10-25 16:13:39,009 INFO: Train Loss:0.272 | Acc:0.9425 | F1:0.9443\n",
      "2021-10-25 16:13:42,591 INFO: val Loss:0.031 | Acc:0.9902 | F1:0.9910\n",
      "2021-10-25 16:13:43,084 INFO: -----------------SAVE:7epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.96it/s]\n",
      "2021-10-25 16:15:14,306 INFO: Epoch:[008/015]\n",
      "2021-10-25 16:15:14,306 INFO: Train Loss:0.164 | Acc:0.9678 | F1:0.9727\n",
      "2021-10-25 16:15:17,955 INFO: val Loss:0.014 | Acc:0.9972 | F1:0.9979\n",
      "2021-10-25 16:15:18,483 INFO: -----------------SAVE:8epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.95it/s]\n",
      "2021-10-25 16:16:50,150 INFO: Epoch:[009/015]\n",
      "2021-10-25 16:16:50,151 INFO: Train Loss:0.141 | Acc:0.9723 | F1:0.9753\n",
      "2021-10-25 16:16:53,749 INFO: val Loss:0.017 | Acc:0.9958 | F1:0.9969\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.96it/s]\n",
      "2021-10-25 16:18:25,228 INFO: Epoch:[010/015]\n",
      "2021-10-25 16:18:25,228 INFO: Train Loss:0.102 | Acc:0.9793 | F1:0.9823\n",
      "2021-10-25 16:18:28,887 INFO: val Loss:0.011 | Acc:0.9958 | F1:0.9969\n",
      "2021-10-25 16:18:29,367 INFO: -----------------SAVE:10epoch----------------\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.96it/s]\n",
      "2021-10-25 16:20:00,792 INFO: Epoch:[011/015]\n",
      "2021-10-25 16:20:00,792 INFO: Train Loss:0.067 | Acc:0.9860 | F1:0.9876\n",
      "2021-10-25 16:20:04,466 INFO: val Loss:0.003 | Acc:1.0000 | F1:1.0000\n",
      "2021-10-25 16:20:04,911 INFO: -----------------SAVE:11epoch----------------\n",
      "100%|██████████| 179/179 [01:32<00:00,  1.94it/s]\n",
      "2021-10-25 16:21:37,042 INFO: Epoch:[012/015]\n",
      "2021-10-25 16:21:37,042 INFO: Train Loss:0.065 | Acc:0.9853 | F1:0.9873\n",
      "2021-10-25 16:21:40,646 INFO: val Loss:0.004 | Acc:0.9986 | F1:0.9990\n",
      "100%|██████████| 179/179 [01:32<00:00,  1.94it/s]\n",
      "2021-10-25 16:23:13,067 INFO: Epoch:[013/015]\n",
      "2021-10-25 16:23:13,068 INFO: Train Loss:0.051 | Acc:0.9905 | F1:0.9913\n",
      "2021-10-25 16:23:16,661 INFO: val Loss:0.008 | Acc:0.9986 | F1:0.9990\n",
      "100%|██████████| 179/179 [01:30<00:00,  1.97it/s]\n",
      "2021-10-25 16:24:47,462 INFO: Epoch:[014/015]\n",
      "2021-10-25 16:24:47,463 INFO: Train Loss:0.041 | Acc:0.9919 | F1:0.9935\n",
      "2021-10-25 16:24:51,066 INFO: val Loss:0.007 | Acc:0.9986 | F1:0.9989\n",
      "100%|██████████| 179/179 [01:31<00:00,  1.95it/s]\n",
      "2021-10-25 16:26:22,758 INFO: Epoch:[015/015]\n",
      "2021-10-25 16:26:22,759 INFO: Train Loss:0.028 | Acc:0.9961 | F1:0.9961\n",
      "2021-10-25 16:26:26,383 INFO: val Loss:0.002 | Acc:1.0000 | F1:1.0000\n",
      "2021-10-25 16:26:26,872 INFO: -----------------SAVE:15epoch----------------\n",
      "2021-10-25 16:26:26,873 INFO: \n",
      "Best Val Epoch:15 | Val Loss:0.0018 | Val Acc:1.0000 | Val F1:1.0000\n",
      "2021-10-25 16:26:26,873 INFO: Total Process time:23.835Minute\n",
      "2021-10-25 16:26:27,147 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "100%|██████████| 75/75 [01:20<00:00,  1.07s/it]\n",
      "2021-10-25 16:27:48,155 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "100%|██████████| 75/75 [01:18<00:00,  1.05s/it]\n",
      "2021-10-25 16:29:07,191 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "100%|██████████| 75/75 [01:19<00:00,  1.06s/it]\n",
      "2021-10-25 16:30:26,971 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "100%|██████████| 75/75 [01:18<00:00,  1.04s/it]\n",
      "2021-10-25 16:31:45,698 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "100%|██████████| 75/75 [01:18<00:00,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make train_6step.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "img_size = 288\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "sub = pd.read_csv('../data/sample_submission.csv')\n",
    "df_train = pd.read_csv('../data/train.csv')\n",
    "df_test = pd.read_csv('../data/test.csv')\n",
    "df_test['img_path'] = df_test['img_path'].apply(lambda x:x.replace('test_imgs', 'test_1024'))\n",
    "test_transform = get_train_augmentation(img_size=img_size, ver=1)\n",
    "test_dataset = Test_dataset(df_test, test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "start = 0 # first time : Only Trainset\n",
    "steps = 6 # Number of pseudo labeling times \n",
    "for step in range(start, steps+1): \n",
    "    models_path = []\n",
    "    args.step = step\n",
    "    for s_fold in range(5): # 5fold\n",
    "        args.fold = s_fold\n",
    "        args.exp_num = str(s_fold)\n",
    "        save_path = main(args)\n",
    "        models_path.append(save_path)\n",
    "    ensemble = ensemble_5fold(models_path, test_loader, device)\n",
    "    make_pseudo_df(df_train, df_test, ensemble, step+1)\n",
    "\n",
    "# For submission\n",
    "sub.iloc[:, 1] = ensemble.argmax(axis=1)\n",
    "sub.to_csv(f'./submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('leafai': conda)",
   "name": "python381jvsc74a57bd07216bea6ebc5ad7aa251f165d376913a86e37da3eaa6f99d2dc79e18699f9776"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "metadata": {
   "interpreter": {
    "hash": "7216bea6ebc5ad7aa251f165d376913a86e37da3eaa6f99d2dc79e18699f9776"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}