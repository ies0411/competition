{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f4d880",
   "metadata": {},
   "source": [
    "\n",
    "**1) Rotate 방법 변경**\n",
    "\n",
    "**2) Scheduler**\n",
    "\n",
    "**3) Cross Validation**\n",
    "\n",
    "**4) Data Paste**\n",
    "\n",
    "***\n",
    "\n",
    "1) Rotate 방법 변경\n",
    "\n",
    "validation data에서 가장 오류가 많이 보이는 부분은 2와 5, 그리고 6과 9를 잘못 구분하는 문제였습니다.\n",
    "\n",
    "2와 5의 경우 y좌표 기준에서, 그리고 6과 9의 경우 x좌표 기준에서 +-90도 이상은 rotate되지 않게 설정했습니다.\n",
    "\n",
    "--> Test score에서는 별 차이가 없었습니다, score가 내려가지는 않았다는 사실에 일단은 이 rotation 방식을 계속 사용했습니다. \n",
    "\n",
    "---\n",
    "\n",
    "2) Scheduler 추가\n",
    "\n",
    "CosineAnnealingWarmUpRestarts 스케쥴러를 추가했습니다.\n",
    "\n",
    "중간에 learning rate를 급격히 올려 local minima에 빠지는 것을 방지해 상대적으로 더 긴 epoch에서 학습이 가능했습니다.\n",
    "\n",
    "---\n",
    "\n",
    "3) Cross Validation\n",
    "\n",
    "주어진 데이터의 앞부분 80%를 train으로, 뒷부분 20%를 Validation Set으로 고정시켜놓고 진행했습니다.\n",
    "\n",
    "이것을 뒷부분 80% train set, 앞의 20%를 Validation Set으로 하는 dataloader를 추가로 만들고\n",
    "epoch마다 하나씩 돌아가면서 학습시켰습니다.\n",
    "\n",
    "---\n",
    "\n",
    "4) Data Paste\n",
    "\n",
    "기존의 50,000개 데이터를 그대로 추가해 100,000개 데이터로 만든 후 진행했습니다.\n",
    "\n",
    "3)과 4)방법을 추가한 이후 최종 private score 0.97892, public score 0.98058의 결과를 얻을 수 있었습니다.\n",
    "\n",
    "---\n",
    "\n",
    "**Ablation Study (Validation Score)**\n",
    "\n",
    "3), 4) 방법 모두 사용 시: 98.825%\n",
    "\n",
    "3) 방법 제거 시: 98.745%\n",
    "\n",
    "4) 방법 제거 시: 98.1%\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c436954",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb08a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, _LRScheduler\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import math\n",
    "from scheduler_C import *\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from dgl.geometry import farthest_point_sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ec33c9",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "413671f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "def translate_pointcloud(pointcloud):\n",
    "    xyz1 = np.random.uniform(low=2./3., high=3./2., size=[3])\n",
    "    xyz2 = np.random.uniform(low=-0.2, high=0.2, size=[3])\n",
    "    translated_pointcloud = np.add(np.multiply(pointcloud, xyz1), xyz2).astype('float32')\n",
    "    return translated_pointcloud\n",
    "def get_farthest_points(points,num_points):\n",
    "    points = torch.Tensor(points).unsqueeze(0).permute(0,1,2)\n",
    "    point_idx = farthest_point_sampler(points, num_points)\n",
    "    fps_points = torch.cat([points[0][idx] for idx in point_idx],1).numpy()\n",
    "    \n",
    "    return fps_points\n",
    "def rotate(dots):\n",
    "    a = np.random.rand()*2*np.pi # x_rot_angle\n",
    "    b = np.random.rand()*2*np.pi # y_rot_angle\n",
    "    c = np.random.rand()*2*np.pi # z_rot_angle\n",
    "    mx = np.array([[1, 0, 0], [0, np.cos(a), -np.sin(a)], [0, np.sin(a), np.cos(a)]])\n",
    "    my = np.array([[np.cos(b), 0, np.sin(b)], [0, 1, 0], [-np.sin(b), 0, np.cos(b)]])\n",
    "    mz = np.array([[np.cos(c), -np.sin(c), 0], [np.sin(c), np.cos(c), 0], [0, 0, 1]])\n",
    "    m = np.dot(np.dot(mx,my),mz)\n",
    "    dots = np.dot(dots, m.T)\n",
    "    return dots\n",
    "################################################################################################\n",
    "def rotate_y(dots):\n",
    "    a = (2*np.random.random_sample() - 1)*(np.pi) # x_rot_angle\n",
    "    b = (2*np.random.random_sample() - 1)*(np.pi/2) # y_rot_angle\n",
    "    c = (2*np.random.random_sample() - 1)*np.pi # z_rot_angle\n",
    "    mx = np.array([[1, 0, 0], [0, np.cos(a), -np.sin(a)], [0, np.sin(a), np.cos(a)]])\n",
    "    my = np.array([[np.cos(b), 0, np.sin(b)], [0, 1, 0], [-np.sin(b), 0, np.cos(b)]])\n",
    "    mz = np.array([[np.cos(c), -np.sin(c), 0], [np.sin(c), np.cos(c), 0], [0, 0, 1]])\n",
    "    m = np.dot(np.dot(mx,my),mz)\n",
    "    dots = np.dot(dots, m.T)\n",
    "    return dots\n",
    "################################################################################################\n",
    "def rotate_x(dots):\n",
    "    a = (2*np.random.random_sample() - 1)*(np.pi/2) # x_rot_angle\n",
    "    b = (2*np.random.random_sample() - 1)*np.pi # y_rot_angle\n",
    "    c = (2*np.random.random_sample() - 1)*np.pi # z_rot_angle\n",
    "    mx = np.array([[1, 0, 0], [0, np.cos(a), -np.sin(a)], [0, np.sin(a), np.cos(a)]])\n",
    "    my = np.array([[np.cos(b), 0, np.sin(b)], [0, 1, 0], [-np.sin(b), 0, np.cos(b)]])\n",
    "    mz = np.array([[np.cos(c), -np.sin(c), 0], [np.sin(c), np.cos(c), 0], [0, 0, 1]])\n",
    "    m = np.dot(np.dot(mx,my),mz)\n",
    "    dots = np.dot(dots, m.T)\n",
    "    return dots\n",
    "################################################################################################\n",
    "class Dacon_On_the_fly(Dataset):\n",
    "    def __init__(self, num_points,id_list, label_list, point_list, partition,sampler_type):\n",
    "        self.id_list = id_list\n",
    "        self.label_list = label_list\n",
    "        self.point_list = point_list\n",
    "        self.num_points = num_points\n",
    "        self.partition = partition\n",
    "        self.sampler_type = sampler_type\n",
    "\n",
    "        #######################################\n",
    "        self.point_list_dict = {i:self.point_list[str(i)][:].astype('float32') for i in tqdm(id_list)}\n",
    "        #######################################\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_id = self.id_list[index]\n",
    "\n",
    "        points = self.point_list_dict[image_id]\n",
    "        \n",
    "        ## -- rotate\n",
    "    \n",
    "        if (self.partition == 'train') or (self.partition == 'val'):\n",
    "            #print(self.label_list[index])\n",
    "            if (self.label_list[index] == 6) or (self.label_list[index] == 9):\n",
    "                points = rotate_x(points)\n",
    "            elif (self.label_list[index] == 2) or (self.label_list[index] == 5):\n",
    "                points = rotate_y(points)\n",
    "            else:\n",
    "                points = rotate(points)\n",
    "    \n",
    "        #points = rotate_y(points)\n",
    "\n",
    "        ## -- select point samples\n",
    "        if self.sampler_type == 'random':\n",
    "            np.random.shuffle(points)\n",
    "            pointcloud = points[:self.num_points]\n",
    "        elif self.sampler_type == 'farthest':\n",
    "            pointcloud = get_farthest_points(points,self.num_points)\n",
    "\n",
    "        #image = self.get_vector(points)\n",
    "        \n",
    "        if self.label_list is not None:\n",
    "            label = self.label_list[index]\n",
    "            if self.partition == 'train':\n",
    "                # pointcloud = random_point_dropout(pointcloud) # open for dgcnn not for our idea  for all\n",
    "                pointcloud = translate_pointcloud(pointcloud)\n",
    "                np.random.shuffle(pointcloud)\n",
    "            return pointcloud, label\n",
    "        else:\n",
    "            return pointcloud\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14dc487",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42e459b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pointnet2_ops import pointnet2_utils\n",
    "\n",
    "def get_activation(activation):\n",
    "    if activation.lower() == 'gelu':\n",
    "        return nn.GELU()\n",
    "    elif activation.lower() == 'rrelu':\n",
    "        return nn.RReLU(inplace=True)\n",
    "    elif activation.lower() == 'selu':\n",
    "        return nn.SELU(inplace=True)\n",
    "    elif activation.lower() == 'silu':\n",
    "        return nn.SiLU(inplace=True)\n",
    "    elif activation.lower() == 'hardswish':\n",
    "        return nn.Hardswish(inplace=True)\n",
    "    elif activation.lower() == 'leakyrelu':\n",
    "        return nn.LeakyReLU(inplace=True)\n",
    "    else:\n",
    "        return nn.ReLU(inplace=True)\n",
    "\n",
    "def square_distance(src, dst):\n",
    "    \"\"\"\n",
    "    Calculate Euclid distance between each two points.\n",
    "    \"\"\"\n",
    "    B, N, _ = src.shape\n",
    "    _, M, _ = dst.shape\n",
    "    dist = -2 * torch.matmul(src, dst.permute(0, 2, 1))\n",
    "    dist += torch.sum(src ** 2, -1).view(B, N, 1)\n",
    "    dist += torch.sum(dst ** 2, -1).view(B, 1, M)\n",
    "    return dist\n",
    "\n",
    "def index_points(points, idx):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        points: input points data, [B, N, C]\n",
    "        idx: sample index data, [B, S]\n",
    "    Return:\n",
    "        new_points:, indexed points data, [B, S, C]\n",
    "    \"\"\"\n",
    "    device = points.device\n",
    "    B = points.shape[0]\n",
    "    view_shape = list(idx.shape)\n",
    "    view_shape[1:] = [1] * (len(view_shape) - 1)\n",
    "    repeat_shape = list(idx.shape)\n",
    "    repeat_shape[0] = 1\n",
    "    batch_indices = torch.arange(B, dtype=torch.long).to(device).view(view_shape).repeat(repeat_shape)\n",
    "    new_points = points[batch_indices, idx, :]\n",
    "    return new_points\n",
    "\n",
    "def farthest_point_sample(xyz, npoint):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        xyz: pointcloud data, [B, N, 3]\n",
    "        npoint: number of samples\n",
    "    Return:\n",
    "        centroids: sampled pointcloud index, [B, npoint]\n",
    "    \"\"\"\n",
    "    device = xyz.device\n",
    "    B, N, C = xyz.shape\n",
    "    centroids = torch.zeros(B, npoint, dtype=torch.long).to(device)\n",
    "    distance = torch.ones(B, N).to(device) * 1e10\n",
    "    farthest = torch.randint(0, N, (B,), dtype=torch.long).to(device)\n",
    "    batch_indices = torch.arange(B, dtype=torch.long).to(device)\n",
    "    for i in range(npoint):\n",
    "        centroids[:, i] = farthest\n",
    "        centroid = xyz[batch_indices, farthest, :].view(B, 1, 3)\n",
    "        dist = torch.sum((xyz - centroid) ** 2, -1)\n",
    "        distance = torch.min(distance, dist)\n",
    "        farthest = torch.max(distance, -1)[1]\n",
    "    return centroids\n",
    "\n",
    "def knn_point(nsample, xyz, new_xyz):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        nsample: max sample number in local region\n",
    "        xyz: all points, [B, N, C]\n",
    "        new_xyz: query points, [B, S, C]\n",
    "    Return:\n",
    "        group_idx: grouped points index, [B, S, nsample]\n",
    "    \"\"\"\n",
    "    sqrdists = square_distance(new_xyz, xyz)\n",
    "    _, group_idx = torch.topk(sqrdists, nsample, dim=-1, largest=False, sorted=False)\n",
    "    return group_idx\n",
    "\n",
    "\n",
    "class LocalGrouper(nn.Module):\n",
    "    def __init__(self, channel, groups, kneighbors, use_xyz=True, normalize=\"center\", **kwargs):\n",
    "        \"\"\"\n",
    "        Give xyz[b,p,3] and fea[b,p,d], return new_xyz[b,g,3] and new_fea[b,g,k,d]\n",
    "        :param groups: groups number\n",
    "        :param kneighbors: k-nerighbors\n",
    "        :param kwargs: others\n",
    "        \"\"\"\n",
    "        super(LocalGrouper, self).__init__()\n",
    "        self.groups = groups\n",
    "        self.kneighbors = kneighbors\n",
    "        self.use_xyz = use_xyz\n",
    "        if normalize is not None:\n",
    "            self.normalize = normalize.lower()\n",
    "        else:\n",
    "            self.normalize = None\n",
    "        if self.normalize not in [\"center\", \"anchor\"]:\n",
    "            print(f\"Unrecognized normalize parameter (self.normalize), set to None. Should be one of [center, anchor].\")\n",
    "            self.normalize = None\n",
    "        if self.normalize is not None:\n",
    "            add_channel=3 if self.use_xyz else 0\n",
    "            self.affine_alpha = nn.Parameter(torch.ones([1,1,1,channel + add_channel]))\n",
    "            self.affine_beta = nn.Parameter(torch.zeros([1, 1, 1, channel + add_channel]))\n",
    "\n",
    "    def forward(self, xyz, points):\n",
    "        B, N, C = xyz.shape\n",
    "        S = self.groups\n",
    "        xyz = xyz.contiguous()  # xyz [btach, points, xyz]\n",
    "\n",
    "        # fps_idx = torch.multinomial(torch.linspace(0, N - 1, steps=N).repeat(B, 1).to(xyz.device), num_samples=self.groups, replacement=False).long()\n",
    "        # fps_idx = farthest_point_sample(xyz, self.groups).long()\n",
    "        fps_idx = pointnet2_utils.furthest_point_sample(xyz, self.groups).long()  # [B, npoint]\n",
    "        new_xyz = index_points(xyz, fps_idx)  # [B, npoint, 3]\n",
    "        new_points = index_points(points, fps_idx)  # [B, npoint, d]\n",
    "\n",
    "        idx = knn_point(self.kneighbors, xyz, new_xyz)\n",
    "        grouped_xyz = index_points(xyz, idx)  # [B, npoint, k, 3]\n",
    "        grouped_points = index_points(points, idx)  # [B, npoint, k, d]\n",
    "        if self.use_xyz:\n",
    "            grouped_points = torch.cat([grouped_points, grouped_xyz],dim=-1)  # [B, npoint, k, d+3]\n",
    "        if self.normalize is not None:\n",
    "            if self.normalize ==\"center\":\n",
    "                mean = torch.mean(grouped_points, dim=2, keepdim=True)\n",
    "            if self.normalize ==\"anchor\":\n",
    "                mean = torch.cat([new_points, new_xyz],dim=-1) if self.use_xyz else new_points\n",
    "                mean = mean.unsqueeze(dim=-2)  # [B, npoint, 1, d+3]\n",
    "            std = torch.std((grouped_points-mean).reshape(B,-1),dim=-1,keepdim=True).unsqueeze(dim=-1).unsqueeze(dim=-1)\n",
    "            grouped_points = (grouped_points-mean)/(std + 1e-5)\n",
    "            grouped_points = self.affine_alpha*grouped_points + self.affine_beta\n",
    "\n",
    "        new_points = torch.cat([grouped_points, new_points.view(B, S, 1, -1).repeat(1, 1, self.kneighbors, 1)], dim=-1)\n",
    "        return new_xyz, new_points\n",
    "\n",
    "\n",
    "class ConvBNReLU1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, bias=True, activation='relu'):\n",
    "        super(ConvBNReLU1D, self).__init__()\n",
    "        self.act = get_activation(activation)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, bias=bias),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            self.act\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class ConvBNReLURes1D(nn.Module):\n",
    "    def __init__(self, channel, kernel_size=1, groups=1, res_expansion=1.0, bias=True, activation='relu'):\n",
    "        super(ConvBNReLURes1D, self).__init__()\n",
    "        self.act = get_activation(activation)\n",
    "        self.net1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=channel, out_channels=int(channel * res_expansion),\n",
    "                      kernel_size=kernel_size, groups=groups, bias=bias),\n",
    "            nn.BatchNorm1d(int(channel * res_expansion)),\n",
    "            self.act\n",
    "        )\n",
    "        if groups > 1:\n",
    "            self.net2 = nn.Sequential(\n",
    "                nn.Conv1d(in_channels=int(channel * res_expansion), out_channels=channel,\n",
    "                          kernel_size=kernel_size, groups=groups, bias=bias),\n",
    "                nn.BatchNorm1d(channel),\n",
    "                self.act,\n",
    "                nn.Conv1d(in_channels=channel, out_channels=channel,\n",
    "                          kernel_size=kernel_size, bias=bias),\n",
    "                nn.BatchNorm1d(channel),\n",
    "            )\n",
    "        else:\n",
    "            self.net2 = nn.Sequential(\n",
    "                nn.Conv1d(in_channels=int(channel * res_expansion), out_channels=channel,\n",
    "                          kernel_size=kernel_size, bias=bias),\n",
    "                nn.BatchNorm1d(channel)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.net2(self.net1(x)) + x)\n",
    "\n",
    "\n",
    "class PreExtraction(nn.Module):\n",
    "    def __init__(self, channels, out_channels,  blocks=1, groups=1, res_expansion=1, bias=True,\n",
    "                 activation='relu', use_xyz=True):\n",
    "        \"\"\"\n",
    "        input: [b,g,k,d]: output:[b,d,g]\n",
    "        :param channels:\n",
    "        :param blocks:\n",
    "        \"\"\"\n",
    "        super(PreExtraction, self).__init__()\n",
    "        in_channels = 3+2*channels if use_xyz else 2*channels\n",
    "        self.transfer = ConvBNReLU1D(in_channels, out_channels, bias=bias, activation=activation)\n",
    "        operation = []\n",
    "        for _ in range(blocks):\n",
    "            operation.append(\n",
    "                ConvBNReLURes1D(out_channels, groups=groups, res_expansion=res_expansion,\n",
    "                                bias=bias, activation=activation)\n",
    "            )\n",
    "        self.operation = nn.Sequential(*operation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, s, d = x.size()  # torch.Size([32, 512, 32, 6])\n",
    "        x = x.permute(0, 1, 3, 2)\n",
    "        x = x.reshape(-1, d, s)\n",
    "        x = self.transfer(x)\n",
    "        batch_size, _, _ = x.size()\n",
    "        x = self.operation(x)  # [b, d, k]\n",
    "        x = F.adaptive_max_pool1d(x, 1).view(batch_size, -1)\n",
    "        x = x.reshape(b, n, -1).permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PosExtraction(nn.Module):\n",
    "    def __init__(self, channels, blocks=1, groups=1, res_expansion=1, bias=True, activation='relu'):\n",
    "        \"\"\"\n",
    "        input[b,d,g]; output[b,d,g]\n",
    "        :param channels:\n",
    "        :param blocks:\n",
    "        \"\"\"\n",
    "        super(PosExtraction, self).__init__()\n",
    "        operation = []\n",
    "        for _ in range(blocks):\n",
    "            operation.append(\n",
    "                ConvBNReLURes1D(channels, groups=groups, res_expansion=res_expansion, bias=bias, activation=activation)\n",
    "            )\n",
    "        self.operation = nn.Sequential(*operation)\n",
    "\n",
    "    def forward(self, x):  # [b, d, g]\n",
    "        return self.operation(x)\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, points=1024, class_num=40, embed_dim=64, groups=1, res_expansion=1.0,\n",
    "                 activation=\"relu\", bias=True, use_xyz=True, normalize=\"center\",\n",
    "                 dim_expansion=[2, 2, 2, 2], pre_blocks=[2, 2, 2, 2], pos_blocks=[2, 2, 2, 2],\n",
    "                 k_neighbors=[32, 32, 32, 32], reducers=[2, 2, 2, 2], **kwargs):\n",
    "        super(Model, self).__init__()\n",
    "        self.stages = len(pre_blocks)\n",
    "        self.class_num = class_num\n",
    "        self.points = points\n",
    "        self.embedding = ConvBNReLU1D(3, embed_dim, bias=bias, activation=activation)\n",
    "        assert len(pre_blocks) == len(k_neighbors) == len(reducers) == len(pos_blocks) == len(dim_expansion), \\\n",
    "            \"Please check stage number consistent for pre_blocks, pos_blocks k_neighbors, reducers.\"\n",
    "        self.local_grouper_list = nn.ModuleList()\n",
    "        self.pre_blocks_list = nn.ModuleList()\n",
    "        self.pos_blocks_list = nn.ModuleList()\n",
    "        last_channel = embed_dim\n",
    "        anchor_points = self.points\n",
    "        for i in range(len(pre_blocks)):\n",
    "            out_channel = last_channel * dim_expansion[i]\n",
    "            pre_block_num = pre_blocks[i]\n",
    "            pos_block_num = pos_blocks[i]\n",
    "            kneighbor = k_neighbors[i]\n",
    "            reduce = reducers[i]\n",
    "            anchor_points = anchor_points // reduce\n",
    "            # append local_grouper_list\n",
    "            local_grouper = LocalGrouper(last_channel, anchor_points, kneighbor, use_xyz, normalize)  # [b,g,k,d]\n",
    "            self.local_grouper_list.append(local_grouper)\n",
    "            # append pre_block_list\n",
    "            pre_block_module = PreExtraction(last_channel, out_channel, pre_block_num, groups=groups,\n",
    "                                             res_expansion=res_expansion,\n",
    "                                             bias=bias, activation=activation, use_xyz=use_xyz)\n",
    "            self.pre_blocks_list.append(pre_block_module)\n",
    "            # append pos_block_list\n",
    "            pos_block_module = PosExtraction(out_channel, pos_block_num, groups=groups,\n",
    "                                             res_expansion=res_expansion, bias=bias, activation=activation)\n",
    "            self.pos_blocks_list.append(pos_block_module)\n",
    "\n",
    "            last_channel = out_channel\n",
    "\n",
    "        self.act = get_activation(activation)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(last_channel, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            self.act,\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            self.act,\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, self.class_num)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        xyz = x.permute(0, 2, 1)\n",
    "        batch_size, _, _ = x.size()\n",
    "        x = self.embedding(x)  # B,D,N\n",
    "        for i in range(self.stages):\n",
    "            # Give xyz[b, p, 3] and fea[b, p, d], return new_xyz[b, g, 3] and new_fea[b, g, k, d]\n",
    "            xyz, x = self.local_grouper_list[i](xyz, x.permute(0, 2, 1))  # [b,g,3]  [b,g,k,d]\n",
    "            x = self.pre_blocks_list[i](x)  # [b,d,g]\n",
    "            x = self.pos_blocks_list[i](x)  # [b,d,g]\n",
    "\n",
    "        x = F.adaptive_max_pool1d(x, 1).squeeze(dim=-1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def pointMLP(num_classes=10, **kwargs) -> Model:\n",
    "    return Model(points=1024, class_num=num_classes, embed_dim=64, groups=1, res_expansion=1.0,\n",
    "                   activation=\"relu\", bias=False, use_xyz=False, normalize=\"anchor\",\n",
    "                   dim_expansion=[2, 2, 2, 2], pre_blocks=[2, 2, 2, 2], pos_blocks=[2, 2, 2, 2],\n",
    "                   k_neighbors=[24, 24, 24, 24], reducers=[2, 2, 2, 2], **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97be24d",
   "metadata": {},
   "source": [
    "### Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c15b131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    \"\"\"Parameters\"\"\"\n",
    "    parser = argparse.ArgumentParser('training')\n",
    "    parser.add_argument('-c', '--checkpoint', type=str, metavar='PATH',\n",
    "                        help='path to save checkpoint (default: checkpoint)')\n",
    "    parser.add_argument('--msg', type=str, default='test_jupyter', help='message after checkpoint')\n",
    "    parser.add_argument('--batch_size', type=int, default=16, help='batch size in training')\n",
    "    parser.add_argument('--model', default='pointMLP', help='model name [default: pointnet_cls]') # pontMLP / pointMLP-elite\n",
    "    parser.add_argument('--epoch', default=400, type=int, help='number of epoch in training')\n",
    "    parser.add_argument('--num_points', type=int, default=512, help='Point Number')\n",
    "    parser.add_argument('--learning_rate', default=0.01, type=float, help='learning rate in training')\n",
    "    parser.add_argument('--min_lr', default=0.00001, type=float, help='min lr')\n",
    "    parser.add_argument('--weight_decay', type=float, default=2e-4, help='decay rate')\n",
    "    parser.add_argument('--seed', type=int, help='random seed')\n",
    "    parser.add_argument('--workers', default=8, type=int, help='workers')\n",
    "\n",
    "    parser.add_argument('--gpu-num', default='1', type=str, help='workers')\n",
    "    parser.add_argument('--sampler-type', default='farthest', type=str, help='workers') # farthest / random\n",
    "    return parser.parse_args(args=[])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b19d63",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53c2f55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_loss(pred, gold, smoothing=True):\n",
    "    ''' Calculate cross entropy loss, apply label smoothing if needed. '''\n",
    "\n",
    "    gold = gold.contiguous().view(-1)\n",
    "\n",
    "    if smoothing:\n",
    "        eps = 0.2\n",
    "        n_class = pred.size(1)\n",
    "\n",
    "        one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1, 1), 1)\n",
    "        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n",
    "        log_prb = F.log_softmax(pred, dim=1)\n",
    "\n",
    "        loss = -(one_hot * log_prb).sum(dim=1).mean()\n",
    "    else:\n",
    "        loss = F.cross_entropy(pred, gold, reduction='mean')\n",
    "\n",
    "    return loss\n",
    "\n",
    "import shutil\n",
    "def save_model(net, epoch, path, acc, is_best, **kwargs):\n",
    "    state = {\n",
    "        'net': net.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'acc': acc\n",
    "    }\n",
    "    for key, value in kwargs.items():\n",
    "        state[key] = value\n",
    "    filepath = os.path.join(path, \"last_checkpoint.pth\")\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(path, 'best_checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d475c946",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b9e4ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = parse_args()\n",
    "    if args.seed is None:\n",
    "        args.seed = np.random.randint(1, 10000)\n",
    "    os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=args.gpu_num\n",
    "\n",
    "    assert torch.cuda.is_available(), \"Please ensure codes are executed in cuda.\"\n",
    "    device = 'cuda'\n",
    "    if args.seed is not None:\n",
    "        torch.manual_seed(args.seed)\n",
    "        np.random.seed(args.seed)\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "        torch.set_printoptions(10)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        os.environ['PYTHONHASHSEED'] = str(args.seed)\n",
    "    time_str = str(datetime.datetime.now().strftime('-%Y%m%d%H%M%S'))\n",
    "    if args.msg is None:\n",
    "        message = time_str\n",
    "    else:\n",
    "        message = \"-\" + args.msg\n",
    "    # Model save path\n",
    "    args.checkpoint = 'checkpoints/' + args.model + message + '-' + str(args.seed)\n",
    "    if not os.path.isdir(args.checkpoint):\n",
    "        os.makedirs(args.checkpoint)\n",
    "\n",
    "    # Model\n",
    "    print(f\"args: {args}\")\n",
    "    print('==> Building model..')\n",
    "    net = pointMLP()\n",
    "    criterion = cal_loss\n",
    "    net = net.to(device)\n",
    "    # criterion = criterion.to(device)\n",
    "    if device == 'cuda:1':\n",
    "        net = torch.nn.DataParallel(net, device_ids = [1, 0])\n",
    "        cudnn.benchmark = True\n",
    "    elif device == 'cuda':\n",
    "        net = torch.nn.DataParallel(net)\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    # initialize accuracy variables\n",
    "    best_test_acc = 0.  # best test accuracy\n",
    "    best_train_acc = 0.\n",
    "    best_test_acc_avg = 0.\n",
    "    best_train_acc_avg = 0.\n",
    "    best_test_loss = float(\"inf\")\n",
    "    best_train_loss = float(\"inf\")\n",
    "    start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "    optimizer_dict = None\n",
    "\n",
    "    print('==> Preparing data..')\n",
    "\n",
    "    #################################################################################################################\n",
    "    # Load given data [csv file] & augment as double size\n",
    "    all_df = pd.read_csv('/home/won/3D_data/3D/train.csv')\n",
    "    all_df = all_df\n",
    "    all_df2_ID = all_df[\"ID\"]\n",
    "    all_df2 = pd.concat([all_df2_ID, all_df[\"label\"]], axis=1)\n",
    "    all_df = pd.concat([all_df, all_df2])\n",
    "    \n",
    "    # Two cross-Validation data\n",
    "    train_df = all_df.iloc[:int(len(all_df)*0.8)]\n",
    "    val_df = all_df.iloc[int(len(all_df)*0.8):]\n",
    "    train_df2 = all_df.iloc[int(len(all_df)*0.2):]\n",
    "    val_df2 = all_df.iloc[:int(len(all_df)*0.2)]\n",
    "\n",
    "    # Load given data [h5 file]\n",
    "    all_points_front = h5py.File('/home/won/3D_data/3D/train.h5', 'r')\n",
    "\n",
    "\n",
    "    train_dataset = Dacon_On_the_fly(args.num_points,train_df['ID'].values, train_df['label'].values, all_points_front,'train',args.sampler_type)\n",
    "    train_loader1 = DataLoader(train_dataset, batch_size = args.batch_size, shuffle=True, num_workers=args.workers) #h5py num_worker 0밖에 안 됨... \n",
    "\n",
    "    val_dataset = Dacon_On_the_fly(args.num_points,val_df['ID'].values, val_df['label'].values, all_points_front,'val',args.sampler_type)\n",
    "    val_loader1 = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)\n",
    "\n",
    "    train_dataset2 = Dacon_On_the_fly(args.num_points,train_df2['ID'].values, train_df2['label'].values, all_points_front,'train',args.sampler_type)\n",
    "    train_loader2 = DataLoader(train_dataset2, batch_size = args.batch_size, shuffle=True, num_workers=args.workers) #h5py num_worker 0밖에 안 됨... \n",
    "\n",
    "    val_dataset2 = Dacon_On_the_fly(args.num_points,val_df2['ID'].values, val_df2['label'].values, all_points_front,'val',args.sampler_type)\n",
    "    val_loader2 = DataLoader(val_dataset2, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)\n",
    "\n",
    "    #################################################################################################################\n",
    "\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=args.learning_rate, momentum=0.9, weight_decay=args.weight_decay)\n",
    "    if optimizer_dict is not None:\n",
    "        optimizer.load_state_dict(optimizer_dict)\n",
    "\n",
    "    scheduler = CosineAnnealingWarmUpRestarts(optimizer, T_0=60, T_mult=2, T_up=10, gamma=0.3, eta_max=0.1, last_epoch=start_epoch - 1)\n",
    "\n",
    "    for epoch in range(start_epoch, args.epoch):\n",
    "        if int(epoch)%2 == 0:\n",
    "            train_loader = train_loader1\n",
    "            val_loader = val_loader1\n",
    "        else:\n",
    "            train_loader, val_loader = train_loader2, val_loader2\n",
    "\n",
    "        train_out = train(net, train_loader, optimizer, criterion, device)  # {\"loss\", \"acc\", \"acc_avg\", \"time\"}\n",
    "        test_out = validate(net, val_loader, criterion, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        if test_out[\"acc\"] > best_test_acc:\n",
    "            best_test_acc = test_out[\"acc\"]\n",
    "            is_best = True\n",
    "        else:\n",
    "            is_best = False\n",
    "\n",
    "        best_test_acc = test_out[\"acc\"] if (test_out[\"acc\"] > best_test_acc) else best_test_acc\n",
    "        best_train_acc = train_out[\"acc\"] if (train_out[\"acc\"] > best_train_acc) else best_train_acc\n",
    "        best_test_acc_avg = test_out[\"acc_avg\"] if (test_out[\"acc_avg\"] > best_test_acc_avg) else best_test_acc_avg\n",
    "        best_train_acc_avg = train_out[\"acc_avg\"] if (train_out[\"acc_avg\"] > best_train_acc_avg) else best_train_acc_avg\n",
    "        best_test_loss = test_out[\"loss\"] if (test_out[\"loss\"] < best_test_loss) else best_test_loss\n",
    "        best_train_loss = train_out[\"loss\"] if (train_out[\"loss\"] < best_train_loss) else best_train_loss\n",
    "\n",
    "        save_model(\n",
    "            net, epoch, path=args.checkpoint, acc=test_out[\"acc\"], is_best=is_best,\n",
    "            best_test_acc=best_test_acc,  # best test accuracy\n",
    "            best_train_acc=best_train_acc,\n",
    "            best_test_acc_avg=best_test_acc_avg,\n",
    "            best_train_acc_avg=best_train_acc_avg,\n",
    "            best_test_loss=best_test_loss,\n",
    "            best_train_loss=best_train_loss,\n",
    "            optimizer=optimizer.state_dict()\n",
    "        )\n",
    "\n",
    "\n",
    "    print(f\"++++++++\" * 2 + \"Final results\" + \"++++++++\" * 2)\n",
    "    print(f\"++  Last Train time: {train_out['time']} | Last Test time: {test_out['time']}  ++\")\n",
    "    print(f\"++  Best Train loss: {best_train_loss} | Best Test loss: {best_test_loss}  ++\")\n",
    "    print(f\"++  Best Train acc_B: {best_train_acc_avg} | Best Test acc_B: {best_test_acc_avg}  ++\")\n",
    "    print(f\"++  Best Train acc: {best_train_acc} | Best Test acc: {best_test_acc}  ++\")\n",
    "    print(f\"++++++++\" * 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244541d2",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba3bb2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainloader, optimizer, criterion, device):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_pred = []\n",
    "    train_true = []\n",
    "    time_cost = datetime.datetime.now()\n",
    "    for batch_idx, (data, label) in tqdm(enumerate(trainloader)):\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        data = data.permute(0, 2, 1)  # so, the input data shape is [batch, 3, 1024]\n",
    "        optimizer.zero_grad()\n",
    "        logits = net(data)\n",
    "        loss = criterion(logits, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        preds = logits.max(dim=1)[1]\n",
    "\n",
    "        train_true.append(label.cpu().numpy())\n",
    "        train_pred.append(preds.detach().cpu().numpy())\n",
    "\n",
    "        total += label.size(0)\n",
    "        correct += preds.eq(label).sum().item()\n",
    "\n",
    "    time_cost = int((datetime.datetime.now() - time_cost).total_seconds())\n",
    "    train_true = np.concatenate(train_true)\n",
    "    train_pred = np.concatenate(train_pred)\n",
    "        \n",
    "    print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "    \n",
    "    return {\n",
    "        \"loss\": float(\"%.3f\" % (train_loss / (batch_idx + 1))),\n",
    "        \"acc\": float(\"%.3f\" % (100. * metrics.accuracy_score(train_true, train_pred))),\n",
    "        \"acc_avg\": float(\"%.3f\" % (100. * metrics.balanced_accuracy_score(train_true, train_pred))),\n",
    "        \"time\": time_cost\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b5a12e",
   "metadata": {},
   "source": [
    "### Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a8d723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, testloader, criterion, device):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_true = []\n",
    "    test_pred = []\n",
    "    time_cost = datetime.datetime.now()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, label) in tqdm(enumerate(testloader)):\n",
    "            #pdb.set_trace()\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            data = data.permute(0, 2, 1)\n",
    "            logits = net(data)\n",
    "            loss = criterion(logits, label)\n",
    "            test_loss += loss.item()\n",
    "            preds = logits.max(dim=1)[1]\n",
    "            test_true.append(label.cpu().numpy())\n",
    "            test_pred.append(preds.detach().cpu().numpy())\n",
    "            total += label.size(0)\n",
    "            correct += preds.eq(label).sum().item()\n",
    "\n",
    "    time_cost = int((datetime.datetime.now() - time_cost).total_seconds())\n",
    "    test_true = np.concatenate(test_true)\n",
    "    test_pred = np.concatenate(test_pred)\n",
    "    print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            % (test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "    return {\n",
    "        \"loss\": float(\"%.3f\" % (test_loss / (batch_idx + 1))),\n",
    "        \"acc\": float(\"%.3f\" % (100. * metrics.accuracy_score(test_true, test_pred))),\n",
    "        \"acc_avg\": float(\"%.3f\" % (100. * metrics.balanced_accuracy_score(test_true, test_pred))),\n",
    "        \"time\": time_cost\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b19b72c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args: Namespace(batch_size=16, checkpoint='checkpoints/pointMLP-test_jupyter-1233', epoch=400, gpu_num='1', learning_rate=0.01, min_lr=1e-05, model='pointMLP', msg='test_jupyter', num_points=512, sampler_type='farthest', seed=1233, weight_decay=0.0002, workers=8)\n",
      "==> Building model..\n",
      "==> Preparing data..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1053d0c2f2460ba85c9a62b302afcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Load given data [h5 file]\u001b[39;00m\n\u001b[1;32m     70\u001b[0m all_points_front \u001b[38;5;241m=\u001b[39m h5py\u001b[38;5;241m.\u001b[39mFile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/won/3D_data/3D/train.h5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDacon_On_the_fly\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_points_front\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampler_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m train_loader1 \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mworkers) \u001b[38;5;66;03m#h5py num_worker 0밖에 안 됨... \u001b[39;00m\n\u001b[1;32m     76\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m Dacon_On_the_fly(args\u001b[38;5;241m.\u001b[39mnum_points,val_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues, val_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues, all_points_front,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m,args\u001b[38;5;241m.\u001b[39msampler_type)\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mDacon_On_the_fly.__init__\u001b[0;34m(self, num_points, id_list, label_list, point_list, partition, sampler_type)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler_type \u001b[38;5;241m=\u001b[39m sampler_type\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#######################################\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoint_list_dict \u001b[38;5;241m=\u001b[39m {i:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoint_list[\u001b[38;5;28mstr\u001b[39m(i)][:]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(id_list)}\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler_type \u001b[38;5;241m=\u001b[39m sampler_type\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#######################################\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoint_list_dict \u001b[38;5;241m=\u001b[39m {i:\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoint_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(id_list)}\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/VAD38/lib/python3.8/site-packages/h5py/_hl/dataset.py:573\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    571\u001b[0m mspace \u001b[38;5;241m=\u001b[39m h5s\u001b[38;5;241m.\u001b[39mcreate_simple(mshape)\n\u001b[1;32m    572\u001b[0m fspace \u001b[38;5;241m=\u001b[39m selection\u001b[38;5;241m.\u001b[39mid\n\u001b[0;32m--> 573\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdxpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dxpl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;66;03m# Patch up the output for NumPy\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(names) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7601cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81e9040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da029b66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
