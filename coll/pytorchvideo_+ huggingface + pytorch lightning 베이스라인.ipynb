{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5471bf37-978d-4b72-931e-bc5bab4b9225",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (1.23.5)\n",
      "Requirement already satisfied: pandas in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: decord in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from decord) (1.23.5)\n",
      "Requirement already satisfied: pytorch-lightning in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (1.9.2)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from pytorch-lightning) (1.23.5)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from pytorch-lightning) (0.11.1)\n",
      "Requirement already satisfied: lightning-utilities>=0.6.0.post0 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from pytorch-lightning) (0.6.0.post0)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from pytorch-lightning) (2023.1.0)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from pytorch-lightning) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from pytorch-lightning) (4.4.0)\n",
      "Requirement already satisfied: packaging>=17.1 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from pytorch-lightning) (22.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from pytorch-lightning) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from pytorch-lightning) (1.10.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.4)\n",
      "Requirement already satisfied: requests in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (22.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.4)\n",
      "Requirement already satisfied: pytorchvideo in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (0.1.5)\n",
      "Requirement already satisfied: av in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from pytorchvideo) (10.0.0)\n",
      "Requirement already satisfied: parameterized in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from pytorchvideo) (0.8.1)\n",
      "Requirement already satisfied: iopath in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from pytorchvideo) (0.1.10)\n",
      "Requirement already satisfied: networkx in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from pytorchvideo) (3.0)\n",
      "Requirement already satisfied: fvcore in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from pytorchvideo) (0.1.5.post20221221)\n",
      "Requirement already satisfied: numpy in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from fvcore->pytorchvideo) (1.23.5)\n",
      "Requirement already satisfied: yacs>=0.1.6 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from fvcore->pytorchvideo) (0.1.8)\n",
      "Requirement already satisfied: termcolor>=1.1 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from fvcore->pytorchvideo) (2.2.0)\n",
      "Requirement already satisfied: tabulate in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from fvcore->pytorchvideo) (0.9.0)\n",
      "Requirement already satisfied: tqdm in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from fvcore->pytorchvideo) (4.64.1)\n",
      "Requirement already satisfied: Pillow in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from fvcore->pytorchvideo) (8.3.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from fvcore->pytorchvideo) (6.0)\n",
      "Requirement already satisfied: portalocker in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from iopath->pytorchvideo) (2.7.0)\n",
      "Requirement already satisfied: typing-extensions in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from iopath->pytorchvideo) (4.4.0)\n",
      "Requirement already satisfied: scikit-learn in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (1.2.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from scikit-learn) (1.10.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scikit-multilearn in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (0.2.0)\n",
      "Requirement already satisfied: segmentation-models-pytorch in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (0.3.2)\n",
      "Requirement already satisfied: pillow in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from segmentation-models-pytorch) (8.3.2)\n",
      "Requirement already satisfied: pretrainedmodels==0.7.4 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from segmentation-models-pytorch) (0.7.4)\n",
      "Requirement already satisfied: torchvision>=0.5.0 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from segmentation-models-pytorch) (0.11.2)\n",
      "Requirement already satisfied: timm==0.6.12 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from segmentation-models-pytorch) (0.6.12)\n",
      "Requirement already satisfied: efficientnet-pytorch==0.7.1 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from segmentation-models-pytorch) (0.7.1)\n",
      "Requirement already satisfied: tqdm in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from segmentation-models-pytorch) (4.64.1)\n",
      "Requirement already satisfied: torch in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.10.1)\n",
      "Requirement already satisfied: munch in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (2.5.0)\n",
      "Requirement already satisfied: pyyaml in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from timm==0.6.12->segmentation-models-pytorch) (6.0)\n",
      "Requirement already satisfied: huggingface-hub in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from timm==0.6.12->segmentation-models-pytorch) (0.12.1)\n",
      "Requirement already satisfied: numpy in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.23.5)\n",
      "Requirement already satisfied: typing_extensions in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (4.4.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from huggingface-hub->timm==0.6.12->segmentation-models-pytorch) (2.28.1)\n",
      "Requirement already satisfied: filelock in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from huggingface-hub->timm==0.6.12->segmentation-models-pytorch) (3.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from huggingface-hub->timm==0.6.12->segmentation-models-pytorch) (22.0)\n",
      "Requirement already satisfied: six in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from requests->huggingface-hub->timm==0.6.12->segmentation-models-pytorch) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from requests->huggingface-hub->timm==0.6.12->segmentation-models-pytorch) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from requests->huggingface-hub->timm==0.6.12->segmentation-models-pytorch) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from requests->huggingface-hub->timm==0.6.12->segmentation-models-pytorch) (1.26.14)\n",
      "Requirement already satisfied: transformers in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (4.26.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: filelock in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: einops in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (0.6.0)\n",
      "Requirement already satisfied: tqdm in /home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages (4.64.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install decord\n",
    "# !pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu117\n",
    "!pip install pytorch-lightning\n",
    "!pip install pytorchvideo\n",
    "!pip install scikit-learn\n",
    "!pip install scikit-multilearn\n",
    "!pip install segmentation-models-pytorch\n",
    "!pip install transformers\n",
    "!pip install einops\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cac5a52-9eb9-4bb7-aa62-b7c419273eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/XuezheMax/apollo/blob/master/optim/apollo.py\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "\n",
    "class Apollo(Optimizer):\n",
    "    r\"\"\"Implements Atom algorithm.\n",
    "        Arguments:\n",
    "            params (iterable): iterable of parameters to optimize or dicts defining\n",
    "                parameter groups\n",
    "            lr (float): learning rate\n",
    "            beta (float, optional): coefficient used for computing running averages of gradient (default: 0.9)\n",
    "            eps (float, optional): term added to the denominator to improve numerical stability (default: 1e-4)\n",
    "            rebound (str, optional): recified bound for diagonal hessian:\n",
    "                ``'constant'`` | ``'belief'`` (default: None)\n",
    "            warmup (int, optional): number of warmup steps (default: 500)\n",
    "            init_lr (float, optional): initial learning rate for warmup (default: lr/1000)\n",
    "            weight_decay (float, optional): weight decay coefficient (default: 0)\n",
    "            weight_decay_type (str, optional): type of weight decay:\n",
    "                ``'L2'`` | ``'decoupled'`` | ``'stable'`` (default: 'L2')\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr, beta=0.9, eps=1e-4, rebound='constant', warmup=500, init_lr=None, weight_decay=0, weight_decay_type=None):\n",
    "        if not 0.0 < lr:\n",
    "            raise ValueError(\"Invalid learning rate value: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= beta < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(beta))\n",
    "        if rebound not in ['constant', 'belief']:\n",
    "            raise ValueError(\"Invalid recitifed bound: {}\".format(rebound))\n",
    "        if not 0.0 <= warmup:\n",
    "            raise ValueError(\"Invalid warmup updates: {}\".format(warmup))\n",
    "        if init_lr is None:\n",
    "            init_lr = lr / 1000\n",
    "        if not 0.0 <= init_lr <= lr:\n",
    "            raise ValueError(\"Invalid initial learning rate: {}\".format(init_lr))\n",
    "        if not 0.0 <= weight_decay:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "        if weight_decay_type is None:\n",
    "            weight_decay_type = 'L2' if rebound == 'constant' else 'decoupled'\n",
    "        if weight_decay_type not in ['L2', 'decoupled', 'stable']:\n",
    "            raise ValueError(\"Invalid weight decay type: {}\".format(weight_decay_type))\n",
    "\n",
    "        defaults = dict(lr=lr, beta=beta, eps=eps, rebound=rebound,\n",
    "                        warmup=warmup, init_lr=init_lr, base_lr=lr,\n",
    "                        weight_decay=weight_decay, weight_decay_type=weight_decay_type)\n",
    "        super(Apollo, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Apollo, self).__setstate__(state)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg_grad'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['approx_hessian'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    # Previous update direction\n",
    "                    state['update'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "\n",
    "                # Calculate current lr\n",
    "                if state['step'] < group['warmup']:\n",
    "                    curr_lr = (group['base_lr'] - group['init_lr']) * state['step'] / group['warmup'] + group['init_lr']\n",
    "                else:\n",
    "                    curr_lr = group['lr']\n",
    "\n",
    "                # Perform optimization step\n",
    "                grad = p.grad\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Atom does not support sparse gradients.')\n",
    "\n",
    "                # Perform step weight decay\n",
    "                if group['weight_decay'] != 0 and group['weight_decay_type'] == 'L2':\n",
    "                    grad = grad.add(p, alpha=group['weight_decay'])\n",
    "\n",
    "                beta = group['beta']\n",
    "                eps = group['eps']\n",
    "                exp_avg_grad = state['exp_avg_grad']\n",
    "                B = state['approx_hessian']\n",
    "                d_p = state['update']\n",
    "\n",
    "                state['step'] += 1\n",
    "                bias_correction = 1 - beta ** state['step']\n",
    "                alpha = (1 - beta) / bias_correction\n",
    "\n",
    "                # calc the diff grad\n",
    "                delta_grad = grad - exp_avg_grad\n",
    "                if group['rebound'] == 'belief':\n",
    "                    rebound = delta_grad.norm(p=np.inf)\n",
    "                else:\n",
    "                    rebound = 0.01\n",
    "                    eps = eps / rebound\n",
    "\n",
    "                # Update the running average grad\n",
    "                exp_avg_grad.add_(delta_grad, alpha=alpha)\n",
    "\n",
    "                denom = d_p.norm(p=4).add(eps)\n",
    "                d_p.div_(denom)\n",
    "                v_sq = d_p.mul(d_p)\n",
    "                delta = delta_grad.div_(denom).mul_(d_p).sum().mul(-alpha) - B.mul(v_sq).sum()\n",
    "\n",
    "                # Update B\n",
    "                B.addcmul_(v_sq, delta)\n",
    "\n",
    "                # calc direction of parameter updates\n",
    "                if group['rebound'] == 'belief':\n",
    "                    denom = torch.max(B.abs(), rebound).add_(eps / alpha)\n",
    "                else:\n",
    "                    denom = B.abs().clamp_(min=rebound)\n",
    "\n",
    "                d_p.copy_(exp_avg_grad.div(denom))\n",
    "\n",
    "                # Perform step weight decay\n",
    "                if group['weight_decay'] != 0 and group['weight_decay_type'] != 'L2':\n",
    "                    if group['weight_decay_type'] == 'stable':\n",
    "                        weight_decay = group['weight_decay'] / denom.mean().item()\n",
    "                    else:\n",
    "                        weight_decay = group['weight_decay']\n",
    "                    d_p.add_(p, alpha=weight_decay)\n",
    "\n",
    "                p.add_(d_p, alpha=-curr_lr)\n",
    "\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe8e4a2b-70dd-4bcc-9eca-6af9b217e09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/issamemari/pytorch-multilabel-balanced-sampler/blob/master/sampler.py\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "\n",
    "class MultilabelBalancedRandomSampler(Sampler):\n",
    "    \"\"\"\n",
    "    MultilabelBalancedRandomSampler: Given a multilabel dataset of length n_samples and\n",
    "    number of classes n_classes, samples from the data with equal probability per class\n",
    "    effectively oversampling minority classes and undersampling majority classes at the\n",
    "    same time. Note that using this sampler does not guarantee that the distribution of\n",
    "    classes in the output samples will be uniform, since the dataset is multilabel and\n",
    "    sampling is based on a single class. This does however guarantee that all classes\n",
    "    will have at least batch_size / n_classes samples as batch_size approaches infinity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, labels, indices=None, class_choice=\"least_sampled\"):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "            labels: a multi-hot encoding numpy array of shape (n_samples, n_classes)\n",
    "            indices: an arbitrary-length 1-dimensional numpy array representing a list\n",
    "            of indices to sample only from\n",
    "            class_choice: a string indicating how class will be selected for every\n",
    "            sample:\n",
    "                \"least_sampled\": class with the least number of sampled labels so far\n",
    "                \"random\": class is chosen uniformly at random\n",
    "                \"cycle\": the sampler cycles through the classes sequentially\n",
    "        \"\"\"\n",
    "        self.labels = labels\n",
    "        self.indices = indices\n",
    "        if self.indices is None:\n",
    "            self.indices = range(len(labels))\n",
    "\n",
    "        self.num_classes = self.labels.shape[1]\n",
    "\n",
    "        # List of lists of example indices per class\n",
    "        self.class_indices = []\n",
    "        for class_ in range(self.num_classes):\n",
    "            lst = np.where(self.labels[:, class_] == 1)[0]\n",
    "            lst = lst[np.isin(lst, self.indices)]\n",
    "            self.class_indices.append(lst)\n",
    "\n",
    "        self.counts = [0] * self.num_classes\n",
    "\n",
    "        assert class_choice in [\"least_sampled\", \"random\", \"cycle\"]\n",
    "        self.class_choice = class_choice\n",
    "        self.current_class = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.count = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.count >= len(self.indices):\n",
    "            raise StopIteration\n",
    "        self.count += 1\n",
    "        return self.sample()\n",
    "\n",
    "    def sample(self):\n",
    "        class_ = self.get_class()\n",
    "        class_indices = self.class_indices[class_]\n",
    "        chosen_index = np.random.choice(class_indices)\n",
    "        if self.class_choice == \"least_sampled\":\n",
    "            for class_, indicator in enumerate(self.labels[chosen_index]):\n",
    "                if indicator == 1:\n",
    "                    self.counts[class_] += 1\n",
    "        return chosen_index\n",
    "\n",
    "    def get_class(self):\n",
    "        if self.class_choice == \"random\":\n",
    "            class_ = random.randint(0, self.labels.shape[1] - 1)\n",
    "        elif self.class_choice == \"cycle\":\n",
    "            class_ = self.current_class\n",
    "            self.current_class = (self.current_class + 1) % self.labels.shape[1]\n",
    "        elif self.class_choice == \"least_sampled\":\n",
    "            min_count = self.counts[0]\n",
    "            min_classes = [0]\n",
    "            for class_ in range(1, self.num_classes):\n",
    "                if self.counts[class_] < min_count:\n",
    "                    min_count = self.counts[class_]\n",
    "                    min_classes = [class_]\n",
    "                if self.counts[class_] == min_count:\n",
    "                    min_classes.append(class_)\n",
    "            class_ = np.random.choice(min_classes)\n",
    "        return class_\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaf9651e-e42f-4b43-a52c-54d8b6fb58bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from einops import rearrange\n",
    "from decord import VideoReader\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from segmentation_models_pytorch.losses import FocalLoss\n",
    "from transformers import AutoModel, AutoImageProcessor, AutoConfig\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorchvideo.transforms.transforms_factory import create_video_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2f4247e-f74d-4323-8005-242038d992a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"seed\":2023,\n",
    "    \"model_name\":\"facebook/timesformer-base-finetuned-k400\",#MCG-NJU/videomae-base\n",
    "    \"batch_size\":2,\n",
    "    \"learning_rate\":1e-5,\n",
    "    \"data_dir\":'./data',\n",
    "    \"checkpoint_dir\":'./checkpoint',\n",
    "    \"submission_dir\":'./submission',\n",
    "    \"n_classes\":(2,3,4,3),\n",
    "    \"label_dict\":{\n",
    "        -1:[-1,-1,-1,-1],\n",
    "        0:[0,0,0,0],\n",
    "        1:[1,1,1,1],\n",
    "        2:[1,1,1,2],\n",
    "        3:[1,1,2,1],\n",
    "        4:[1,1,2,2],\n",
    "        5:[1,1,3,1],\n",
    "        6:[1,1,3,2],\n",
    "        7:[1,2,1,1],\n",
    "        8:[1,2,1,2],\n",
    "        9:[1,2,2,1],\n",
    "        10:[1,2,2,2],\n",
    "        11:[1,2,3,1],\n",
    "        12:[1,2,3,2]\n",
    "    },\n",
    "    \"label_reverse_dict\":{\n",
    "        (0,0,0,0):0,\n",
    "        (1,1,1,1):1,\n",
    "        (1,1,1,2):2,\n",
    "        (1,1,2,1):3,\n",
    "        (1,1,2,2):4,\n",
    "        (1,1,3,1):5,\n",
    "        (1,1,3,2):6,\n",
    "        (1,2,1,1):7,\n",
    "        (1,2,1,2):8,\n",
    "        (1,2,2,1):9,\n",
    "        (1,2,2,2):10,\n",
    "        (1,2,3,1):11,\n",
    "        (1,2,3,2):12,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7430352-4e4d-414b-a180-ec554b3c9441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 2023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2023"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0a1692b-89ab-4739-a668-a191d43d85d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f\"{config['data_dir']}/train.csv\")\n",
    "test_df = pd.read_csv(f\"{config['data_dir']}/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c12e5f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>video_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./train/TRAIN_0000.mp4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./train/TRAIN_0001.mp4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>./train/TRAIN_0002.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>./train/TRAIN_0003.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./train/TRAIN_0004.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>TRAIN_2693</td>\n",
       "      <td>./train/TRAIN_2693.mp4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694</th>\n",
       "      <td>TRAIN_2694</td>\n",
       "      <td>./train/TRAIN_2694.mp4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>TRAIN_2695</td>\n",
       "      <td>./train/TRAIN_2695.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>TRAIN_2696</td>\n",
       "      <td>./train/TRAIN_2696.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>TRAIN_2697</td>\n",
       "      <td>./train/TRAIN_2697.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2698 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sample_id              video_path  label\n",
       "0     TRAIN_0000  ./train/TRAIN_0000.mp4      7\n",
       "1     TRAIN_0001  ./train/TRAIN_0001.mp4      7\n",
       "2     TRAIN_0002  ./train/TRAIN_0002.mp4      0\n",
       "3     TRAIN_0003  ./train/TRAIN_0003.mp4      0\n",
       "4     TRAIN_0004  ./train/TRAIN_0004.mp4      1\n",
       "...          ...                     ...    ...\n",
       "2693  TRAIN_2693  ./train/TRAIN_2693.mp4      3\n",
       "2694  TRAIN_2694  ./train/TRAIN_2694.mp4      5\n",
       "2695  TRAIN_2695  ./train/TRAIN_2695.mp4      0\n",
       "2696  TRAIN_2696  ./train/TRAIN_2696.mp4      0\n",
       "2697  TRAIN_2697  ./train/TRAIN_2697.mp4      0\n",
       "\n",
       "[2698 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04928136-a408-4278-acbb-6284073c853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['sample_id'] = train_df['sample_id'].apply(lambda x: int(x.split('_')[1]))\n",
    "test_df['sample_id'] = test_df['sample_id'].apply(lambda x: int(x.split('_')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "290b7ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>video_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>./train/TRAIN_0000.mp4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>./train/TRAIN_0001.mp4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>./train/TRAIN_0002.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>./train/TRAIN_0003.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>./train/TRAIN_0004.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>2693</td>\n",
       "      <td>./train/TRAIN_2693.mp4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694</th>\n",
       "      <td>2694</td>\n",
       "      <td>./train/TRAIN_2694.mp4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>2695</td>\n",
       "      <td>./train/TRAIN_2695.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>2696</td>\n",
       "      <td>./train/TRAIN_2696.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>2697</td>\n",
       "      <td>./train/TRAIN_2697.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2698 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sample_id              video_path  label\n",
       "0             0  ./train/TRAIN_0000.mp4      7\n",
       "1             1  ./train/TRAIN_0001.mp4      7\n",
       "2             2  ./train/TRAIN_0002.mp4      0\n",
       "3             3  ./train/TRAIN_0003.mp4      0\n",
       "4             4  ./train/TRAIN_0004.mp4      1\n",
       "...         ...                     ...    ...\n",
       "2693       2693  ./train/TRAIN_2693.mp4      3\n",
       "2694       2694  ./train/TRAIN_2694.mp4      5\n",
       "2695       2695  ./train/TRAIN_2695.mp4      0\n",
       "2696       2696  ./train/TRAIN_2696.mp4      0\n",
       "2697       2697  ./train/TRAIN_2697.mp4      0\n",
       "\n",
       "[2698 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a728f070-8d8f-4961-9258-bb482aea315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['video_path'] = train_df['video_path'].apply(lambda x: config['data_dir'] + x[1:])\n",
    "test_df['video_path'] = test_df['video_path'].apply(lambda x: config['data_dir'] + x[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6c78068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>video_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>./data/train/TRAIN_0000.mp4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>./data/train/TRAIN_0001.mp4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>./data/train/TRAIN_0002.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>./data/train/TRAIN_0003.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>./data/train/TRAIN_0004.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>2693</td>\n",
       "      <td>./data/train/TRAIN_2693.mp4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694</th>\n",
       "      <td>2694</td>\n",
       "      <td>./data/train/TRAIN_2694.mp4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>2695</td>\n",
       "      <td>./data/train/TRAIN_2695.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>2696</td>\n",
       "      <td>./data/train/TRAIN_2696.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>2697</td>\n",
       "      <td>./data/train/TRAIN_2697.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2698 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sample_id                   video_path  label\n",
       "0             0  ./data/train/TRAIN_0000.mp4      7\n",
       "1             1  ./data/train/TRAIN_0001.mp4      7\n",
       "2             2  ./data/train/TRAIN_0002.mp4      0\n",
       "3             3  ./data/train/TRAIN_0003.mp4      0\n",
       "4             4  ./data/train/TRAIN_0004.mp4      1\n",
       "...         ...                          ...    ...\n",
       "2693       2693  ./data/train/TRAIN_2693.mp4      3\n",
       "2694       2694  ./data/train/TRAIN_2694.mp4      5\n",
       "2695       2695  ./data/train/TRAIN_2695.mp4      0\n",
       "2696       2696  ./data/train/TRAIN_2696.mp4      0\n",
       "2697       2697  ./data/train/TRAIN_2697.mp4      0\n",
       "\n",
       "[2698 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0403f478-2bb1-4ab6-8589-86d1ffc8e7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['label']=-1\n",
    "test_df['label_split'] = test_df['label'].apply(config['label_dict'].get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1267ff48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>video_path</th>\n",
       "      <th>label</th>\n",
       "      <th>label_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>./data/test/TEST_0000.mp4</td>\n",
       "      <td>-1</td>\n",
       "      <td>[-1, -1, -1, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>./data/test/TEST_0001.mp4</td>\n",
       "      <td>-1</td>\n",
       "      <td>[-1, -1, -1, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>./data/test/TEST_0002.mp4</td>\n",
       "      <td>-1</td>\n",
       "      <td>[-1, -1, -1, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>./data/test/TEST_0003.mp4</td>\n",
       "      <td>-1</td>\n",
       "      <td>[-1, -1, -1, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>./data/test/TEST_0004.mp4</td>\n",
       "      <td>-1</td>\n",
       "      <td>[-1, -1, -1, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>1795</td>\n",
       "      <td>./data/test/TEST_1795.mp4</td>\n",
       "      <td>-1</td>\n",
       "      <td>[-1, -1, -1, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>1796</td>\n",
       "      <td>./data/test/TEST_1796.mp4</td>\n",
       "      <td>-1</td>\n",
       "      <td>[-1, -1, -1, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>1797</td>\n",
       "      <td>./data/test/TEST_1797.mp4</td>\n",
       "      <td>-1</td>\n",
       "      <td>[-1, -1, -1, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>1798</td>\n",
       "      <td>./data/test/TEST_1798.mp4</td>\n",
       "      <td>-1</td>\n",
       "      <td>[-1, -1, -1, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>1799</td>\n",
       "      <td>./data/test/TEST_1799.mp4</td>\n",
       "      <td>-1</td>\n",
       "      <td>[-1, -1, -1, -1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1800 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sample_id                 video_path  label       label_split\n",
       "0             0  ./data/test/TEST_0000.mp4     -1  [-1, -1, -1, -1]\n",
       "1             1  ./data/test/TEST_0001.mp4     -1  [-1, -1, -1, -1]\n",
       "2             2  ./data/test/TEST_0002.mp4     -1  [-1, -1, -1, -1]\n",
       "3             3  ./data/test/TEST_0003.mp4     -1  [-1, -1, -1, -1]\n",
       "4             4  ./data/test/TEST_0004.mp4     -1  [-1, -1, -1, -1]\n",
       "...         ...                        ...    ...               ...\n",
       "1795       1795  ./data/test/TEST_1795.mp4     -1  [-1, -1, -1, -1]\n",
       "1796       1796  ./data/test/TEST_1796.mp4     -1  [-1, -1, -1, -1]\n",
       "1797       1797  ./data/test/TEST_1797.mp4     -1  [-1, -1, -1, -1]\n",
       "1798       1798  ./data/test/TEST_1798.mp4     -1  [-1, -1, -1, -1]\n",
       "1799       1799  ./data/test/TEST_1799.mp4     -1  [-1, -1, -1, -1]\n",
       "\n",
       "[1800 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ba29cd0-c1da-43ab-a68c-b09688c70576",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['label_split'] = train_df['label'].apply(config['label_dict'].get)\n",
    "train_label_split = np.array(train_df['label_split'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1090254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>video_path</th>\n",
       "      <th>label</th>\n",
       "      <th>label_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>./data/train/TRAIN_0000.mp4</td>\n",
       "      <td>7</td>\n",
       "      <td>[1, 2, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>./data/train/TRAIN_0001.mp4</td>\n",
       "      <td>7</td>\n",
       "      <td>[1, 2, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>./data/train/TRAIN_0002.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>./data/train/TRAIN_0003.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>./data/train/TRAIN_0004.mp4</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>2693</td>\n",
       "      <td>./data/train/TRAIN_2693.mp4</td>\n",
       "      <td>3</td>\n",
       "      <td>[1, 1, 2, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694</th>\n",
       "      <td>2694</td>\n",
       "      <td>./data/train/TRAIN_2694.mp4</td>\n",
       "      <td>5</td>\n",
       "      <td>[1, 1, 3, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>2695</td>\n",
       "      <td>./data/train/TRAIN_2695.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>2696</td>\n",
       "      <td>./data/train/TRAIN_2696.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>2697</td>\n",
       "      <td>./data/train/TRAIN_2697.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2698 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sample_id                   video_path  label   label_split\n",
       "0             0  ./data/train/TRAIN_0000.mp4      7  [1, 2, 1, 1]\n",
       "1             1  ./data/train/TRAIN_0001.mp4      7  [1, 2, 1, 1]\n",
       "2             2  ./data/train/TRAIN_0002.mp4      0  [0, 0, 0, 0]\n",
       "3             3  ./data/train/TRAIN_0003.mp4      0  [0, 0, 0, 0]\n",
       "4             4  ./data/train/TRAIN_0004.mp4      1  [1, 1, 1, 1]\n",
       "...         ...                          ...    ...           ...\n",
       "2693       2693  ./data/train/TRAIN_2693.mp4      3  [1, 1, 2, 1]\n",
       "2694       2694  ./data/train/TRAIN_2694.mp4      5  [1, 1, 3, 1]\n",
       "2695       2695  ./data/train/TRAIN_2695.mp4      0  [0, 0, 0, 0]\n",
       "2696       2696  ./data/train/TRAIN_2696.mp4      0  [0, 0, 0, 0]\n",
       "2697       2697  ./data/train/TRAIN_2697.mp4      0  [0, 0, 0, 0]\n",
       "\n",
       "[2698 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7eac6350-eef7-4f10-b9e1-65ed21fd00ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_multi_hot = np.hstack([np.eye(n_class, dtype=np.int32)[train_label_split[:,idx]] for idx, n_class in enumerate(config['n_classes'])])\n",
    "train_df['label_multi_hot'] = train_label_multi_hot.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9d51128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>video_path</th>\n",
       "      <th>label</th>\n",
       "      <th>label_split</th>\n",
       "      <th>label_multi_hot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>./data/train/TRAIN_0000.mp4</td>\n",
       "      <td>7</td>\n",
       "      <td>[1, 2, 1, 1]</td>\n",
       "      <td>[0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>./data/train/TRAIN_0001.mp4</td>\n",
       "      <td>7</td>\n",
       "      <td>[1, 2, 1, 1]</td>\n",
       "      <td>[0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>./data/train/TRAIN_0002.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>./data/train/TRAIN_0003.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>./data/train/TRAIN_0004.mp4</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 1]</td>\n",
       "      <td>[0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>2693</td>\n",
       "      <td>./data/train/TRAIN_2693.mp4</td>\n",
       "      <td>3</td>\n",
       "      <td>[1, 1, 2, 1]</td>\n",
       "      <td>[0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694</th>\n",
       "      <td>2694</td>\n",
       "      <td>./data/train/TRAIN_2694.mp4</td>\n",
       "      <td>5</td>\n",
       "      <td>[1, 1, 3, 1]</td>\n",
       "      <td>[0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>2695</td>\n",
       "      <td>./data/train/TRAIN_2695.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>2696</td>\n",
       "      <td>./data/train/TRAIN_2696.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>2697</td>\n",
       "      <td>./data/train/TRAIN_2697.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2698 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sample_id                   video_path  label   label_split  \\\n",
       "0             0  ./data/train/TRAIN_0000.mp4      7  [1, 2, 1, 1]   \n",
       "1             1  ./data/train/TRAIN_0001.mp4      7  [1, 2, 1, 1]   \n",
       "2             2  ./data/train/TRAIN_0002.mp4      0  [0, 0, 0, 0]   \n",
       "3             3  ./data/train/TRAIN_0003.mp4      0  [0, 0, 0, 0]   \n",
       "4             4  ./data/train/TRAIN_0004.mp4      1  [1, 1, 1, 1]   \n",
       "...         ...                          ...    ...           ...   \n",
       "2693       2693  ./data/train/TRAIN_2693.mp4      3  [1, 1, 2, 1]   \n",
       "2694       2694  ./data/train/TRAIN_2694.mp4      5  [1, 1, 3, 1]   \n",
       "2695       2695  ./data/train/TRAIN_2695.mp4      0  [0, 0, 0, 0]   \n",
       "2696       2696  ./data/train/TRAIN_2696.mp4      0  [0, 0, 0, 0]   \n",
       "2697       2697  ./data/train/TRAIN_2697.mp4      0  [0, 0, 0, 0]   \n",
       "\n",
       "                           label_multi_hot  \n",
       "0     [0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0]  \n",
       "1     [0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0]  \n",
       "2     [1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]  \n",
       "3     [1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]  \n",
       "4     [0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0]  \n",
       "...                                    ...  \n",
       "2693  [0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0]  \n",
       "2694  [0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0]  \n",
       "2695  [1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]  \n",
       "2696  [1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]  \n",
       "2697  [1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]  \n",
       "\n",
       "[2698 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4be86239-8eb6-4a4d-9b7c-08e5ae197ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_for_dataset, _ , val_df_for_dataset, _  = iterative_train_test_split(X=train_df.values, y=train_label_multi_hot, test_size=0.2)\n",
    "test_df_for_dataset = test_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "132b5c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, './data/train/TRAIN_0001.mp4', 7, list([1, 2, 1, 1]),\n",
       "        list([0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0])],\n",
       "       [3, './data/train/TRAIN_0003.mp4', 0, list([0, 0, 0, 0]),\n",
       "        list([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0])],\n",
       "       [4, './data/train/TRAIN_0004.mp4', 1, list([1, 1, 1, 1]),\n",
       "        list([0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0])],\n",
       "       ...,\n",
       "       [2695, './data/train/TRAIN_2695.mp4', 0, list([0, 0, 0, 0]),\n",
       "        list([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0])],\n",
       "       [2696, './data/train/TRAIN_2696.mp4', 0, list([0, 0, 0, 0]),\n",
       "        list([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0])],\n",
       "       [2697, './data/train/TRAIN_2697.mp4', 0, list([0, 0, 0, 0]),\n",
       "        list([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0])]], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_for_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b69d041-f0d9-4ba9-94df-e70aedb58319",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_multi_hot_for_sampler = np.array(train_df_for_dataset[:,4].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ae18b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2158, 12)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_multi_hot_for_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58f4029b-f6e6-4b17-82c3-fb898fcec84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, df_for_dataset, transform=None):\n",
    "        self.sample_id = df_for_dataset[:,0]\n",
    "        self.video_path = df_for_dataset[:,1]\n",
    "        self.label = df_for_dataset[:,2]\n",
    "        self.label_split = np.array(df_for_dataset[:,3].tolist())\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_id)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_id = self.sample_id[idx]\n",
    "        video_path = self.video_path[idx]\n",
    "        vr = VideoReader(video_path)\n",
    "        video = torch.from_numpy(vr.get_batch(range(50)).asnumpy())\n",
    "        video = rearrange(video, 't h w c -> c t h w')\n",
    "        label = self.label[idx]\n",
    "        label_split = self.label_split[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            video = self.transform(video)\n",
    "        video = rearrange(video, 'c t h w -> t c h w')\n",
    "\n",
    "        sample = {\n",
    "            'sample_id':sample_id,\n",
    "            'video':video,\n",
    "            'label':label,\n",
    "            'label_split':label_split\n",
    "        }\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d84d5626-6b8b-4ea9-81a8-3320337d93d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = AutoConfig.from_pretrained(config['model_name'])\n",
    "image_processor_config = AutoImageProcessor.from_pretrained(config['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7612a650-4c4d-4f1a-a7f7-11c22ff6b39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = create_video_transform(\n",
    "    mode='train',\n",
    "    num_samples=model_config.num_frames,\n",
    "    video_mean = tuple(image_processor_config.image_mean),\n",
    "    video_std = tuple(image_processor_config.image_std),\n",
    "    crop_size = tuple(image_processor_config.crop_size.values())\n",
    ")\n",
    "\n",
    "val_transform = create_video_transform(\n",
    "    mode='val',\n",
    "    num_samples=model_config.num_frames,\n",
    "    video_mean = tuple(image_processor_config.image_mean),\n",
    "    video_std = tuple(image_processor_config.image_std),\n",
    "    crop_size = tuple(image_processor_config.crop_size.values())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d1b230d-4ca8-43ce-b7ea-0003c8352f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VideoDataset(train_df_for_dataset, transform=train_transform)\n",
    "val_dataset = VideoDataset(val_df_for_dataset, transform=val_transform)\n",
    "test_dataset = VideoDataset(test_df_for_dataset, transform=val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4b25026-b607-4f60-a500-49d1ee93fb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = MultilabelBalancedRandomSampler(train_multi_hot_for_sampler)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size= config['batch_size'], sampler=train_sampler)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = config['batch_size']*2)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = config['batch_size']*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69747543-ce8b-4f0c-aa58-de46c616490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLVideoModel(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.model = AutoModel.from_pretrained(config['model_name'])\n",
    "        self.classifiers = nn.ModuleList([\n",
    "            nn.LazyLinear(n_class) for n_class in config['n_classes']\n",
    "        ])\n",
    "        self.loss = FocalLoss('multiclass')\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.size())\n",
    "        x = self.model(x).last_hidden_state.mean(dim=1)\n",
    "        print(x.size())\n",
    "        x_out = [classifier(x) for classifier in self.classifiers]\n",
    "        print(x_out)\n",
    "        return x_out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        video, label, label_split = batch['video'], batch['label'], batch['label_split']\n",
    "        y_hats = self.forward(batch[\"video\"])\n",
    "        loss = sum([self.loss(y_hats[i], batch[\"label_split\"][:,i]) for i in range(len(self.config['n_classes']))])\n",
    "        loss = loss/len(self.config['n_classes'])\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        video, label, label_split = batch['video'], batch['label'], batch['label_split']\n",
    "        y_hats = self.forward(batch[\"video\"])\n",
    "        step_output = [*y_hats, label]\n",
    "        return step_output\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        video, _, _ = batch['video'], batch['label'], batch['label_split']\n",
    "        y_hats = self.forward(batch[\"video\"])\n",
    "        step_output = y_hats\n",
    "        return step_output\n",
    "\n",
    "    def validation_epoch_end(self, step_outputs):\n",
    "        pred1, pred2, pred3, pred4, label = [], [], [], [], []\n",
    "        for step_output in step_outputs:\n",
    "            pred1.append(step_output[0])\n",
    "            pred2.append(step_output[1])\n",
    "            pred3.append(step_output[2])\n",
    "            pred4.append(step_output[3])\n",
    "            label.append(step_output[4])\n",
    "            \n",
    "        pred1 = torch.cat(pred1).argmax(1)\n",
    "        pred2 = torch.cat(pred2).argmax(1)\n",
    "        pred3 = torch.cat(pred3).argmax(1)\n",
    "        pred4 = torch.cat(pred4).argmax(1)\n",
    "        label = torch.cat(label).tolist()\n",
    "\n",
    "        pred = torch.stack([pred1,pred2,pred3,pred4],dim=1).cpu().detach().numpy().tolist()\n",
    "        pred = list(map(lambda x: self.config['label_reverse_dict'].get(tuple(x),0),pred))\n",
    "        \n",
    "        score = f1_score(label,pred, average='macro')\n",
    "        print(score)\n",
    "        self.log(\"val_score\", score)\n",
    "        return score\n",
    "    \n",
    "    def post_preproc(self, step_outputs):\n",
    "        pred1, pred2, pred3, pred4 = [], [], [], []\n",
    "        for step_output in step_outputs:\n",
    "            pred1.append(step_output[0])\n",
    "            pred2.append(step_output[1])\n",
    "            pred3.append(step_output[2])\n",
    "            pred4.append(step_output[3])\n",
    "            \n",
    "        pred1 = torch.cat(pred1).argmax(1)\n",
    "        pred2 = torch.cat(pred2).argmax(1)\n",
    "        pred3 = torch.cat(pred3).argmax(1)\n",
    "        pred4 = torch.cat(pred4).argmax(1)\n",
    "\n",
    "        pred = torch.stack([pred1,pred2,pred3,pred4],dim=1).cpu().detach().numpy().tolist()\n",
    "        pred = list(map(lambda x: self.config['label_reverse_dict'].get(tuple(x),0),pred))\n",
    "\n",
    "        return pred\n",
    "            \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Apollo(self.parameters(), lr=self.learning_rate)\n",
    "        return [optimizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dbd5dbf9-b7f7-404b-9843-faae40e7eb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/timesformer-base-finetuned-k400 were not used when initializing TimesformerModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing TimesformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TimesformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Using 16bit None Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | model       | TimesformerModel | 121 M \n",
      "1 | classifiers | ModuleList       | 0     \n",
      "2 | loss        | FocalLoss        | 0     \n",
      "-------------------------------------------------\n",
      "121 M     Trainable params\n",
      "0         Non-trainable params\n",
      "121 M     Total params\n",
      "242.518   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 3, 224, 224])\n",
      "torch.Size([4, 768])\n",
      "[tensor([[-0.3755,  0.1315],\n",
      "        [ 0.1779,  0.1495],\n",
      "        [-0.1920,  0.6040],\n",
      "        [ 0.0194,  0.1460]], device='cuda:0', dtype=torch.float16), tensor([[ 0.0178, -0.0349,  0.1082],\n",
      "        [ 0.8633, -0.3875,  0.4675],\n",
      "        [ 0.5112, -0.0091,  0.2064],\n",
      "        [ 0.5308, -0.0790,  0.5483]], device='cuda:0', dtype=torch.float16), tensor([[ 0.2771,  0.8252,  0.1499,  0.4822],\n",
      "        [ 0.0164,  0.3091, -0.2158, -0.0652],\n",
      "        [ 0.5723, -0.1686,  0.0879,  0.2129],\n",
      "        [-0.1787,  0.5571, -0.1542,  0.3877]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[ 0.5464, -0.4514,  0.4651],\n",
      "        [ 0.6831,  0.1516,  0.0208],\n",
      "        [ 0.2886, -0.2388,  0.0267],\n",
      "        [-0.0567, -0.4988,  0.3354]], device='cuda:0', dtype=torch.float16)]\n",
      "torch.Size([4, 8, 3, 224, 224])\n",
      "torch.Size([4, 768])\n",
      "[tensor([[-0.0512,  0.3577],\n",
      "        [ 0.0923, -0.4131],\n",
      "        [ 0.1078, -0.1378],\n",
      "        [ 0.4050, -0.8223]], device='cuda:0', dtype=torch.float16), tensor([[ 0.6831, -0.4089, -0.1787],\n",
      "        [ 0.3457, -0.5254,  0.1763],\n",
      "        [ 0.1517,  0.0179, -0.0992],\n",
      "        [ 0.1317, -0.0056,  0.3806]], device='cuda:0', dtype=torch.float16), tensor([[-0.0908,  0.6528,  0.0027,  0.6787],\n",
      "        [ 0.2856,  0.5781, -0.0546,  0.1945],\n",
      "        [ 0.0842,  0.3965,  0.5811,  0.5967],\n",
      "        [-0.1293,  0.2593,  0.1864,  0.5483]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[ 0.5503, -0.4333,  0.1428],\n",
      "        [ 0.3281, -0.4333,  0.1144],\n",
      "        [ 0.5474, -0.5688,  0.5352],\n",
      "        [ 0.4844, -0.1897,  0.1935]], device='cuda:0', dtype=torch.float16)]\n",
      "0.14545454545454545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935331b7c79347ef94c4bc078c901e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 3, 224, 224])\n",
      "torch.Size([2, 768])\n",
      "[tensor([[ 0.3740, -0.0908],\n",
      "        [ 0.1077,  0.2303]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.1600, -0.1077, -0.0716],\n",
      "        [-0.1410, -0.1024,  0.1221]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.4299, -0.3625, -0.0904,  0.2900],\n",
      "        [ 0.2208, -0.0435,  0.2352,  0.1519]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddmmBackward0>), tensor([[-0.3579, -0.1097, -0.0953],\n",
      "        [-0.1508, -0.4819,  0.1792]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<AddmmBackward0>)]\n",
      "torch.Size([2, 8, 3, 224, 224])\n",
      "torch.Size([2, 768])\n",
      "[tensor([[-0.0063,  0.3027],\n",
      "        [ 0.3606, -0.1448]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.3403, -0.1469,  0.0217],\n",
      "        [ 0.5122,  0.2395,  0.1624]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0795,  0.0496,  0.2166,  0.4387],\n",
      "        [-0.0181, -0.1208,  0.4250,  0.4019]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddmmBackward0>), tensor([[ 0.4622, -0.3262,  0.3328],\n",
      "        [ 0.3638, -0.0991,  0.7056]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<AddmmBackward0>)]\n",
      "torch.Size([2, 8, 3, 224, 224])\n",
      "torch.Size([2, 768])\n",
      "[tensor([[ 0.1235, -0.2380],\n",
      "        [ 0.2394,  0.1703]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.8682, -0.2218,  0.5703],\n",
      "        [ 0.2465, -0.1522,  0.0025]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0427,  0.8384, -0.4114,  0.2255],\n",
      "        [-0.2336,  0.2930, -0.1084,  0.3760]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddmmBackward0>), tensor([[-0.0888, -0.2588, -0.0060],\n",
      "        [ 0.2253, -0.2920,  0.4812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<AddmmBackward0>)]\n",
      "torch.Size([2, 8, 3, 224, 224])\n",
      "torch.Size([2, 768])\n",
      "[tensor([[ 0.2443,  0.2510],\n",
      "        [-0.2939,  0.1082]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.4060,  0.1136, -0.0150],\n",
      "        [-0.0789,  0.1907,  0.3245]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.1138,  0.5708,  0.0503,  0.4236],\n",
      "        [ 0.0808,  0.6343, -0.3447, -0.2896]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddmmBackward0>), tensor([[ 0.3047, -0.3181,  0.2168],\n",
      "        [ 0.1367, -0.5361,  0.1055]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<AddmmBackward0>)]\n",
      "torch.Size([2, 8, 3, 224, 224])\n",
      "torch.Size([2, 768])\n",
      "[tensor([[ 0.3376, -0.2798],\n",
      "        [ 0.0177,  0.3105]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0732,  0.0348,  0.2115],\n",
      "        [ 0.4077, -0.0359, -0.0215]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.2141, -0.1953,  0.3928,  0.3977],\n",
      "        [-0.3713, -0.0285,  0.6162,  0.4900]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddmmBackward0>), tensor([[ 0.0939, -0.2954, -0.0411],\n",
      "        [ 0.1696, -0.2325,  0.5493]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<AddmmBackward0>)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_score',\n",
    "    dirpath=config['checkpoint_dir'],\n",
    "    filename=f'{config[\"model_name\"]}'+'-{epoch:02d}-{train_loss:.4f}-{val_score:.4f}',\n",
    "    mode='max'\n",
    ")\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"train_loss\",\n",
    "    patience=3,\n",
    "    verbose=False,\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "pl_video_model = PLVideoModel(config)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    accelerator='auto', \n",
    "    precision=16,\n",
    "    callbacks=[early_stop_callback, checkpoint_callback]\n",
    "                    \n",
    ")\n",
    "trainer.fit(pl_video_model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "531d95da-c86f-44e5-9a56-b16505bdc5d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/server-003/workspace/competition/collision/checkpoint/facebook/timesformer-base-finetuned-k400-epoch=08-train_loss=0.1318-val_score=0.5356.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pl_video_model_pretrained \u001b[38;5;241m=\u001b[39m \u001b[43mPLVideoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./checkpoint/facebook/timesformer-base-finetuned-k400-epoch=08-train_loss=0.1318-val_score=0.5356.ckpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m pred \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(pl_video_model_pretrained, test_dataloader)\n",
      "File \u001b[0;32m~/anaconda3/envs/competition/lib/python3.8/site-packages/pytorch_lightning/core/saving.py:139\u001b[0m, in \u001b[0;36mModelIO.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     67\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:  \u001b[38;5;66;03m# type: ignore[valid-type]\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m    Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    it stores the arguments passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m        y_hat = pretrained_model(x)\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/competition/lib/python3.8/site-packages/pytorch_lightning/core/saving.py:160\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m     map_location \u001b[38;5;241m=\u001b[39m cast(_MAP_LOCATION_TYPE, \u001b[38;5;28;01mlambda\u001b[39;00m storage, loc: storage)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pl_legacy_patch():\n\u001b[0;32m--> 160\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mpl_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# convert legacy checkpoints to the new format\u001b[39;00m\n\u001b[1;32m    163\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m _pl_migrate_checkpoint(\n\u001b[1;32m    164\u001b[0m     checkpoint, checkpoint_path\u001b[38;5;241m=\u001b[39m(checkpoint_path \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(checkpoint_path, (\u001b[38;5;28mstr\u001b[39m, Path)) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    165\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/competition/lib/python3.8/site-packages/lightning_fabric/utilities/cloud_io.py:47\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(path_or_url, map_location)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload_state_dict_from_url(\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28mstr\u001b[39m(path_or_url),\n\u001b[1;32m     44\u001b[0m         map_location\u001b[38;5;241m=\u001b[39mmap_location,  \u001b[38;5;66;03m# type: ignore[arg-type] # upstream annotation is not correct\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     )\n\u001b[1;32m     46\u001b[0m fs \u001b[38;5;241m=\u001b[39m get_filesystem(path_or_url)\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mload(f, map_location\u001b[38;5;241m=\u001b[39mmap_location)\n",
      "File \u001b[0;32m~/anaconda3/envs/competition/lib/python3.8/site-packages/fsspec/spec.py:1135\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1134\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[0;32m-> 1135\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1144\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "File \u001b[0;32m~/anaconda3/envs/competition/lib/python3.8/site-packages/fsspec/implementations/local.py:183\u001b[0m, in \u001b[0;36mLocalFileSystem._open\u001b[0;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_mkdir \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent(path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLocalFileOpener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/competition/lib/python3.8/site-packages/fsspec/implementations/local.py:285\u001b[0m, in \u001b[0;36mLocalFileOpener.__init__\u001b[0;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression \u001b[38;5;241m=\u001b[39m get_compression(path, compression)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[0;32m--> 285\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/competition/lib/python3.8/site-packages/fsspec/implementations/local.py:290\u001b[0m, in \u001b[0;36mLocalFileOpener._open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf\u001b[38;5;241m.\u001b[39mclosed:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocommit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m--> 290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression:\n\u001b[1;32m    292\u001b[0m             compress \u001b[38;5;241m=\u001b[39m compr[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/server-003/workspace/competition/collision/checkpoint/facebook/timesformer-base-finetuned-k400-epoch=08-train_loss=0.1318-val_score=0.5356.ckpt'"
     ]
    }
   ],
   "source": [
    "pl_video_model_pretrained = PLVideoModel.load_from_checkpoint(\n",
    "    \"./checkpoint/facebook/timesformer-base-finetuned-k400-epoch=08-train_loss=0.1318-val_score=0.5356.ckpt\",\n",
    "    config=config\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(accelerator='auto')\n",
    "pred = trainer.predict(pl_video_model_pretrained, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae171f4-24fe-4428-94e7-e2e920b20b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_post_proc = pl_video_model_pretrained.post_preproc(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53206209-b381-44e9-a27b-815db93db13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv(f\"{config['data_dir']}/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14c78f9-1a7a-4a5b-8e32-b768e43ea392",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit['label'] = pred_post_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cc42ee-4233-496b-98df-d7b08f0abe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv(f\"{config['submission_dir']}/testsubmit.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8a2770-efb1-460a-a9be-f5f81c7311d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
