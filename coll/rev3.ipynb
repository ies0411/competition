{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1db13f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchsampler\n",
    "#https://github.com/ufoym/imbalanced-dataset-sampler\n",
    "\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     train_dataset,\n",
    "#     sampler=ImbalancedDatasetSampler(train_dataset),\n",
    "#     batch_size=args.batch_size,\n",
    "#     **kwargs\n",
    "# )\n",
    "#https://www.kaggle.com/competitions/dfl-bundesliga-data-shootout/discussion/360236\n",
    "#model kinetic version\n",
    "#label seperate\n",
    "#video augmentation\n",
    "#imbalance data\n",
    "#focal loss\n",
    "#https://github.com/HHTseng/video-classification\n",
    "#https://huggingface.co/models?other=video-classification\n",
    "#앙상블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfb75f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed9ba04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torchsampler import ImbalancedDatasetSampler\n",
    "from sklearn.model_selection import KFold\n",
    "import albumentations as A\n",
    "\n",
    "# from einops import rearrange\n",
    "# from decord import VideoReader\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from segmentation_models_pytorch.losses import FocalLoss\n",
    "# from transformers import AutoModel, AutoImageProcessor, AutoConfig\n",
    "# from skmultilearn.model_selection import iterative_train_test_split\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorchvideo.transforms.transforms_factory import create_video_transform\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a95a426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f09879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "all_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0de55a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>video_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./train/TRAIN_0000.mp4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./train/TRAIN_0001.mp4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>./train/TRAIN_0002.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>./train/TRAIN_0003.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./train/TRAIN_0004.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>TRAIN_2693</td>\n",
       "      <td>./train/TRAIN_2693.mp4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694</th>\n",
       "      <td>TRAIN_2694</td>\n",
       "      <td>./train/TRAIN_2694.mp4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>TRAIN_2695</td>\n",
       "      <td>./train/TRAIN_2695.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>TRAIN_2696</td>\n",
       "      <td>./train/TRAIN_2696.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>TRAIN_2697</td>\n",
       "      <td>./train/TRAIN_2697.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2698 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sample_id              video_path  label\n",
       "0     TRAIN_0000  ./train/TRAIN_0000.mp4      7\n",
       "1     TRAIN_0001  ./train/TRAIN_0001.mp4      7\n",
       "2     TRAIN_0002  ./train/TRAIN_0002.mp4      0\n",
       "3     TRAIN_0003  ./train/TRAIN_0003.mp4      0\n",
       "4     TRAIN_0004  ./train/TRAIN_0004.mp4      1\n",
       "...          ...                     ...    ...\n",
       "2693  TRAIN_2693  ./train/TRAIN_2693.mp4      3\n",
       "2694  TRAIN_2694  ./train/TRAIN_2694.mp4      5\n",
       "2695  TRAIN_2695  ./train/TRAIN_2695.mp4      0\n",
       "2696  TRAIN_2696  ./train/TRAIN_2696.mp4      0\n",
       "2697  TRAIN_2697  ./train/TRAIN_2697.mp4      0\n",
       "\n",
       "[2698 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90281795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1783\n",
       "1      318\n",
       "7      317\n",
       "3       78\n",
       "2       51\n",
       "9       34\n",
       "11      33\n",
       "8       30\n",
       "5       28\n",
       "4       13\n",
       "12       6\n",
       "10       4\n",
       "6        3\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b01d9001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterative_train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ef3570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CFG = {\n",
    "    'VIDEO_LENGTH':50, # 10프레임 * 5초\n",
    "    'IMG_SIZE':120,\n",
    "    'EPOCHS':30,\n",
    "    'LEARNING_RATE':3e-5,\n",
    "    'BATCH_SIZE':1,\n",
    "    'SEED':41,\n",
    "    'SPLIT':5,\n",
    "    'ROOT':'./data',\n",
    "    'MODEL':'MCG-NJU/videomae-base-finetuned-ssv2'\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b42aa213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skf = StratifiedKFold(n_splits = CFG['SPLITS']) todo kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee5d0e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['video_path'] = all_df['video_path'].apply(lambda x:CFG['ROOT']+x[1:])\n",
    "test_df['video_path'] = test_df['video_path'].apply(lambda x:CFG['ROOT']+x[1:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65e1c289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>video_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./data/train/TRAIN_0000.mp4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./data/train/TRAIN_0001.mp4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>./data/train/TRAIN_0002.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>./data/train/TRAIN_0003.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./data/train/TRAIN_0004.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>TRAIN_2693</td>\n",
       "      <td>./data/train/TRAIN_2693.mp4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694</th>\n",
       "      <td>TRAIN_2694</td>\n",
       "      <td>./data/train/TRAIN_2694.mp4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>TRAIN_2695</td>\n",
       "      <td>./data/train/TRAIN_2695.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>TRAIN_2696</td>\n",
       "      <td>./data/train/TRAIN_2696.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>TRAIN_2697</td>\n",
       "      <td>./data/train/TRAIN_2697.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2698 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sample_id                   video_path  label\n",
       "0     TRAIN_0000  ./data/train/TRAIN_0000.mp4      7\n",
       "1     TRAIN_0001  ./data/train/TRAIN_0001.mp4      7\n",
       "2     TRAIN_0002  ./data/train/TRAIN_0002.mp4      0\n",
       "3     TRAIN_0003  ./data/train/TRAIN_0003.mp4      0\n",
       "4     TRAIN_0004  ./data/train/TRAIN_0004.mp4      1\n",
       "...          ...                          ...    ...\n",
       "2693  TRAIN_2693  ./data/train/TRAIN_2693.mp4      3\n",
       "2694  TRAIN_2694  ./data/train/TRAIN_2694.mp4      5\n",
       "2695  TRAIN_2695  ./data/train/TRAIN_2695.mp4      0\n",
       "2696  TRAIN_2696  ./data/train/TRAIN_2696.mp4      0\n",
       "2697  TRAIN_2697  ./data/train/TRAIN_2697.mp4      0\n",
       "\n",
       "[2698 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b433f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_df = all_df.copy()\n",
    "crash_df['label'] = crash_df['label'].apply(lambda x: 1 if x != 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "660a98ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>video_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./data/train/TRAIN_0000.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./data/train/TRAIN_0001.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>./data/train/TRAIN_0002.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>./data/train/TRAIN_0003.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./data/train/TRAIN_0004.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>TRAIN_2693</td>\n",
       "      <td>./data/train/TRAIN_2693.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694</th>\n",
       "      <td>TRAIN_2694</td>\n",
       "      <td>./data/train/TRAIN_2694.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>TRAIN_2695</td>\n",
       "      <td>./data/train/TRAIN_2695.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>TRAIN_2696</td>\n",
       "      <td>./data/train/TRAIN_2696.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>TRAIN_2697</td>\n",
       "      <td>./data/train/TRAIN_2697.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2698 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sample_id                   video_path  label\n",
       "0     TRAIN_0000  ./data/train/TRAIN_0000.mp4      1\n",
       "1     TRAIN_0001  ./data/train/TRAIN_0001.mp4      1\n",
       "2     TRAIN_0002  ./data/train/TRAIN_0002.mp4      0\n",
       "3     TRAIN_0003  ./data/train/TRAIN_0003.mp4      0\n",
       "4     TRAIN_0004  ./data/train/TRAIN_0004.mp4      1\n",
       "...          ...                          ...    ...\n",
       "2693  TRAIN_2693  ./data/train/TRAIN_2693.mp4      1\n",
       "2694  TRAIN_2694  ./data/train/TRAIN_2694.mp4      1\n",
       "2695  TRAIN_2695  ./data/train/TRAIN_2695.mp4      0\n",
       "2696  TRAIN_2696  ./data/train/TRAIN_2696.mp4      0\n",
       "2697  TRAIN_2697  ./data/train/TRAIN_2697.mp4      0\n",
       "\n",
       "[2698 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crash_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f497d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# train_df, val_df = train_test_split(\n",
    "#     all_df, test_size=0.2, stratify=all_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fed98cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58170cb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Some weights of the model checkpoint at MCG-NJU/videomae-base-finetuned-ssv2 were not used when initializing VideoMAEModel: ['classifier.weight', 'classifier.bias', 'fc_norm.weight', 'fc_norm.bias']\n",
      "- This IS expected if you are initializing VideoMAEModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VideoMAEModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import VideoMAEConfig, VideoMAEModel\n",
    "from transformers import AutoImageProcessor, VideoMAEForVideoClassification\n",
    "\n",
    "image_processor_config = AutoImageProcessor.from_pretrained(CFG['MODEL'])\n",
    "configuration = VideoMAEConfig()\n",
    "\n",
    "\n",
    "configuration.image_size=CFG['IMG_SIZE']\n",
    "configuration.num_frames = CFG['VIDEO_LENGTH']\n",
    "# image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "model = VideoMAEModel.from_pretrained(CFG['MODEL'],config=configuration,ignore_mismatched_sizes=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3336aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoMAEConfig {\n",
       "  \"_name_or_path\": \"MCG-NJU/videomae-base-finetuned-ssv2\",\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"decoder_hidden_size\": 384,\n",
       "  \"decoder_intermediate_size\": 1536,\n",
       "  \"decoder_num_attention_heads\": 6,\n",
       "  \"decoder_num_hidden_layers\": 4,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 768,\n",
       "  \"image_size\": 120,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"model_type\": \"videomae\",\n",
       "  \"norm_pix_loss\": true,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_frames\": 50,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"patch_size\": 16,\n",
       "  \"qkv_bias\": true,\n",
       "  \"transformers_version\": \"4.26.1\",\n",
       "  \"tubelet_size\": 2,\n",
       "  \"use_mean_pooling\": true\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e5c84fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoMAEImageProcessor {\n",
       "  \"crop_size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  },\n",
       "  \"do_center_crop\": true,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.485,\n",
       "    0.456,\n",
       "    0.406\n",
       "  ],\n",
       "  \"image_processor_type\": \"VideoMAEImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.229,\n",
       "    0.224,\n",
       "    0.225\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"shortest_edge\": 224\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_processor_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4df59751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #https://torchvideo.readthedocs.io/en/latest/transforms.html#examples\n",
    "# train_transform = create_video_transform(\n",
    "#     mode='train',\n",
    "#     num_samples=model.config.num_frames,\n",
    "#     video_mean = tuple(image_processor_config.image_mean),\n",
    "#     video_std = tuple(image_processor_config.image_std),\n",
    "#     crop_size = tuple(image_processor_config.crop_size.values())\n",
    "# )\n",
    "\n",
    "# val_transform = create_video_transform(\n",
    "#     mode='val',\n",
    "#     num_samples=model.config.num_frames,\n",
    "#     video_mean = tuple(image_processor_config.image_mean),\n",
    "#     video_std = tuple(image_processor_config.image_std),\n",
    "#     crop_size = tuple(image_processor_config.crop_size.values())\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "414e3138",
   "metadata": {},
   "outputs": [],
   "source": [
    "Alb = A.Compose([\n",
    "        A.Resize(width=CFG['IMG_SIZE'], height=CFG['IMG_SIZE']),\n",
    "#         A.HorizontalFlip(p=0.5),\n",
    "#         A.ShiftScaleRotate(rotate_limit=15, p=0.3),\n",
    "#         A.ChannelDropout(p=0.1),\n",
    "#         A.RandomRain(p=0.1),\n",
    "#         A.GridDistortion(p=0.3),\n",
    "        A.Normalize()\n",
    "    ], p=1)\n",
    "\n",
    "\n",
    "def aug_video(vid, tfms):\n",
    "    seed = random.randint(0,99999)\n",
    "    aug_vid = []\n",
    "    for x in vid:\n",
    "        random.seed(seed)\n",
    "        aug_vid.append((tfms(image = np.asarray(x)))['image'])\n",
    "    return torch.from_numpy(np.stack(aug_vid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f3ffee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_path_list, label_list,transform=None):\n",
    "        self.video_path_list = video_path_list\n",
    "        self.label_list = label_list\n",
    "#         self.transform=transform\n",
    "        self.Alb = transform\n",
    "    \n",
    "    def get_labels(self):   \n",
    "        return self.label_list  \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        frames = self.get_video(self.video_path_list[index])\n",
    "        \n",
    "#         vr = VideoReader(self.video_path_list[index])\n",
    "#         video = torch.from_numpy(vr.get_batch(range(CFG['VIDEO_LENGTH'])).asnumpy())\n",
    "#         video = rearrange(video, 't h w c -> c t h w')\n",
    "        if self.label_list is not None:\n",
    "#             if self.transform:\n",
    "#                 frames = transform(frames)\n",
    "            label = self.label_list[index]\n",
    "            return frames, label\n",
    "        else:\n",
    "#             if self.transform:\n",
    "#                 frames = transform(frames)\n",
    "            return frames\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.video_path_list)\n",
    "    \n",
    "    def get_video(self, path):\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        for _ in range(CFG['VIDEO_LENGTH']):\n",
    "            _, img = cap.read()\n",
    "            img = cv2.resize(img, (CFG['IMG_SIZE'], CFG['IMG_SIZE']))\n",
    "#             img = img / 255.\n",
    "            frames.append(img)\n",
    "        if self.Alb is not None:\n",
    "            frames = aug_video(frames, tfms=self.Alb)\n",
    "#         8, 224, 224, 3\n",
    "        return torch.FloatTensor(np.array(frames)).permute(0, 3, 1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d90e7112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoMAEModel(\n",
       "  (embeddings): VideoMAEEmbeddings(\n",
       "    (patch_embeddings): VideoMAEPatchEmbeddings(\n",
       "      (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
       "    )\n",
       "  )\n",
       "  (encoder): VideoMAEEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): VideoMAELayer(\n",
       "        (attention): VideoMAEAttention(\n",
       "          (attention): VideoMAESelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VideoMAESelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VideoMAEIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): VideoMAEOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (1): VideoMAELayer(\n",
       "        (attention): VideoMAEAttention(\n",
       "          (attention): VideoMAESelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VideoMAESelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VideoMAEIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): VideoMAEOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (2): VideoMAELayer(\n",
       "        (attention): VideoMAEAttention(\n",
       "          (attention): VideoMAESelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VideoMAESelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VideoMAEIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): VideoMAEOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (3): VideoMAELayer(\n",
       "        (attention): VideoMAEAttention(\n",
       "          (attention): VideoMAESelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VideoMAESelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VideoMAEIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): VideoMAEOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (4): VideoMAELayer(\n",
       "        (attention): VideoMAEAttention(\n",
       "          (attention): VideoMAESelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VideoMAESelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VideoMAEIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): VideoMAEOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (5): VideoMAELayer(\n",
       "        (attention): VideoMAEAttention(\n",
       "          (attention): VideoMAESelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VideoMAESelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VideoMAEIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): VideoMAEOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (6): VideoMAELayer(\n",
       "        (attention): VideoMAEAttention(\n",
       "          (attention): VideoMAESelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VideoMAESelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VideoMAEIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): VideoMAEOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (7): VideoMAELayer(\n",
       "        (attention): VideoMAEAttention(\n",
       "          (attention): VideoMAESelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VideoMAESelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VideoMAEIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): VideoMAEOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (8): VideoMAELayer(\n",
       "        (attention): VideoMAEAttention(\n",
       "          (attention): VideoMAESelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VideoMAESelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VideoMAEIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): VideoMAEOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (9): VideoMAELayer(\n",
       "        (attention): VideoMAEAttention(\n",
       "          (attention): VideoMAESelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VideoMAESelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VideoMAEIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): VideoMAEOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (10): VideoMAELayer(\n",
       "        (attention): VideoMAEAttention(\n",
       "          (attention): VideoMAESelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VideoMAESelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VideoMAEIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): VideoMAEOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (11): VideoMAELayer(\n",
       "        (attention): VideoMAEAttention(\n",
       "          (attention): VideoMAESelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VideoMAESelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VideoMAEIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): VideoMAEOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a477b553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CrashClsModel(nn.Module):\n",
    "    def __init__(self,pretrained_model):\n",
    "        super().__init__()\n",
    "        self.model = pretrained_model\n",
    "        self.classifier = nn.LazyLinear(2)\n",
    "    def forward(self, x):\n",
    "#         print(x.size())\n",
    "        batch_size = x.size(0)\n",
    "        x = self.model(x)\n",
    "        x = x[0].view(batch_size,-1)\n",
    "        x = self.classifier(x)\n",
    "#         print(x.size())\n",
    "#         print(x.size())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b731a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "crash_train_df, val_df = train_test_split(\n",
    "    crash_df, test_size=0.2, stratify=crash_df['label'])\n",
    "\n",
    "train_dataset = VideoDataset(crash_train_df['video_path'].values, crash_train_df['label'].values,transform=Alb)\n",
    "#  transform=train_transform\n",
    "val_dataset = VideoDataset(val_df['video_path'].values, val_df['label'].values, transform=None)\n",
    "# ,transform=val_transform\n",
    "# test_dataset = VideoDataset(test_df_for_dataset, transform=val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bebd83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crash_sampler = np.array(crash_df['label'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "604dd1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crash_train_df=crash_train_df.rename(columns={'label':'labels'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d55e0d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crash_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "172697a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader = DataLoader(train_dataset, batch_size= 4)\n",
    "train_loader = DataLoader(train_dataset, sampler=ImbalancedDatasetSampler(train_dataset),shuffle=False,batch_size = CFG['BATCH_SIZE'],  num_workers=12)\n",
    "val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], num_workers=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05370681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "#     criterion = nn.BCELoss().to(device)\n",
    "\n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for videos, labels in tqdm(iter(train_loader)):\n",
    "            videos = videos.to(device)\n",
    "            labels = labels.to(device)\n",
    "#             print(labels)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(videos)\n",
    "#             print(output.armax(-1).item())\n",
    "            \n",
    "#             output = F.softmax(output,dim=1)[:1]\n",
    "#             print(output)\n",
    "#             print(\"==\")\n",
    "#             print(labels)\n",
    "            loss = criterion(output, labels)\n",
    "#             loss =FocalLoss(gamma=0)(output,labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "                    \n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val F1 : [{_val_score:.5f}]')\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(_val_score)\n",
    "            \n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "            torch.save(best_model.state_dict(), './'+str(epoch)+'_best_model.pth')\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51fa21fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    preds, trues = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for videos, labels in tqdm(iter(val_loader)):\n",
    "            videos = videos.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            logit = model(videos)\n",
    "            loss = criterion(logit, labels)\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "            \n",
    "            preds += logit.argmax(1).detach().cpu().numpy().tolist()\n",
    "            trues += labels.detach().cpu().numpy().tolist()\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "    \n",
    "    _val_score = f1_score(trues, preds, average='macro')\n",
    "    return _val_loss, _val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f3e8cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "video_model = CrashClsModel(model).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2,threshold_mode='abs',min_lr=1e-10, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "935c8047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 2158/2158 [05:13<00:00,  6.88it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 540/540 [00:25<00:00, 21.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.30735] Val Loss : [61.83992] Val F1 : [0.37111]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 2158/2158 [05:12<00:00,  6.90it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 540/540 [00:25<00:00, 21.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.11632] Val Loss : [108.27217] Val F1 : [0.33324]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 2158/2158 [05:12<00:00,  6.90it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 540/540 [00:25<00:00, 21.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.08989] Val Loss : [53.29246] Val F1 : [0.39256]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▍                                                                                      | 104/2158 [00:15<05:05,  6.72it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m infer_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_loader, val_loader, scheduler, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#             loss =FocalLoss(gamma=0)(output,labels)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m             loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 28\u001b[0m             \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m             train_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     32\u001b[0m         _val_loss, _val_score \u001b[38;5;241m=\u001b[39m validation(model, criterion, val_loader, device)\n",
      "File \u001b[0;32m~/anaconda3/envs/competition/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/competition/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/competition/lib/python3.8/site-packages/torch/optim/adam.py:133\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[38;5;66;03m# record the step after step update\u001b[39;00m\n\u001b[1;32m    131\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 133\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m           \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m           \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m           \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/competition/lib/python3.8/site-packages/torch/optim/_functional.py:98\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     94\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(bias_correction2))\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m     96\u001b[0m step_size \u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m/\u001b[39m bias_correction1\n\u001b[0;32m---> 98\u001b[0m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "infer_model = train(video_model, optimizer, train_loader, val_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dce525b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d95f675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f55d35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bdd37e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
