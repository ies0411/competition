{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1db13f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pseudo label -> randomrain\n",
    "#crash auto label and confidence기반 수작업 -> night\n",
    "# pip uninstall opencv-python-headless==4.5.5.62\n",
    "#pip install opencv-python-headless==4.5.2.52\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed9ba04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torchsampler import ImbalancedDatasetSampler\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "import albumentations as A\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from segmentation_models_pytorch.losses import FocalLoss\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoImageProcessor, AutoConfig\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eee9bfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://github.com/davda54/sam\n",
    "\n",
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                e_w = p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "                self.state[p][\"e_w\"] = e_w\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
    "\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        p.grad.norm(p=2).to(shared_device)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a95a426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f09879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "all_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0de55a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>video_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./train/TRAIN_0000.mp4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./train/TRAIN_0001.mp4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>./train/TRAIN_0002.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>./train/TRAIN_0003.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./train/TRAIN_0004.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>TRAIN_2693</td>\n",
       "      <td>./train/TRAIN_2693.mp4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694</th>\n",
       "      <td>TRAIN_2694</td>\n",
       "      <td>./train/TRAIN_2694.mp4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>TRAIN_2695</td>\n",
       "      <td>./train/TRAIN_2695.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>TRAIN_2696</td>\n",
       "      <td>./train/TRAIN_2696.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>TRAIN_2697</td>\n",
       "      <td>./train/TRAIN_2697.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2698 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sample_id              video_path  label\n",
       "0     TRAIN_0000  ./train/TRAIN_0000.mp4      7\n",
       "1     TRAIN_0001  ./train/TRAIN_0001.mp4      7\n",
       "2     TRAIN_0002  ./train/TRAIN_0002.mp4      0\n",
       "3     TRAIN_0003  ./train/TRAIN_0003.mp4      0\n",
       "4     TRAIN_0004  ./train/TRAIN_0004.mp4      1\n",
       "...          ...                     ...    ...\n",
       "2693  TRAIN_2693  ./train/TRAIN_2693.mp4      3\n",
       "2694  TRAIN_2694  ./train/TRAIN_2694.mp4      5\n",
       "2695  TRAIN_2695  ./train/TRAIN_2695.mp4      0\n",
       "2696  TRAIN_2696  ./train/TRAIN_2696.mp4      0\n",
       "2697  TRAIN_2697  ./train/TRAIN_2697.mp4      0\n",
       "\n",
       "[2698 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "865dbbcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1783\n",
       "1      318\n",
       "7      317\n",
       "3       78\n",
       "2       51\n",
       "9       34\n",
       "11      33\n",
       "8       30\n",
       "5       28\n",
       "4       13\n",
       "12       6\n",
       "10       4\n",
       "6        3\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ef3570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CFG = {\n",
    "    'VIDEO_LENGTH':25, \n",
    "    'IMG_SIZE':230,\n",
    "    'EPOCHS':4,\n",
    "    'LEARNING_RATE':1e-5,\n",
    "    'BATCH_SIZE':2,\n",
    "    'SEED':2023,\n",
    "    'SPLIT':5,\n",
    "    'ROOT':'./data',\n",
    "    'MODEL':'MCG-NJU/videomae-base-finetuned-kinetics',\n",
    "    'LOAD_WEIGHT':False,\n",
    "    'LOAD_WEIGHT_NAME' :'a',\n",
    "    'VAL_SCORE_THRES' : 0.98,\n",
    "    'NUM_ASB':4,\n",
    "    'DATA_GENERATE_THRES':6.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b42aa213",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits = CFG['SPLIT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee5d0e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['video_path'] = all_df['video_path'].apply(lambda x:CFG['ROOT']+x[1:])\n",
    "test_df['video_path'] = test_df['video_path'].apply(lambda x:CFG['ROOT']+x[1:])\n",
    "\n",
    "crash_df = all_df.copy()\n",
    "crash_df['label'] = crash_df['label'].apply(lambda x: 1 if x != 0 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d72cd202",
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_df = all_df.copy()\n",
    "idx = ego_df[ego_df['label']==0].index\n",
    "ego_df.drop(idx,inplace=True)\n",
    "#0:no 1:yes\n",
    "ego_df['label'] = ego_df['label'].apply(lambda x: 1 if x < 7 else 0)\n",
    "ego_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba66f9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = all_df.copy()\n",
    "idx = weather_df[weather_df['label']==0].index\n",
    "weather_df.drop(idx,inplace=True)\n",
    "#0:normal,1:snow,2:rain\n",
    "weather_df['label'] = weather_df['label'].apply(lambda x: 0 if x==1 or x==2 or x==7 or x==8 else 1 if x==3 or x==4 or x==8 or x==9 else 2)\n",
    "weather_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cd0fd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df = all_df.copy()\n",
    "idx = time_df[time_df['label']==0].index\n",
    "time_df.drop(idx,inplace=True)\n",
    "#0:day 1:night\n",
    "time_df['label'] = time_df['label'].apply(lambda x: 0 if x%2==1 else 1)\n",
    "time_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65e1c289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>video_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./data/train/TRAIN_0000.mp4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./data/train/TRAIN_0001.mp4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>./data/train/TRAIN_0002.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>./data/train/TRAIN_0003.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./data/train/TRAIN_0004.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>TRAIN_2693</td>\n",
       "      <td>./data/train/TRAIN_2693.mp4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694</th>\n",
       "      <td>TRAIN_2694</td>\n",
       "      <td>./data/train/TRAIN_2694.mp4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>TRAIN_2695</td>\n",
       "      <td>./data/train/TRAIN_2695.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>TRAIN_2696</td>\n",
       "      <td>./data/train/TRAIN_2696.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>TRAIN_2697</td>\n",
       "      <td>./data/train/TRAIN_2697.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2698 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sample_id                   video_path  label\n",
       "0     TRAIN_0000  ./data/train/TRAIN_0000.mp4      7\n",
       "1     TRAIN_0001  ./data/train/TRAIN_0001.mp4      7\n",
       "2     TRAIN_0002  ./data/train/TRAIN_0002.mp4      0\n",
       "3     TRAIN_0003  ./data/train/TRAIN_0003.mp4      0\n",
       "4     TRAIN_0004  ./data/train/TRAIN_0004.mp4      1\n",
       "...          ...                          ...    ...\n",
       "2693  TRAIN_2693  ./data/train/TRAIN_2693.mp4      3\n",
       "2694  TRAIN_2694  ./data/train/TRAIN_2694.mp4      5\n",
       "2695  TRAIN_2695  ./data/train/TRAIN_2695.mp4      0\n",
       "2696  TRAIN_2696  ./data/train/TRAIN_2696.mp4      0\n",
       "2697  TRAIN_2697  ./data/train/TRAIN_2697.mp4      0\n",
       "\n",
       "[2698 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b433f3c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>video_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./data/train/TRAIN_0000.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./data/train/TRAIN_0001.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./data/train/TRAIN_0004.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0006</td>\n",
       "      <td>./data/train/TRAIN_0006.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0007</td>\n",
       "      <td>./data/train/TRAIN_0007.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>TRAIN_2685</td>\n",
       "      <td>./data/train/TRAIN_2685.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>TRAIN_2689</td>\n",
       "      <td>./data/train/TRAIN_2689.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>TRAIN_2692</td>\n",
       "      <td>./data/train/TRAIN_2692.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>TRAIN_2693</td>\n",
       "      <td>./data/train/TRAIN_2693.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>TRAIN_2694</td>\n",
       "      <td>./data/train/TRAIN_2694.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>915 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sample_id                   video_path  label\n",
       "0    TRAIN_0000  ./data/train/TRAIN_0000.mp4      0\n",
       "1    TRAIN_0001  ./data/train/TRAIN_0001.mp4      0\n",
       "2    TRAIN_0004  ./data/train/TRAIN_0004.mp4      1\n",
       "3    TRAIN_0006  ./data/train/TRAIN_0006.mp4      1\n",
       "4    TRAIN_0007  ./data/train/TRAIN_0007.mp4      0\n",
       "..          ...                          ...    ...\n",
       "910  TRAIN_2685  ./data/train/TRAIN_2685.mp4      0\n",
       "911  TRAIN_2689  ./data/train/TRAIN_2689.mp4      1\n",
       "912  TRAIN_2692  ./data/train/TRAIN_2692.mp4      0\n",
       "913  TRAIN_2693  ./data/train/TRAIN_2693.mp4      1\n",
       "914  TRAIN_2694  ./data/train/TRAIN_2694.mp4      1\n",
       "\n",
       "[915 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ego_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "660a98ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>video_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./data/train/TRAIN_0000.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./data/train/TRAIN_0001.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>./data/train/TRAIN_0002.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>./data/train/TRAIN_0003.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./data/train/TRAIN_0004.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>TRAIN_2693</td>\n",
       "      <td>./data/train/TRAIN_2693.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694</th>\n",
       "      <td>TRAIN_2694</td>\n",
       "      <td>./data/train/TRAIN_2694.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>TRAIN_2695</td>\n",
       "      <td>./data/train/TRAIN_2695.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>TRAIN_2696</td>\n",
       "      <td>./data/train/TRAIN_2696.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>TRAIN_2697</td>\n",
       "      <td>./data/train/TRAIN_2697.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2698 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sample_id                   video_path  label\n",
       "0     TRAIN_0000  ./data/train/TRAIN_0000.mp4      1\n",
       "1     TRAIN_0001  ./data/train/TRAIN_0001.mp4      1\n",
       "2     TRAIN_0002  ./data/train/TRAIN_0002.mp4      0\n",
       "3     TRAIN_0003  ./data/train/TRAIN_0003.mp4      0\n",
       "4     TRAIN_0004  ./data/train/TRAIN_0004.mp4      1\n",
       "...          ...                          ...    ...\n",
       "2693  TRAIN_2693  ./data/train/TRAIN_2693.mp4      1\n",
       "2694  TRAIN_2694  ./data/train/TRAIN_2694.mp4      1\n",
       "2695  TRAIN_2695  ./data/train/TRAIN_2695.mp4      0\n",
       "2696  TRAIN_2696  ./data/train/TRAIN_2696.mp4      0\n",
       "2697  TRAIN_2697  ./data/train/TRAIN_2697.mp4      0\n",
       "\n",
       "[2698 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crash_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57731127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>video_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./data/train/TRAIN_0000.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./data/train/TRAIN_0001.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./data/train/TRAIN_0004.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0006</td>\n",
       "      <td>./data/train/TRAIN_0006.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0007</td>\n",
       "      <td>./data/train/TRAIN_0007.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>TRAIN_2685</td>\n",
       "      <td>./data/train/TRAIN_2685.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>TRAIN_2689</td>\n",
       "      <td>./data/train/TRAIN_2689.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>TRAIN_2692</td>\n",
       "      <td>./data/train/TRAIN_2692.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>TRAIN_2693</td>\n",
       "      <td>./data/train/TRAIN_2693.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>TRAIN_2694</td>\n",
       "      <td>./data/train/TRAIN_2694.mp4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>915 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sample_id                   video_path  label\n",
       "0    TRAIN_0000  ./data/train/TRAIN_0000.mp4      0\n",
       "1    TRAIN_0001  ./data/train/TRAIN_0001.mp4      0\n",
       "2    TRAIN_0004  ./data/train/TRAIN_0004.mp4      0\n",
       "3    TRAIN_0006  ./data/train/TRAIN_0006.mp4      1\n",
       "4    TRAIN_0007  ./data/train/TRAIN_0007.mp4      0\n",
       "..          ...                          ...    ...\n",
       "910  TRAIN_2685  ./data/train/TRAIN_2685.mp4      0\n",
       "911  TRAIN_2689  ./data/train/TRAIN_2689.mp4      0\n",
       "912  TRAIN_2692  ./data/train/TRAIN_2692.mp4      0\n",
       "913  TRAIN_2693  ./data/train/TRAIN_2693.mp4      1\n",
       "914  TRAIN_2694  ./data/train/TRAIN_2694.mp4      2\n",
       "\n",
       "[915 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bebf1af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>video_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./data/train/TRAIN_0000.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./data/train/TRAIN_0001.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./data/train/TRAIN_0004.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0006</td>\n",
       "      <td>./data/train/TRAIN_0006.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0007</td>\n",
       "      <td>./data/train/TRAIN_0007.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>TRAIN_2685</td>\n",
       "      <td>./data/train/TRAIN_2685.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>TRAIN_2689</td>\n",
       "      <td>./data/train/TRAIN_2689.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>TRAIN_2692</td>\n",
       "      <td>./data/train/TRAIN_2692.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>TRAIN_2693</td>\n",
       "      <td>./data/train/TRAIN_2693.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>TRAIN_2694</td>\n",
       "      <td>./data/train/TRAIN_2694.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>915 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sample_id                   video_path  label\n",
       "0    TRAIN_0000  ./data/train/TRAIN_0000.mp4      0\n",
       "1    TRAIN_0001  ./data/train/TRAIN_0001.mp4      0\n",
       "2    TRAIN_0004  ./data/train/TRAIN_0004.mp4      0\n",
       "3    TRAIN_0006  ./data/train/TRAIN_0006.mp4      0\n",
       "4    TRAIN_0007  ./data/train/TRAIN_0007.mp4      0\n",
       "..          ...                          ...    ...\n",
       "910  TRAIN_2685  ./data/train/TRAIN_2685.mp4      1\n",
       "911  TRAIN_2689  ./data/train/TRAIN_2689.mp4      0\n",
       "912  TRAIN_2692  ./data/train/TRAIN_2692.mp4      0\n",
       "913  TRAIN_2693  ./data/train/TRAIN_2693.mp4      0\n",
       "914  TRAIN_2694  ./data/train/TRAIN_2694.mp4      0\n",
       "\n",
       "[915 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fed98cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58170cb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at MCG-NJU/videomae-base-finetuned-kinetics were not used when initializing VideoMAEModel: ['classifier.bias', 'fc_norm.weight', 'fc_norm.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing VideoMAEModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VideoMAEModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "from transformers import VideoMAEConfig, VideoMAEModel\n",
    "from transformers import AutoImageProcessor, VideoMAEForVideoClassification\n",
    "from transformers import XCLIPVisionModel, XCLIPVisionConfig\n",
    "\n",
    "configuration = VideoMAEConfig()\n",
    "configuration = AutoConfig.from_pretrained(CFG['MODEL'])\n",
    "configuration.num_frames = CFG['VIDEO_LENGTH']\n",
    "configuration.image_size=CFG['IMG_SIZE']\n",
    "configuration.image_size=CFG['IMG_SIZE']\n",
    "configuration.attention_probs_dropout_prob = 0.3\n",
    "configuration.hidden_dropout_prob = 0.3\n",
    "\n",
    "model = AutoModel.from_pretrained(CFG['MODEL'],config= configuration,ignore_mismatched_sizes=True)\n",
    "\n",
    "\n",
    "image_processor_config = AutoImageProcessor.from_pretrained(CFG['MODEL'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45094ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoMAEConfig {\n",
       "  \"_name_or_path\": \"MCG-NJU/videomae-base-finetuned-kinetics\",\n",
       "  \"architectures\": [\n",
       "    \"VideoMAEForVideoClassification\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.3,\n",
       "  \"decoder_hidden_size\": 384,\n",
       "  \"decoder_intermediate_size\": 1536,\n",
       "  \"decoder_num_attention_heads\": 6,\n",
       "  \"decoder_num_hidden_layers\": 4,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.3,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"abseiling\",\n",
       "    \"1\": \"air drumming\",\n",
       "    \"2\": \"answering questions\",\n",
       "    \"3\": \"applauding\",\n",
       "    \"4\": \"applying cream\",\n",
       "    \"5\": \"archery\",\n",
       "    \"6\": \"arm wrestling\",\n",
       "    \"7\": \"arranging flowers\",\n",
       "    \"8\": \"assembling computer\",\n",
       "    \"9\": \"auctioning\",\n",
       "    \"10\": \"baby waking up\",\n",
       "    \"11\": \"baking cookies\",\n",
       "    \"12\": \"balloon blowing\",\n",
       "    \"13\": \"bandaging\",\n",
       "    \"14\": \"barbequing\",\n",
       "    \"15\": \"bartending\",\n",
       "    \"16\": \"beatboxing\",\n",
       "    \"17\": \"bee keeping\",\n",
       "    \"18\": \"belly dancing\",\n",
       "    \"19\": \"bench pressing\",\n",
       "    \"20\": \"bending back\",\n",
       "    \"21\": \"bending metal\",\n",
       "    \"22\": \"biking through snow\",\n",
       "    \"23\": \"blasting sand\",\n",
       "    \"24\": \"blowing glass\",\n",
       "    \"25\": \"blowing leaves\",\n",
       "    \"26\": \"blowing nose\",\n",
       "    \"27\": \"blowing out candles\",\n",
       "    \"28\": \"bobsledding\",\n",
       "    \"29\": \"bookbinding\",\n",
       "    \"30\": \"bouncing on trampoline\",\n",
       "    \"31\": \"bowling\",\n",
       "    \"32\": \"braiding hair\",\n",
       "    \"33\": \"breading or breadcrumbing\",\n",
       "    \"34\": \"breakdancing\",\n",
       "    \"35\": \"brush painting\",\n",
       "    \"36\": \"brushing hair\",\n",
       "    \"37\": \"brushing teeth\",\n",
       "    \"38\": \"building cabinet\",\n",
       "    \"39\": \"building shed\",\n",
       "    \"40\": \"bungee jumping\",\n",
       "    \"41\": \"busking\",\n",
       "    \"42\": \"canoeing or kayaking\",\n",
       "    \"43\": \"capoeira\",\n",
       "    \"44\": \"carrying baby\",\n",
       "    \"45\": \"cartwheeling\",\n",
       "    \"46\": \"carving pumpkin\",\n",
       "    \"47\": \"catching fish\",\n",
       "    \"48\": \"catching or throwing baseball\",\n",
       "    \"49\": \"catching or throwing frisbee\",\n",
       "    \"50\": \"catching or throwing softball\",\n",
       "    \"51\": \"celebrating\",\n",
       "    \"52\": \"changing oil\",\n",
       "    \"53\": \"changing wheel\",\n",
       "    \"54\": \"checking tires\",\n",
       "    \"55\": \"cheerleading\",\n",
       "    \"56\": \"chopping wood\",\n",
       "    \"57\": \"clapping\",\n",
       "    \"58\": \"clay pottery making\",\n",
       "    \"59\": \"clean and jerk\",\n",
       "    \"60\": \"cleaning floor\",\n",
       "    \"61\": \"cleaning gutters\",\n",
       "    \"62\": \"cleaning pool\",\n",
       "    \"63\": \"cleaning shoes\",\n",
       "    \"64\": \"cleaning toilet\",\n",
       "    \"65\": \"cleaning windows\",\n",
       "    \"66\": \"climbing a rope\",\n",
       "    \"67\": \"climbing ladder\",\n",
       "    \"68\": \"climbing tree\",\n",
       "    \"69\": \"contact juggling\",\n",
       "    \"70\": \"cooking chicken\",\n",
       "    \"71\": \"cooking egg\",\n",
       "    \"72\": \"cooking on campfire\",\n",
       "    \"73\": \"cooking sausages\",\n",
       "    \"74\": \"counting money\",\n",
       "    \"75\": \"country line dancing\",\n",
       "    \"76\": \"cracking neck\",\n",
       "    \"77\": \"crawling baby\",\n",
       "    \"78\": \"crossing river\",\n",
       "    \"79\": \"crying\",\n",
       "    \"80\": \"curling hair\",\n",
       "    \"81\": \"cutting nails\",\n",
       "    \"82\": \"cutting pineapple\",\n",
       "    \"83\": \"cutting watermelon\",\n",
       "    \"84\": \"dancing ballet\",\n",
       "    \"85\": \"dancing charleston\",\n",
       "    \"86\": \"dancing gangnam style\",\n",
       "    \"87\": \"dancing macarena\",\n",
       "    \"88\": \"deadlifting\",\n",
       "    \"89\": \"decorating the christmas tree\",\n",
       "    \"90\": \"digging\",\n",
       "    \"91\": \"dining\",\n",
       "    \"92\": \"disc golfing\",\n",
       "    \"93\": \"diving cliff\",\n",
       "    \"94\": \"dodgeball\",\n",
       "    \"95\": \"doing aerobics\",\n",
       "    \"96\": \"doing laundry\",\n",
       "    \"97\": \"doing nails\",\n",
       "    \"98\": \"drawing\",\n",
       "    \"99\": \"dribbling basketball\",\n",
       "    \"100\": \"drinking\",\n",
       "    \"101\": \"drinking beer\",\n",
       "    \"102\": \"drinking shots\",\n",
       "    \"103\": \"driving car\",\n",
       "    \"104\": \"driving tractor\",\n",
       "    \"105\": \"drop kicking\",\n",
       "    \"106\": \"drumming fingers\",\n",
       "    \"107\": \"dunking basketball\",\n",
       "    \"108\": \"dying hair\",\n",
       "    \"109\": \"eating burger\",\n",
       "    \"110\": \"eating cake\",\n",
       "    \"111\": \"eating carrots\",\n",
       "    \"112\": \"eating chips\",\n",
       "    \"113\": \"eating doughnuts\",\n",
       "    \"114\": \"eating hotdog\",\n",
       "    \"115\": \"eating ice cream\",\n",
       "    \"116\": \"eating spaghetti\",\n",
       "    \"117\": \"eating watermelon\",\n",
       "    \"118\": \"egg hunting\",\n",
       "    \"119\": \"exercising arm\",\n",
       "    \"120\": \"exercising with an exercise ball\",\n",
       "    \"121\": \"extinguishing fire\",\n",
       "    \"122\": \"faceplanting\",\n",
       "    \"123\": \"feeding birds\",\n",
       "    \"124\": \"feeding fish\",\n",
       "    \"125\": \"feeding goats\",\n",
       "    \"126\": \"filling eyebrows\",\n",
       "    \"127\": \"finger snapping\",\n",
       "    \"128\": \"fixing hair\",\n",
       "    \"129\": \"flipping pancake\",\n",
       "    \"130\": \"flying kite\",\n",
       "    \"131\": \"folding clothes\",\n",
       "    \"132\": \"folding napkins\",\n",
       "    \"133\": \"folding paper\",\n",
       "    \"134\": \"front raises\",\n",
       "    \"135\": \"frying vegetables\",\n",
       "    \"136\": \"garbage collecting\",\n",
       "    \"137\": \"gargling\",\n",
       "    \"138\": \"getting a haircut\",\n",
       "    \"139\": \"getting a tattoo\",\n",
       "    \"140\": \"giving or receiving award\",\n",
       "    \"141\": \"golf chipping\",\n",
       "    \"142\": \"golf driving\",\n",
       "    \"143\": \"golf putting\",\n",
       "    \"144\": \"grinding meat\",\n",
       "    \"145\": \"grooming dog\",\n",
       "    \"146\": \"grooming horse\",\n",
       "    \"147\": \"gymnastics tumbling\",\n",
       "    \"148\": \"hammer throw\",\n",
       "    \"149\": \"headbanging\",\n",
       "    \"150\": \"headbutting\",\n",
       "    \"151\": \"high jump\",\n",
       "    \"152\": \"high kick\",\n",
       "    \"153\": \"hitting baseball\",\n",
       "    \"154\": \"hockey stop\",\n",
       "    \"155\": \"holding snake\",\n",
       "    \"156\": \"hopscotch\",\n",
       "    \"157\": \"hoverboarding\",\n",
       "    \"158\": \"hugging\",\n",
       "    \"159\": \"hula hooping\",\n",
       "    \"160\": \"hurdling\",\n",
       "    \"161\": \"hurling (sport)\",\n",
       "    \"162\": \"ice climbing\",\n",
       "    \"163\": \"ice fishing\",\n",
       "    \"164\": \"ice skating\",\n",
       "    \"165\": \"ironing\",\n",
       "    \"166\": \"javelin throw\",\n",
       "    \"167\": \"jetskiing\",\n",
       "    \"168\": \"jogging\",\n",
       "    \"169\": \"juggling balls\",\n",
       "    \"170\": \"juggling fire\",\n",
       "    \"171\": \"juggling soccer ball\",\n",
       "    \"172\": \"jumping into pool\",\n",
       "    \"173\": \"jumpstyle dancing\",\n",
       "    \"174\": \"kicking field goal\",\n",
       "    \"175\": \"kicking soccer ball\",\n",
       "    \"176\": \"kissing\",\n",
       "    \"177\": \"kitesurfing\",\n",
       "    \"178\": \"knitting\",\n",
       "    \"179\": \"krumping\",\n",
       "    \"180\": \"laughing\",\n",
       "    \"181\": \"laying bricks\",\n",
       "    \"182\": \"long jump\",\n",
       "    \"183\": \"lunge\",\n",
       "    \"184\": \"making a cake\",\n",
       "    \"185\": \"making a sandwich\",\n",
       "    \"186\": \"making bed\",\n",
       "    \"187\": \"making jewelry\",\n",
       "    \"188\": \"making pizza\",\n",
       "    \"189\": \"making snowman\",\n",
       "    \"190\": \"making sushi\",\n",
       "    \"191\": \"making tea\",\n",
       "    \"192\": \"marching\",\n",
       "    \"193\": \"massaging back\",\n",
       "    \"194\": \"massaging feet\",\n",
       "    \"195\": \"massaging legs\",\n",
       "    \"196\": \"massaging person's head\",\n",
       "    \"197\": \"milking cow\",\n",
       "    \"198\": \"mopping floor\",\n",
       "    \"199\": \"motorcycling\",\n",
       "    \"200\": \"moving furniture\",\n",
       "    \"201\": \"mowing lawn\",\n",
       "    \"202\": \"news anchoring\",\n",
       "    \"203\": \"opening bottle\",\n",
       "    \"204\": \"opening present\",\n",
       "    \"205\": \"paragliding\",\n",
       "    \"206\": \"parasailing\",\n",
       "    \"207\": \"parkour\",\n",
       "    \"208\": \"passing American football (in game)\",\n",
       "    \"209\": \"passing American football (not in game)\",\n",
       "    \"210\": \"peeling apples\",\n",
       "    \"211\": \"peeling potatoes\",\n",
       "    \"212\": \"petting animal (not cat)\",\n",
       "    \"213\": \"petting cat\",\n",
       "    \"214\": \"picking fruit\",\n",
       "    \"215\": \"planting trees\",\n",
       "    \"216\": \"plastering\",\n",
       "    \"217\": \"playing accordion\",\n",
       "    \"218\": \"playing badminton\",\n",
       "    \"219\": \"playing bagpipes\",\n",
       "    \"220\": \"playing basketball\",\n",
       "    \"221\": \"playing bass guitar\",\n",
       "    \"222\": \"playing cards\",\n",
       "    \"223\": \"playing cello\",\n",
       "    \"224\": \"playing chess\",\n",
       "    \"225\": \"playing clarinet\",\n",
       "    \"226\": \"playing controller\",\n",
       "    \"227\": \"playing cricket\",\n",
       "    \"228\": \"playing cymbals\",\n",
       "    \"229\": \"playing didgeridoo\",\n",
       "    \"230\": \"playing drums\",\n",
       "    \"231\": \"playing flute\",\n",
       "    \"232\": \"playing guitar\",\n",
       "    \"233\": \"playing harmonica\",\n",
       "    \"234\": \"playing harp\",\n",
       "    \"235\": \"playing ice hockey\",\n",
       "    \"236\": \"playing keyboard\",\n",
       "    \"237\": \"playing kickball\",\n",
       "    \"238\": \"playing monopoly\",\n",
       "    \"239\": \"playing organ\",\n",
       "    \"240\": \"playing paintball\",\n",
       "    \"241\": \"playing piano\",\n",
       "    \"242\": \"playing poker\",\n",
       "    \"243\": \"playing recorder\",\n",
       "    \"244\": \"playing saxophone\",\n",
       "    \"245\": \"playing squash or racquetball\",\n",
       "    \"246\": \"playing tennis\",\n",
       "    \"247\": \"playing trombone\",\n",
       "    \"248\": \"playing trumpet\",\n",
       "    \"249\": \"playing ukulele\",\n",
       "    \"250\": \"playing violin\",\n",
       "    \"251\": \"playing volleyball\",\n",
       "    \"252\": \"playing xylophone\",\n",
       "    \"253\": \"pole vault\",\n",
       "    \"254\": \"presenting weather forecast\",\n",
       "    \"255\": \"pull ups\",\n",
       "    \"256\": \"pumping fist\",\n",
       "    \"257\": \"pumping gas\",\n",
       "    \"258\": \"punching bag\",\n",
       "    \"259\": \"punching person (boxing)\",\n",
       "    \"260\": \"push up\",\n",
       "    \"261\": \"pushing car\",\n",
       "    \"262\": \"pushing cart\",\n",
       "    \"263\": \"pushing wheelchair\",\n",
       "    \"264\": \"reading book\",\n",
       "    \"265\": \"reading newspaper\",\n",
       "    \"266\": \"recording music\",\n",
       "    \"267\": \"riding a bike\",\n",
       "    \"268\": \"riding camel\",\n",
       "    \"269\": \"riding elephant\",\n",
       "    \"270\": \"riding mechanical bull\",\n",
       "    \"271\": \"riding mountain bike\",\n",
       "    \"272\": \"riding mule\",\n",
       "    \"273\": \"riding or walking with horse\",\n",
       "    \"274\": \"riding scooter\",\n",
       "    \"275\": \"riding unicycle\",\n",
       "    \"276\": \"ripping paper\",\n",
       "    \"277\": \"robot dancing\",\n",
       "    \"278\": \"rock climbing\",\n",
       "    \"279\": \"rock scissors paper\",\n",
       "    \"280\": \"roller skating\",\n",
       "    \"281\": \"running on treadmill\",\n",
       "    \"282\": \"sailing\",\n",
       "    \"283\": \"salsa dancing\",\n",
       "    \"284\": \"sanding floor\",\n",
       "    \"285\": \"scrambling eggs\",\n",
       "    \"286\": \"scuba diving\",\n",
       "    \"287\": \"setting table\",\n",
       "    \"288\": \"shaking hands\",\n",
       "    \"289\": \"shaking head\",\n",
       "    \"290\": \"sharpening knives\",\n",
       "    \"291\": \"sharpening pencil\",\n",
       "    \"292\": \"shaving head\",\n",
       "    \"293\": \"shaving legs\",\n",
       "    \"294\": \"shearing sheep\",\n",
       "    \"295\": \"shining shoes\",\n",
       "    \"296\": \"shooting basketball\",\n",
       "    \"297\": \"shooting goal (soccer)\",\n",
       "    \"298\": \"shot put\",\n",
       "    \"299\": \"shoveling snow\",\n",
       "    \"300\": \"shredding paper\",\n",
       "    \"301\": \"shuffling cards\",\n",
       "    \"302\": \"side kick\",\n",
       "    \"303\": \"sign language interpreting\",\n",
       "    \"304\": \"singing\",\n",
       "    \"305\": \"situp\",\n",
       "    \"306\": \"skateboarding\",\n",
       "    \"307\": \"ski jumping\",\n",
       "    \"308\": \"skiing (not slalom or crosscountry)\",\n",
       "    \"309\": \"skiing crosscountry\",\n",
       "    \"310\": \"skiing slalom\",\n",
       "    \"311\": \"skipping rope\",\n",
       "    \"312\": \"skydiving\",\n",
       "    \"313\": \"slacklining\",\n",
       "    \"314\": \"slapping\",\n",
       "    \"315\": \"sled dog racing\",\n",
       "    \"316\": \"smoking\",\n",
       "    \"317\": \"smoking hookah\",\n",
       "    \"318\": \"snatch weight lifting\",\n",
       "    \"319\": \"sneezing\",\n",
       "    \"320\": \"sniffing\",\n",
       "    \"321\": \"snorkeling\",\n",
       "    \"322\": \"snowboarding\",\n",
       "    \"323\": \"snowkiting\",\n",
       "    \"324\": \"snowmobiling\",\n",
       "    \"325\": \"somersaulting\",\n",
       "    \"326\": \"spinning poi\",\n",
       "    \"327\": \"spray painting\",\n",
       "    \"328\": \"spraying\",\n",
       "    \"329\": \"springboard diving\",\n",
       "    \"330\": \"squat\",\n",
       "    \"331\": \"sticking tongue out\",\n",
       "    \"332\": \"stomping grapes\",\n",
       "    \"333\": \"stretching arm\",\n",
       "    \"334\": \"stretching leg\",\n",
       "    \"335\": \"strumming guitar\",\n",
       "    \"336\": \"surfing crowd\",\n",
       "    \"337\": \"surfing water\",\n",
       "    \"338\": \"sweeping floor\",\n",
       "    \"339\": \"swimming backstroke\",\n",
       "    \"340\": \"swimming breast stroke\",\n",
       "    \"341\": \"swimming butterfly stroke\",\n",
       "    \"342\": \"swing dancing\",\n",
       "    \"343\": \"swinging legs\",\n",
       "    \"344\": \"swinging on something\",\n",
       "    \"345\": \"sword fighting\",\n",
       "    \"346\": \"tai chi\",\n",
       "    \"347\": \"taking a shower\",\n",
       "    \"348\": \"tango dancing\",\n",
       "    \"349\": \"tap dancing\",\n",
       "    \"350\": \"tapping guitar\",\n",
       "    \"351\": \"tapping pen\",\n",
       "    \"352\": \"tasting beer\",\n",
       "    \"353\": \"tasting food\",\n",
       "    \"354\": \"testifying\",\n",
       "    \"355\": \"texting\",\n",
       "    \"356\": \"throwing axe\",\n",
       "    \"357\": \"throwing ball\",\n",
       "    \"358\": \"throwing discus\",\n",
       "    \"359\": \"tickling\",\n",
       "    \"360\": \"tobogganing\",\n",
       "    \"361\": \"tossing coin\",\n",
       "    \"362\": \"tossing salad\",\n",
       "    \"363\": \"training dog\",\n",
       "    \"364\": \"trapezing\",\n",
       "    \"365\": \"trimming or shaving beard\",\n",
       "    \"366\": \"trimming trees\",\n",
       "    \"367\": \"triple jump\",\n",
       "    \"368\": \"tying bow tie\",\n",
       "    \"369\": \"tying knot (not on a tie)\",\n",
       "    \"370\": \"tying tie\",\n",
       "    \"371\": \"unboxing\",\n",
       "    \"372\": \"unloading truck\",\n",
       "    \"373\": \"using computer\",\n",
       "    \"374\": \"using remote controller (not gaming)\",\n",
       "    \"375\": \"using segway\",\n",
       "    \"376\": \"vault\",\n",
       "    \"377\": \"waiting in line\",\n",
       "    \"378\": \"walking the dog\",\n",
       "    \"379\": \"washing dishes\",\n",
       "    \"380\": \"washing feet\",\n",
       "    \"381\": \"washing hair\",\n",
       "    \"382\": \"washing hands\",\n",
       "    \"383\": \"water skiing\",\n",
       "    \"384\": \"water sliding\",\n",
       "    \"385\": \"watering plants\",\n",
       "    \"386\": \"waxing back\",\n",
       "    \"387\": \"waxing chest\",\n",
       "    \"388\": \"waxing eyebrows\",\n",
       "    \"389\": \"waxing legs\",\n",
       "    \"390\": \"weaving basket\",\n",
       "    \"391\": \"welding\",\n",
       "    \"392\": \"whistling\",\n",
       "    \"393\": \"windsurfing\",\n",
       "    \"394\": \"wrapping present\",\n",
       "    \"395\": \"wrestling\",\n",
       "    \"396\": \"writing\",\n",
       "    \"397\": \"yawning\",\n",
       "    \"398\": \"yoga\",\n",
       "    \"399\": \"zumba\"\n",
       "  },\n",
       "  \"image_size\": 230,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"abseiling\": 0,\n",
       "    \"air drumming\": 1,\n",
       "    \"answering questions\": 2,\n",
       "    \"applauding\": 3,\n",
       "    \"applying cream\": 4,\n",
       "    \"archery\": 5,\n",
       "    \"arm wrestling\": 6,\n",
       "    \"arranging flowers\": 7,\n",
       "    \"assembling computer\": 8,\n",
       "    \"auctioning\": 9,\n",
       "    \"baby waking up\": 10,\n",
       "    \"baking cookies\": 11,\n",
       "    \"balloon blowing\": 12,\n",
       "    \"bandaging\": 13,\n",
       "    \"barbequing\": 14,\n",
       "    \"bartending\": 15,\n",
       "    \"beatboxing\": 16,\n",
       "    \"bee keeping\": 17,\n",
       "    \"belly dancing\": 18,\n",
       "    \"bench pressing\": 19,\n",
       "    \"bending back\": 20,\n",
       "    \"bending metal\": 21,\n",
       "    \"biking through snow\": 22,\n",
       "    \"blasting sand\": 23,\n",
       "    \"blowing glass\": 24,\n",
       "    \"blowing leaves\": 25,\n",
       "    \"blowing nose\": 26,\n",
       "    \"blowing out candles\": 27,\n",
       "    \"bobsledding\": 28,\n",
       "    \"bookbinding\": 29,\n",
       "    \"bouncing on trampoline\": 30,\n",
       "    \"bowling\": 31,\n",
       "    \"braiding hair\": 32,\n",
       "    \"breading or breadcrumbing\": 33,\n",
       "    \"breakdancing\": 34,\n",
       "    \"brush painting\": 35,\n",
       "    \"brushing hair\": 36,\n",
       "    \"brushing teeth\": 37,\n",
       "    \"building cabinet\": 38,\n",
       "    \"building shed\": 39,\n",
       "    \"bungee jumping\": 40,\n",
       "    \"busking\": 41,\n",
       "    \"canoeing or kayaking\": 42,\n",
       "    \"capoeira\": 43,\n",
       "    \"carrying baby\": 44,\n",
       "    \"cartwheeling\": 45,\n",
       "    \"carving pumpkin\": 46,\n",
       "    \"catching fish\": 47,\n",
       "    \"catching or throwing baseball\": 48,\n",
       "    \"catching or throwing frisbee\": 49,\n",
       "    \"catching or throwing softball\": 50,\n",
       "    \"celebrating\": 51,\n",
       "    \"changing oil\": 52,\n",
       "    \"changing wheel\": 53,\n",
       "    \"checking tires\": 54,\n",
       "    \"cheerleading\": 55,\n",
       "    \"chopping wood\": 56,\n",
       "    \"clapping\": 57,\n",
       "    \"clay pottery making\": 58,\n",
       "    \"clean and jerk\": 59,\n",
       "    \"cleaning floor\": 60,\n",
       "    \"cleaning gutters\": 61,\n",
       "    \"cleaning pool\": 62,\n",
       "    \"cleaning shoes\": 63,\n",
       "    \"cleaning toilet\": 64,\n",
       "    \"cleaning windows\": 65,\n",
       "    \"climbing a rope\": 66,\n",
       "    \"climbing ladder\": 67,\n",
       "    \"climbing tree\": 68,\n",
       "    \"contact juggling\": 69,\n",
       "    \"cooking chicken\": 70,\n",
       "    \"cooking egg\": 71,\n",
       "    \"cooking on campfire\": 72,\n",
       "    \"cooking sausages\": 73,\n",
       "    \"counting money\": 74,\n",
       "    \"country line dancing\": 75,\n",
       "    \"cracking neck\": 76,\n",
       "    \"crawling baby\": 77,\n",
       "    \"crossing river\": 78,\n",
       "    \"crying\": 79,\n",
       "    \"curling hair\": 80,\n",
       "    \"cutting nails\": 81,\n",
       "    \"cutting pineapple\": 82,\n",
       "    \"cutting watermelon\": 83,\n",
       "    \"dancing ballet\": 84,\n",
       "    \"dancing charleston\": 85,\n",
       "    \"dancing gangnam style\": 86,\n",
       "    \"dancing macarena\": 87,\n",
       "    \"deadlifting\": 88,\n",
       "    \"decorating the christmas tree\": 89,\n",
       "    \"digging\": 90,\n",
       "    \"dining\": 91,\n",
       "    \"disc golfing\": 92,\n",
       "    \"diving cliff\": 93,\n",
       "    \"dodgeball\": 94,\n",
       "    \"doing aerobics\": 95,\n",
       "    \"doing laundry\": 96,\n",
       "    \"doing nails\": 97,\n",
       "    \"drawing\": 98,\n",
       "    \"dribbling basketball\": 99,\n",
       "    \"drinking\": 100,\n",
       "    \"drinking beer\": 101,\n",
       "    \"drinking shots\": 102,\n",
       "    \"driving car\": 103,\n",
       "    \"driving tractor\": 104,\n",
       "    \"drop kicking\": 105,\n",
       "    \"drumming fingers\": 106,\n",
       "    \"dunking basketball\": 107,\n",
       "    \"dying hair\": 108,\n",
       "    \"eating burger\": 109,\n",
       "    \"eating cake\": 110,\n",
       "    \"eating carrots\": 111,\n",
       "    \"eating chips\": 112,\n",
       "    \"eating doughnuts\": 113,\n",
       "    \"eating hotdog\": 114,\n",
       "    \"eating ice cream\": 115,\n",
       "    \"eating spaghetti\": 116,\n",
       "    \"eating watermelon\": 117,\n",
       "    \"egg hunting\": 118,\n",
       "    \"exercising arm\": 119,\n",
       "    \"exercising with an exercise ball\": 120,\n",
       "    \"extinguishing fire\": 121,\n",
       "    \"faceplanting\": 122,\n",
       "    \"feeding birds\": 123,\n",
       "    \"feeding fish\": 124,\n",
       "    \"feeding goats\": 125,\n",
       "    \"filling eyebrows\": 126,\n",
       "    \"finger snapping\": 127,\n",
       "    \"fixing hair\": 128,\n",
       "    \"flipping pancake\": 129,\n",
       "    \"flying kite\": 130,\n",
       "    \"folding clothes\": 131,\n",
       "    \"folding napkins\": 132,\n",
       "    \"folding paper\": 133,\n",
       "    \"front raises\": 134,\n",
       "    \"frying vegetables\": 135,\n",
       "    \"garbage collecting\": 136,\n",
       "    \"gargling\": 137,\n",
       "    \"getting a haircut\": 138,\n",
       "    \"getting a tattoo\": 139,\n",
       "    \"giving or receiving award\": 140,\n",
       "    \"golf chipping\": 141,\n",
       "    \"golf driving\": 142,\n",
       "    \"golf putting\": 143,\n",
       "    \"grinding meat\": 144,\n",
       "    \"grooming dog\": 145,\n",
       "    \"grooming horse\": 146,\n",
       "    \"gymnastics tumbling\": 147,\n",
       "    \"hammer throw\": 148,\n",
       "    \"headbanging\": 149,\n",
       "    \"headbutting\": 150,\n",
       "    \"high jump\": 151,\n",
       "    \"high kick\": 152,\n",
       "    \"hitting baseball\": 153,\n",
       "    \"hockey stop\": 154,\n",
       "    \"holding snake\": 155,\n",
       "    \"hopscotch\": 156,\n",
       "    \"hoverboarding\": 157,\n",
       "    \"hugging\": 158,\n",
       "    \"hula hooping\": 159,\n",
       "    \"hurdling\": 160,\n",
       "    \"hurling (sport)\": 161,\n",
       "    \"ice climbing\": 162,\n",
       "    \"ice fishing\": 163,\n",
       "    \"ice skating\": 164,\n",
       "    \"ironing\": 165,\n",
       "    \"javelin throw\": 166,\n",
       "    \"jetskiing\": 167,\n",
       "    \"jogging\": 168,\n",
       "    \"juggling balls\": 169,\n",
       "    \"juggling fire\": 170,\n",
       "    \"juggling soccer ball\": 171,\n",
       "    \"jumping into pool\": 172,\n",
       "    \"jumpstyle dancing\": 173,\n",
       "    \"kicking field goal\": 174,\n",
       "    \"kicking soccer ball\": 175,\n",
       "    \"kissing\": 176,\n",
       "    \"kitesurfing\": 177,\n",
       "    \"knitting\": 178,\n",
       "    \"krumping\": 179,\n",
       "    \"laughing\": 180,\n",
       "    \"laying bricks\": 181,\n",
       "    \"long jump\": 182,\n",
       "    \"lunge\": 183,\n",
       "    \"making a cake\": 184,\n",
       "    \"making a sandwich\": 185,\n",
       "    \"making bed\": 186,\n",
       "    \"making jewelry\": 187,\n",
       "    \"making pizza\": 188,\n",
       "    \"making snowman\": 189,\n",
       "    \"making sushi\": 190,\n",
       "    \"making tea\": 191,\n",
       "    \"marching\": 192,\n",
       "    \"massaging back\": 193,\n",
       "    \"massaging feet\": 194,\n",
       "    \"massaging legs\": 195,\n",
       "    \"massaging person's head\": 196,\n",
       "    \"milking cow\": 197,\n",
       "    \"mopping floor\": 198,\n",
       "    \"motorcycling\": 199,\n",
       "    \"moving furniture\": 200,\n",
       "    \"mowing lawn\": 201,\n",
       "    \"news anchoring\": 202,\n",
       "    \"opening bottle\": 203,\n",
       "    \"opening present\": 204,\n",
       "    \"paragliding\": 205,\n",
       "    \"parasailing\": 206,\n",
       "    \"parkour\": 207,\n",
       "    \"passing American football (in game)\": 208,\n",
       "    \"passing American football (not in game)\": 209,\n",
       "    \"peeling apples\": 210,\n",
       "    \"peeling potatoes\": 211,\n",
       "    \"petting animal (not cat)\": 212,\n",
       "    \"petting cat\": 213,\n",
       "    \"picking fruit\": 214,\n",
       "    \"planting trees\": 215,\n",
       "    \"plastering\": 216,\n",
       "    \"playing accordion\": 217,\n",
       "    \"playing badminton\": 218,\n",
       "    \"playing bagpipes\": 219,\n",
       "    \"playing basketball\": 220,\n",
       "    \"playing bass guitar\": 221,\n",
       "    \"playing cards\": 222,\n",
       "    \"playing cello\": 223,\n",
       "    \"playing chess\": 224,\n",
       "    \"playing clarinet\": 225,\n",
       "    \"playing controller\": 226,\n",
       "    \"playing cricket\": 227,\n",
       "    \"playing cymbals\": 228,\n",
       "    \"playing didgeridoo\": 229,\n",
       "    \"playing drums\": 230,\n",
       "    \"playing flute\": 231,\n",
       "    \"playing guitar\": 232,\n",
       "    \"playing harmonica\": 233,\n",
       "    \"playing harp\": 234,\n",
       "    \"playing ice hockey\": 235,\n",
       "    \"playing keyboard\": 236,\n",
       "    \"playing kickball\": 237,\n",
       "    \"playing monopoly\": 238,\n",
       "    \"playing organ\": 239,\n",
       "    \"playing paintball\": 240,\n",
       "    \"playing piano\": 241,\n",
       "    \"playing poker\": 242,\n",
       "    \"playing recorder\": 243,\n",
       "    \"playing saxophone\": 244,\n",
       "    \"playing squash or racquetball\": 245,\n",
       "    \"playing tennis\": 246,\n",
       "    \"playing trombone\": 247,\n",
       "    \"playing trumpet\": 248,\n",
       "    \"playing ukulele\": 249,\n",
       "    \"playing violin\": 250,\n",
       "    \"playing volleyball\": 251,\n",
       "    \"playing xylophone\": 252,\n",
       "    \"pole vault\": 253,\n",
       "    \"presenting weather forecast\": 254,\n",
       "    \"pull ups\": 255,\n",
       "    \"pumping fist\": 256,\n",
       "    \"pumping gas\": 257,\n",
       "    \"punching bag\": 258,\n",
       "    \"punching person (boxing)\": 259,\n",
       "    \"push up\": 260,\n",
       "    \"pushing car\": 261,\n",
       "    \"pushing cart\": 262,\n",
       "    \"pushing wheelchair\": 263,\n",
       "    \"reading book\": 264,\n",
       "    \"reading newspaper\": 265,\n",
       "    \"recording music\": 266,\n",
       "    \"riding a bike\": 267,\n",
       "    \"riding camel\": 268,\n",
       "    \"riding elephant\": 269,\n",
       "    \"riding mechanical bull\": 270,\n",
       "    \"riding mountain bike\": 271,\n",
       "    \"riding mule\": 272,\n",
       "    \"riding or walking with horse\": 273,\n",
       "    \"riding scooter\": 274,\n",
       "    \"riding unicycle\": 275,\n",
       "    \"ripping paper\": 276,\n",
       "    \"robot dancing\": 277,\n",
       "    \"rock climbing\": 278,\n",
       "    \"rock scissors paper\": 279,\n",
       "    \"roller skating\": 280,\n",
       "    \"running on treadmill\": 281,\n",
       "    \"sailing\": 282,\n",
       "    \"salsa dancing\": 283,\n",
       "    \"sanding floor\": 284,\n",
       "    \"scrambling eggs\": 285,\n",
       "    \"scuba diving\": 286,\n",
       "    \"setting table\": 287,\n",
       "    \"shaking hands\": 288,\n",
       "    \"shaking head\": 289,\n",
       "    \"sharpening knives\": 290,\n",
       "    \"sharpening pencil\": 291,\n",
       "    \"shaving head\": 292,\n",
       "    \"shaving legs\": 293,\n",
       "    \"shearing sheep\": 294,\n",
       "    \"shining shoes\": 295,\n",
       "    \"shooting basketball\": 296,\n",
       "    \"shooting goal (soccer)\": 297,\n",
       "    \"shot put\": 298,\n",
       "    \"shoveling snow\": 299,\n",
       "    \"shredding paper\": 300,\n",
       "    \"shuffling cards\": 301,\n",
       "    \"side kick\": 302,\n",
       "    \"sign language interpreting\": 303,\n",
       "    \"singing\": 304,\n",
       "    \"situp\": 305,\n",
       "    \"skateboarding\": 306,\n",
       "    \"ski jumping\": 307,\n",
       "    \"skiing (not slalom or crosscountry)\": 308,\n",
       "    \"skiing crosscountry\": 309,\n",
       "    \"skiing slalom\": 310,\n",
       "    \"skipping rope\": 311,\n",
       "    \"skydiving\": 312,\n",
       "    \"slacklining\": 313,\n",
       "    \"slapping\": 314,\n",
       "    \"sled dog racing\": 315,\n",
       "    \"smoking\": 316,\n",
       "    \"smoking hookah\": 317,\n",
       "    \"snatch weight lifting\": 318,\n",
       "    \"sneezing\": 319,\n",
       "    \"sniffing\": 320,\n",
       "    \"snorkeling\": 321,\n",
       "    \"snowboarding\": 322,\n",
       "    \"snowkiting\": 323,\n",
       "    \"snowmobiling\": 324,\n",
       "    \"somersaulting\": 325,\n",
       "    \"spinning poi\": 326,\n",
       "    \"spray painting\": 327,\n",
       "    \"spraying\": 328,\n",
       "    \"springboard diving\": 329,\n",
       "    \"squat\": 330,\n",
       "    \"sticking tongue out\": 331,\n",
       "    \"stomping grapes\": 332,\n",
       "    \"stretching arm\": 333,\n",
       "    \"stretching leg\": 334,\n",
       "    \"strumming guitar\": 335,\n",
       "    \"surfing crowd\": 336,\n",
       "    \"surfing water\": 337,\n",
       "    \"sweeping floor\": 338,\n",
       "    \"swimming backstroke\": 339,\n",
       "    \"swimming breast stroke\": 340,\n",
       "    \"swimming butterfly stroke\": 341,\n",
       "    \"swing dancing\": 342,\n",
       "    \"swinging legs\": 343,\n",
       "    \"swinging on something\": 344,\n",
       "    \"sword fighting\": 345,\n",
       "    \"tai chi\": 346,\n",
       "    \"taking a shower\": 347,\n",
       "    \"tango dancing\": 348,\n",
       "    \"tap dancing\": 349,\n",
       "    \"tapping guitar\": 350,\n",
       "    \"tapping pen\": 351,\n",
       "    \"tasting beer\": 352,\n",
       "    \"tasting food\": 353,\n",
       "    \"testifying\": 354,\n",
       "    \"texting\": 355,\n",
       "    \"throwing axe\": 356,\n",
       "    \"throwing ball\": 357,\n",
       "    \"throwing discus\": 358,\n",
       "    \"tickling\": 359,\n",
       "    \"tobogganing\": 360,\n",
       "    \"tossing coin\": 361,\n",
       "    \"tossing salad\": 362,\n",
       "    \"training dog\": 363,\n",
       "    \"trapezing\": 364,\n",
       "    \"trimming or shaving beard\": 365,\n",
       "    \"trimming trees\": 366,\n",
       "    \"triple jump\": 367,\n",
       "    \"tying bow tie\": 368,\n",
       "    \"tying knot (not on a tie)\": 369,\n",
       "    \"tying tie\": 370,\n",
       "    \"unboxing\": 371,\n",
       "    \"unloading truck\": 372,\n",
       "    \"using computer\": 373,\n",
       "    \"using remote controller (not gaming)\": 374,\n",
       "    \"using segway\": 375,\n",
       "    \"vault\": 376,\n",
       "    \"waiting in line\": 377,\n",
       "    \"walking the dog\": 378,\n",
       "    \"washing dishes\": 379,\n",
       "    \"washing feet\": 380,\n",
       "    \"washing hair\": 381,\n",
       "    \"washing hands\": 382,\n",
       "    \"water skiing\": 383,\n",
       "    \"water sliding\": 384,\n",
       "    \"watering plants\": 385,\n",
       "    \"waxing back\": 386,\n",
       "    \"waxing chest\": 387,\n",
       "    \"waxing eyebrows\": 388,\n",
       "    \"waxing legs\": 389,\n",
       "    \"weaving basket\": 390,\n",
       "    \"welding\": 391,\n",
       "    \"whistling\": 392,\n",
       "    \"windsurfing\": 393,\n",
       "    \"wrapping present\": 394,\n",
       "    \"wrestling\": 395,\n",
       "    \"writing\": 396,\n",
       "    \"yawning\": 397,\n",
       "    \"yoga\": 398,\n",
       "    \"zumba\": 399\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"model_type\": \"videomae\",\n",
       "  \"norm_pix_loss\": false,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_frames\": 25,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"patch_size\": 16,\n",
       "  \"qkv_bias\": true,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.26.1\",\n",
       "  \"tubelet_size\": 2,\n",
       "  \"use_mean_pooling\": true\n",
       "}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "691f6248",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.dropout_1 = nn.Dropout(p=0.3)\n",
    "        self.dropout_2 = nn.Dropout(p=0.3)\n",
    "        self.cls_layer_1 = nn.LazyLinear(100)\n",
    "        self.cls_layer_2 = nn.Linear(100,13)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.model(x).last_hidden_state.mean(dim=1)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.cls_layer_1(x)\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.cls_layer_2(x)\n",
    "        x = torch.nn.functional.log_softmax(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42b57bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(model)\n",
    "# ego_model = MyModel(model)\n",
    "# weather_model = MyModel(model)\n",
    "# time_model = MyModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65141d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (model): VideoMAEModel(\n",
       "    (embeddings): VideoMAEEmbeddings(\n",
       "      (patch_embeddings): VideoMAEPatchEmbeddings(\n",
       "        (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
       "      )\n",
       "    )\n",
       "    (encoder): VideoMAEEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (6): VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (7): VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (8): VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (9): VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (10): VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (11): VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout_1): Dropout(p=0.3, inplace=False)\n",
       "  (dropout_2): Dropout(p=0.3, inplace=False)\n",
       "  (cls_layer_1): LazyLinear(in_features=0, out_features=100, bias=True)\n",
       "  (cls_layer_2): Linear(in_features=100, out_features=13, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "477ce134",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for idx,param in enumerate(model.model.parameters()):\n",
    "    if idx>150:\n",
    "        param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75955f78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "414e3138",
   "metadata": {},
   "outputs": [],
   "source": [
    "Alb = A.Compose([\n",
    "        A.Resize(width=CFG['IMG_SIZE'], height=CFG['IMG_SIZE']),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "#         A.OneOf([\n",
    "#             A.Blur(blur_limit=3,p=0.3),\n",
    "#             A.GaussNoise(p=0.3,var_limit=(0, 26)),\n",
    "#             A.Downscale(p=0.3,scale_min=0.7, scale_max=0.99, interpolation=2),\n",
    "# #             A.RandomBrightness(p=0.2, limit=0.05),    \n",
    "#             A.CoarseDropout(p=0.2, max_holes=10, max_height=8, max_width=8, min_holes=5, min_height=2, min_width=2),\n",
    "#         ], p=0.7),\n",
    "#         A.OneOf([\n",
    "#         A.ElasticTransform(p=0.3),\n",
    "#         A.SafeRotate(limit=45,p=0.3),\n",
    "#         ],p=0.7),\n",
    "        \n",
    "        A.Normalize(mean=tuple(image_processor_config.image_mean)\n",
    "                   ,std=tuple(image_processor_config.image_std),p=1)\n",
    "    ], p=1)\n",
    "\n",
    "\n",
    "def aug_video(vid, tfms):\n",
    "    aug_vid = []\n",
    "    for x in vid:\n",
    "        aug_vid.append((tfms(image = np.asarray(x)))['image'])\n",
    "    return torch.from_numpy(np.stack(aug_vid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f3ffee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_path_list, label_list,transform=None):\n",
    "        self.video_path_list = video_path_list\n",
    "        self.label_list = label_list\n",
    "        self.Alb = transform\n",
    "    \n",
    "    def get_labels(self):   \n",
    "        return self.label_list  \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        frames = self.get_video(self.video_path_list[index])\n",
    "        if self.label_list is not None:\n",
    "            label = self.label_list[index]\n",
    "            return frames, label\n",
    "        else:\n",
    "            return frames\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.video_path_list)\n",
    "    \n",
    "    def get_video(self, path):\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        for idx in range(50):\n",
    "            if idx%2 == 1:\n",
    "                _, img = cap.read()\n",
    "#                 img = cv2.resize(img, (CFG['IMG_SIZE'], CFG['IMG_SIZE']))\n",
    "                frames.append(img)\n",
    "        if self.Alb is not None:\n",
    "            frames = aug_video(frames, tfms=self.Alb)\n",
    "        return torch.FloatTensor(np.array(frames)).permute(0, 3, 1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05370681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(skf_idx, model, optimizer, train_loader, val_loader, scheduler, device, cls_type):\n",
    "    model.to(device)\n",
    "    criterion = FocalLoss('multiclass')\n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    achieve = False\n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "      \n",
    "        for videos, labels in tqdm(iter(train_loader)):\n",
    "            videos = videos.to(device)\n",
    "            labels = labels.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "            output = model(videos)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "#             optimizer.step()\n",
    "            optimizer.first_step(zero_grad=True)\n",
    "\n",
    "            criterion(model(videos), labels).backward()\n",
    "            optimizer.second_step(zero_grad=True)\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "       \n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val F1 : [{_val_score:.5f}]')\n",
    "        if _val_score > CFG['VAL_SCORE_THRES']:\n",
    "            achieve=True\n",
    "            print(\"archieve score!!\")\n",
    "#             break\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(_val_score)\n",
    "            \n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "            date=datetime.today().strftime(\"%m_%d_%H_%M\")\n",
    "            torch.save(best_model.state_dict(), './'+cls_type + '_' + str(skf_idx) +'_'+ date + '_best_model.pth')\n",
    "        skf_idx+=1\n",
    "    return best_model,achieve,skf_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51fa21fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    preds, trues = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for videos, labels in tqdm(iter(val_loader)):\n",
    "            videos = videos.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            output = model(videos)\n",
    "            \n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "            \n",
    "            preds += output.argmax(1).detach().cpu().numpy().tolist()\n",
    "            trues += labels.detach().cpu().numpy().tolist()\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "    \n",
    "    _val_score = f1_score(trues, preds, average='macro')\n",
    "    return _val_loss, _val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3663b709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_pl = []\n",
    "\n",
    "# crash = {\n",
    "#     'model' : crash_model,\n",
    "#     'cls_type' : 'crash',\n",
    "#     'df' : crash_df\n",
    "# }\n",
    "\n",
    "# ego = {\n",
    "#     'model' : ego_model,\n",
    "#     'cls_type' : 'ego',\n",
    "#     'df' : ego_df\n",
    "# }\n",
    "\n",
    "# weather = {\n",
    "#     'model' : weather_model,\n",
    "#     'cls_type' : 'weather',\n",
    "#     'df' : weather_df\n",
    "# }\n",
    "\n",
    "# time = {\n",
    "#     'model' : time_model,\n",
    "#     'cls_type' : 'time',\n",
    "#     'df' : time_df\n",
    "# }\n",
    "\n",
    "# dict_pl.append(crash)\n",
    "# dict_pl.append(ego)\n",
    "# dict_pl.append(weather)\n",
    "# dict_pl.append(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b731a4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crash_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1079/1079 [11:46<00:00,  1.53it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [00:58<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [2.70577] Val Loss : [2.64036] Val F1 : [0.02880]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|█████████████████████████████████████████████████████████████                              | 724/1079 [07:53<03:51,  1.53it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset,sampler\u001b[38;5;241m=\u001b[39mImbalancedDatasetSampler(train_dataset),shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,batch_size \u001b[38;5;241m=\u001b[39m CFG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBATCH_SIZE\u001b[39m\u001b[38;5;124m'\u001b[39m],  num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     17\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size \u001b[38;5;241m=\u001b[39m CFG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBATCH_SIZE\u001b[39m\u001b[38;5;124m'\u001b[39m], num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m _,achieve,skf_idx\u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mskf_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcls_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(skf_idx, model, optimizer, train_loader, val_loader, scheduler, device, cls_type)\u001b[0m\n\u001b[1;32m     21\u001b[0m     criterion(model(videos), labels)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39msecond_step(zero_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 24\u001b[0m     train_loss\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     26\u001b[0m _val_loss, _val_score \u001b[38;5;241m=\u001b[39m validation(model, criterion, val_loader, device)\n\u001b[1;32m     28\u001b[0m _train_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(train_loss)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# if CFG['LOAD_WEIGHT'] == True:\n",
    "# checkpoint = torch.load('./checkpoint/crash/crash_4_02_21_16_03_best_model.pth')\n",
    "# crash_model.load_state_dict(checkpoint)\n",
    "\n",
    "print('all_model')\n",
    "apply_df = all_df.copy()\n",
    "cls_type = 'crash'\n",
    "base_optimizer = torch.optim.SGD  # define an optimizer for the \"sharpness-aware\" update\n",
    "optimizer = SAM(model.parameters(), base_optimizer, lr=CFG[\"LEARNING_RATE\"], momentum=0.5)\n",
    "# optimizer = torch.optim.Adam(params = crash_model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3,threshold_mode='abs',min_lr=1e-12, verbose=True)\n",
    "skf_idx=1\n",
    "for train_idx,val_idx in skf.split(apply_df['video_path'],apply_df['label']):\n",
    "    train_dataset = VideoDataset(apply_df['video_path'][train_idx].values, apply_df['label'][train_idx].values,transform=Alb)\n",
    "    val_dataset = VideoDataset(apply_df['video_path'][val_idx].values, apply_df['label'][val_idx].values, transform=Alb)\n",
    "    train_loader = DataLoader(train_dataset,sampler=ImbalancedDatasetSampler(train_dataset),shuffle=False,batch_size = CFG['BATCH_SIZE'],  num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], num_workers=4)\n",
    "    _,achieve,skf_idx= train(skf_idx,model, optimizer, train_loader, val_loader, scheduler, device, cls_type)\n",
    "#         skf_idx+=1\n",
    "#     if achieve == True:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29de8360",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checkpoint = torch.load('./checkpoint/ego_0_02_22_00_44_best_model.pth')\n",
    "\n",
    "\n",
    "# ego_model.load_state_dict(checkpoint)\n",
    "\n",
    "# # from sam import SAM\n",
    "# # !pip install sam\n",
    "# base_optimizer = torch.optim.SGD  # define an optimizer for the \"sharpness-aware\" update\n",
    "# optimizer = SAM(ego_model.parameters(), base_optimizer, lr=CFG[\"LEARNING_RATE\"], momentum=0.5)\n",
    "# print('ego model')\n",
    "# apply_df = ego_df.copy()\n",
    "# cls_type = 'ego'\n",
    "# # optimizer = torch.optim.Adadelta(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "# # optimizer = torch.optim.Adam(params = ego_model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "# # scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda1, lambda2], verbose=True)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3,threshold_mode='abs',min_lr=1e-12, verbose=True)\n",
    "# skf_idx=0\n",
    "# for train_idx,val_idx in skf.split(apply_df['video_path'],apply_df['label']):\n",
    "#     train_dataset = VideoDataset(apply_df['video_path'][train_idx].values, apply_df['label'][train_idx].values,transform=Alb)\n",
    "#     val_dataset = VideoDataset(apply_df['video_path'][val_idx].values, apply_df['label'][val_idx].values, transform=Alb)\n",
    "#     train_loader = DataLoader(train_dataset,sampler=ImbalancedDatasetSampler(train_dataset),shuffle=False,batch_size = CFG['BATCH_SIZE'],  num_workers=4)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], num_workers=4)\n",
    "#     ego_model,achieve,skf_idx = train(skf_idx,ego_model, optimizer, train_loader, val_loader, scheduler, device, cls_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b971f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load('./checkpoint/weather_0_02_22_00_49_best_model.pth')\n",
    "\n",
    "\n",
    "# weather_model.load_state_dict(checkpoint)\n",
    "# print('weather model')\n",
    "# apply_df = weather_df.copy()\n",
    "# cls_type = 'weather'\n",
    "\n",
    "# base_optimizer = torch.optim.SGD  # define an optimizer for the \"sharpness-aware\" update\n",
    "# optimizer = SAM(weather_model.parameters(), base_optimizer, lr=CFG[\"LEARNING_RATE\"], momentum=0.5)\n",
    "# # optimizer = torch.optim.Adam(params = weather_model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3,threshold_mode='abs',min_lr=1e-12, verbose=True)\n",
    "# skf_idx=0\n",
    "# for train_idx,val_idx in skf.split(apply_df['video_path'],apply_df['label']):\n",
    "#     train_dataset = VideoDataset(apply_df['video_path'][train_idx].values, apply_df['label'][train_idx].values,transform=Alb)\n",
    "#     val_dataset = VideoDataset(apply_df['video_path'][val_idx].values, apply_df['label'][val_idx].values, transform=Alb)\n",
    "#     train_loader = DataLoader(train_dataset,sampler=ImbalancedDatasetSampler(train_dataset),shuffle=False,batch_size = CFG['BATCH_SIZE'],  num_workers=4)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], num_workers=4)\n",
    "#     weather_model,achieve,skf_idx = train(skf_idx,weather_model, optimizer, train_loader, val_loader, scheduler, device, cls_type)\n",
    "# #         skf_idx+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07f60b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checkpoint = torch.load('./checkpoint/time_0_02_22_01_01_best_model.pth')\n",
    "\n",
    "\n",
    "# time_model.load_state_dict(checkpoint)\n",
    "# print('time model')\n",
    "# apply_df = time_df.copy()\n",
    "# cls_type = 'time'\n",
    "\n",
    "# base_optimizer = torch.optim.SGD  # define an optimizer for the \"sharpness-aware\" update\n",
    "# optimizer = SAM(time_model.parameters(), base_optimizer, lr=CFG[\"LEARNING_RATE\"], momentum=0.5)\n",
    "\n",
    "# # optimizer = torch.optim.Adam(params = time_model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3,threshold_mode='abs',min_lr=1e-12, verbose=True)\n",
    "# skf_idx=0\n",
    "# for train_idx,val_idx in skf.split(apply_df['video_path'],apply_df['label']):\n",
    "#     train_dataset = VideoDataset(apply_df['video_path'][train_idx].values, apply_df['label'][train_idx].values,transform=Alb)\n",
    "#     val_dataset = VideoDataset(apply_df['video_path'][val_idx].values, apply_df['label'][val_idx].values, transform=Alb)\n",
    "#     train_loader = DataLoader(train_dataset,sampler=ImbalancedDatasetSampler(train_dataset),shuffle=False,batch_size = CFG['BATCH_SIZE'],  num_workers=4)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], num_workers=4)\n",
    "#     time_model,achieve,skf_idx = train(skf_idx,time_model, optimizer, train_loader, val_loader, scheduler, device, cls_type)\n",
    "# #         skf_idx+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd895bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = VideoDataset(test_df['video_path'].values,label_list= None, transform=Alb)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False,batch_size = CFG['BATCH_SIZE']*2,  num_workers=4)\n",
    "   \n",
    "# train_test_dataset = VideoDataset(all_df['video_path'].values,label_list= None, transform=Alb)\n",
    "# train_test_loader = DataLoader(train_test_dataset, shuffle=False,batch_size = CFG['BATCH_SIZE'],  num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d95f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for videos in tqdm(iter(test_loader)):\n",
    "            videos = videos.to(device)\n",
    "            output = model(videos)\n",
    "#             preds += output.logits.argmax(1).detach().cpu().numpy().tolist()\n",
    "            preds += output.logits.detach().cpu().numpy().tolist()\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcde3e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_checkpoint = torch.load('./checkpoint/crash/crash_13_02_22_15_11_best_model.pth')\n",
    "crash_model.load_state_dict(crash_checkpoint)\n",
    "\n",
    "ego_checkpoint = torch.load('./checkpoint/ego/ego_19_02_22_16_23_best_model.pth')\n",
    "ego_model.load_state_dict(ego_checkpoint)\n",
    "\n",
    "weather_checkpoint = torch.load('./checkpoint/weather/weather_18_02_22_17_12_best_model.pth')\n",
    "weather_model.load_state_dict(weather_checkpoint)\n",
    "\n",
    "time_checkpoint = torch.load('./checkpoint/time/time_0_02_22_21_23_best_model.pth')\n",
    "time_model.load_state_dict(time_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7001f7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noncrash_dataset = VideoDataset(noncrash_df['video_path'].values,label_list= None, transform=Alb)\n",
    "# noncrash_loader = DataLoader(noncrash_dataset, shuffle=False,batch_size = CFG['BATCH_SIZE'],  num_workers=4)\n",
    "   \n",
    "# g_time_preds = inference(time_model, noncrash_loader, device)\n",
    "# g_ego_preds = inference(ego_model, noncrash_loader, device)\n",
    "# g_weahter_preds = inference(weather_model, noncrash_loader, device)\n",
    "\n",
    "# idx = weather_g_df[weather_g_df['label']==0].index\n",
    "# weather_g_df.drop(idx,inplace=True)\n",
    "# print(len(weather_g_df[weather_g_df['label']==2]))\n",
    "# weather_g_df.to_csv('./weather_g_df.csv', index=False)\n",
    "\n",
    "# time_g_df = generate_label(g_time_preds,noncrash_df)\n",
    "# time_g_df.to_csv('./time_g_df.csv', index=False)\n",
    "\n",
    "# ego_g_df = generate_label(ego_preds,noncrash_df)\n",
    "# ego_g_df.to_csv('./ego_g_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fb5949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_label(pred,ref_df):\n",
    "    generate_data = pd.DataFrame(pred,columns = ['one','two'])\n",
    "    generate_data_index = generate_data[abs(generate_data['one']-generate_data['two'])>CFG['DATA_GENERATE_THRES']].index.values.tolist()\n",
    "    generate_data_list = generate_data[abs(generate_data['one']-generate_data['two'])>CFG['DATA_GENERATE_THRES']].values.tolist()\n",
    "    label = np.array(generate_data_list).argmax(1)\n",
    "    path=ref_df['video_path'][generate_data_index].values.tolist()\n",
    "    g_df = pd.DataFrame(columns = ['video_path','label'])\n",
    "    g_df['video_path'] = path\n",
    "    g_df['label'] = label\n",
    "    return g_df\n",
    "# df.max(axis = 1, numeric_only = True)\n",
    "def generate_weather_label(pred,ref_df):\n",
    "    generate_data = pd.DataFrame(pred,columns = ['one','two','three'])\n",
    "    generate_data_index = generate_data[generate_data.max(axis=1,numeric_only=True)>2].index.values.tolist()\n",
    "    generate_data_list = generate_data[generate_data.max(axis=1,numeric_only=True)>2].values.tolist()\n",
    "\n",
    "    label = np.array(generate_data_list).argmax(1)\n",
    "    path=ref_df['video_path'][generate_data_index].values.tolist()\n",
    "    g_df = pd.DataFrame(columns = ['video_path','label'])\n",
    "    g_df['video_path'] = path\n",
    "    g_df['label'] = label\n",
    "    return g_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f55d35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_preds_list = []\n",
    "crash_preds_list=[]\n",
    "ego_preds_list=[]\n",
    "weather_preds_list=[]\n",
    "time_preds_list=[]\n",
    "crash_preds=None\n",
    "ego_preds=None\n",
    "weather_preds=None\n",
    "time_preds=None\n",
    "\n",
    "\n",
    "for idx in range(CFG['NUM_ASB']):\n",
    "    crash_preds = inference(crash_model, test_loader, device)\n",
    "    crash_preds_list.append(crash_preds)\n",
    "    \n",
    "crash_pred_sum = np.sum(crash_preds_list,axis=0)\n",
    "crash_max = crash_pred_sum.argmax(1).tolist()\n",
    "\n",
    "for idx in range(CFG['NUM_ASB']):\n",
    "    ego_preds = inference(ego_model, test_loader, device)\n",
    "    ego_preds_list.append(ego_preds)\n",
    "    \n",
    "ego_pred_sum = np.sum(ego_preds_list,axis=0)\n",
    "ego_max = ego_pred_sum.argmax(1).tolist()\n",
    "    \n",
    "for idx in range(CFG['NUM_ASB']):\n",
    "    weather_preds = inference(weather_model, test_loader, device)\n",
    "    weather_preds_list.append(weather_preds)\n",
    "\n",
    "weather_pred_sum = np.sum(weather_preds_list,axis=0)\n",
    "weather_max = weather_pred_sum.argmax(1).tolist()\n",
    "    \n",
    "for idx in range(CFG['NUM_ASB']):\n",
    "    time_preds = inference(time_model, test_loader, device)\n",
    "    time_preds_list.append(time_preds)\n",
    "\n",
    "time_pred_sum = np.sum(time_preds_list,axis=0)\n",
    "time_max = time_pred_sum.argmax(1).tolist()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bdd37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crash_preds_df = pd.DataFrame(crash_preds,columns=['crash'])\n",
    "# ego_preds_df = pd.DataFrame(ego_preds,columns=['ego'])\n",
    "# weather_preds_df = pd.DataFrame(weather_preds,columns=['weather'])\n",
    "# time_preds_df = pd.DataFrame(time_preds,columns=['time'])\n",
    "\n",
    "preds=[]\n",
    "\n",
    "print(len(crash_preds))\n",
    "print(len(ego_preds))\n",
    "print(len(weather_preds))\n",
    "print(len(time_preds))\n",
    "\n",
    "for idx,crash in enumerate(crash_max):\n",
    "    ego = ego_max[idx]\n",
    "    weather = weather_max[idx]\n",
    "    time = time_max[idx]\n",
    "    if crash == 0:\n",
    "        preds.append(0)\n",
    "    else:\n",
    "        if ego==0:\n",
    "            if weather==0:\n",
    "                if time == 0:\n",
    "                    preds.append(7)\n",
    "                else:\n",
    "                    preds.append(8)\n",
    "            elif weather==1:\n",
    "                if time == 0:\n",
    "                    preds.append(9)\n",
    "                else:\n",
    "                    preds.append(10)\n",
    "            else:\n",
    "                if time == 0:\n",
    "                    preds.append(11)\n",
    "                else:\n",
    "                    preds.append(12)\n",
    "                    \n",
    "        else:\n",
    "            if weather==0:\n",
    "                if time == 0:\n",
    "                    preds.append(1)\n",
    "                else:\n",
    "                    preds.append(2)\n",
    "            elif weather==1:\n",
    "                if time == 0:\n",
    "                    preds.append(3)\n",
    "                else:\n",
    "                    preds.append(4)\n",
    "            else:\n",
    "                if time == 0:\n",
    "                    preds.append(5)\n",
    "                else:\n",
    "                    preds.append(6)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39d5d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # all_df['label'].values.tolist()\n",
    "# gt = crash_df['label'].values.tolist()\n",
    "# f1_score(gt,crash_max,average='macro')\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bf40f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# index = (np.array(preds) != np.array(gt))\n",
    "# len(index==True)\n",
    "eff = np.where(np.array(preds) != np.array(gt))\n",
    "eff[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b85f91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gt[1758])\n",
    "print(preds[1758])\n",
    "#weather.., crash.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb0231",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# path = all_df['video_path'][649]\n",
    "path = all_df['video_path'][343]\n",
    "frames=[]\n",
    "cap = cv2.VideoCapture(path)\n",
    "for idx in range(50):\n",
    "    if idx%5 == 3:\n",
    "        _, img = cap.read()\n",
    "        frames.append(img)\n",
    "visualize(frames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac56625",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = all_df['label'].values.tolist()\n",
    "f1_score(gt, preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610ac0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./data/sample_submission.csv')\n",
    "# comp = pd.read_csv('./submit/2023_02_21_10_44_04.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9741e2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit['label'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c079b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comp['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beda436d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submit['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2374bb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "date=datetime.today().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "submit.to_csv('./'+date+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de756792",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
