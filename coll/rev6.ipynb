{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1db13f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchsampler\n",
    "#https://github.com/ufoym/imbalanced-dataset-sampler\n",
    "\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     train_dataset,\n",
    "#     sampler=ImbalancedDatasetSampler(train_dataset),\n",
    "#     batch_size=args.batch_size,\n",
    "#     **kwargs\n",
    "# )\n",
    "#https://www.kaggle.com/competitions/dfl-bundesliga-data-shootout/discussion/360236\n",
    "#model kinetic version\n",
    "#label seperate\n",
    "#video augmentation\n",
    "#imbalance data\n",
    "#focal loss\n",
    "#https://github.com/HHTseng/video-classification\n",
    "#https://huggingface.co/models?other=video-classification\n",
    "#앙상블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed9ba04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torchsampler import ImbalancedDatasetSampler\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "import albumentations as A\n",
    "from datetime import datetime\n",
    "# from einops import rearrange\n",
    "# from decord import VideoReader\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from segmentation_models_pytorch.losses import FocalLoss\n",
    "# from transformers import AutoModel, AutoImageProcessor, AutoConfig\n",
    "# from skmultilearn.model_selection import iterative_train_test_split\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorchvideo.transforms.transforms_factory import create_video_transform\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoModel, AutoImageProcessor, AutoConfig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfb75f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://github.com/davda54/sam\n",
    "\n",
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                e_w = p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "                self.state[p][\"e_w\"] = e_w\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
    "\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        p.grad.norm(p=2).to(shared_device)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a95a426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f09879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "all_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ef3570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CFG = {\n",
    "    'VIDEO_LENGTH':10, \n",
    "    'IMG_SIZE':235,\n",
    "    'EPOCHS':5,\n",
    "    'LEARNING_RATE':1e-5,\n",
    "    'BATCH_SIZE':2,\n",
    "    'SEED':2023,\n",
    "    'SPLIT':5,\n",
    "    'ROOT':'./data',\n",
    "    'MODEL':'MCG-NJU/videomae-base-finetuned-ssv2',\n",
    "    'LOAD_WEIGHT':True\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b42aa213",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits = CFG['SPLIT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee5d0e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['video_path'] = all_df['video_path'].apply(lambda x:CFG['ROOT']+x[1:])\n",
    "test_df['video_path'] = test_df['video_path'].apply(lambda x:CFG['ROOT']+x[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65e1c289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>video_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./data/train/TRAIN_0000.mp4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./data/train/TRAIN_0001.mp4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>./data/train/TRAIN_0002.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>./data/train/TRAIN_0003.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./data/train/TRAIN_0004.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>TRAIN_2693</td>\n",
       "      <td>./data/train/TRAIN_2693.mp4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694</th>\n",
       "      <td>TRAIN_2694</td>\n",
       "      <td>./data/train/TRAIN_2694.mp4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>TRAIN_2695</td>\n",
       "      <td>./data/train/TRAIN_2695.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>TRAIN_2696</td>\n",
       "      <td>./data/train/TRAIN_2696.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>TRAIN_2697</td>\n",
       "      <td>./data/train/TRAIN_2697.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2698 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sample_id                   video_path  label\n",
       "0     TRAIN_0000  ./data/train/TRAIN_0000.mp4      7\n",
       "1     TRAIN_0001  ./data/train/TRAIN_0001.mp4      7\n",
       "2     TRAIN_0002  ./data/train/TRAIN_0002.mp4      0\n",
       "3     TRAIN_0003  ./data/train/TRAIN_0003.mp4      0\n",
       "4     TRAIN_0004  ./data/train/TRAIN_0004.mp4      1\n",
       "...          ...                          ...    ...\n",
       "2693  TRAIN_2693  ./data/train/TRAIN_2693.mp4      3\n",
       "2694  TRAIN_2694  ./data/train/TRAIN_2694.mp4      5\n",
       "2695  TRAIN_2695  ./data/train/TRAIN_2695.mp4      0\n",
       "2696  TRAIN_2696  ./data/train/TRAIN_2696.mp4      0\n",
       "2697  TRAIN_2697  ./data/train/TRAIN_2697.mp4      0\n",
       "\n",
       "[2698 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fed98cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58170cb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base-finetuned-ssv2 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([174, 768]) in the checkpoint and torch.Size([13, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([174]) in the checkpoint and torch.Size([13]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import VideoMAEConfig, VideoMAEModel\n",
    "from transformers import AutoImageProcessor, VideoMAEForVideoClassification\n",
    "from transformers import XCLIPVisionModel, XCLIPVisionConfig\n",
    "\n",
    "configuration = VideoMAEConfig()\n",
    "configuration = AutoConfig.from_pretrained(CFG['MODEL'])\n",
    "\n",
    "configuration.num_frames = CFG['VIDEO_LENGTH']\n",
    "configuration.num_frames=CFG['VIDEO_LENGTH']\n",
    "configuration.image_size=CFG['IMG_SIZE']\n",
    "configuration.id2label = {0:'no crash',1:'crash ego normal day',2:'crash ego normal night',3:'crash ego snow day',4:'crash ego snow night',\n",
    "                         5:'crash ego rain day',6:'crash ego rain night',7:'crash other normal day',8:'crash other normal night',\n",
    "                         9:'crash other snow day',10:'crash other snow night',11:'crash other rain day',12:'crash other rain night'}\n",
    "configuration.label2id = {'no crash':0,'crash ego normal day':1,'crash ego normal night':2,'crash ego snow day':3,'crash ego snow night':4,\n",
    "                         'crash ego rain day':5,'crash ego rain night':6,'crash other normal day':7,'crash other normal night':8,\n",
    "                         'crash other snow day':9,'crash other snow night':10,'crash other rain day':11,'crash other rain night':12}\n",
    "\n",
    "image_processor_config = AutoImageProcessor.from_pretrained(CFG['MODEL'])\n",
    "\n",
    "mae_model = VideoMAEForVideoClassification.from_pretrained(CFG['MODEL'],config=configuration,ignore_mismatched_sizes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "414e3138",
   "metadata": {},
   "outputs": [],
   "source": [
    "Alb = A.Compose([\n",
    "        A.Resize(width=CFG['IMG_SIZE'], height=CFG['IMG_SIZE']),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.OneOf([\n",
    "            A.Blur(blur_limit=3,p=0.3),\n",
    "            A.GaussNoise(p=0.3,var_limit=(0, 26)),\n",
    "            A.Downscale(p=0.3,scale_min=0.7, scale_max=0.99, interpolation=2),\n",
    "#             A.RandomBrightness(p=0.2, limit=0.05),    \n",
    "#             A.CoarseDropout(p=0.2, max_holes=10, max_height=8, max_width=8, min_holes=5, min_height=2, min_width=2),\n",
    "        ], p=0.7),\n",
    "        A.Normalize(mean=tuple(image_processor_config.image_mean)\n",
    "                   ,std=tuple(image_processor_config.image_std))\n",
    "#             A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "\n",
    "    ], p=1)\n",
    "\n",
    "\n",
    "def aug_video(vid, tfms):\n",
    "    aug_vid = []\n",
    "    for x in vid:\n",
    "        aug_vid.append((tfms(image = np.asarray(x)))['image'])\n",
    "    return torch.from_numpy(np.stack(aug_vid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f3ffee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_path_list, label_list,transform=None):\n",
    "        self.video_path_list = video_path_list\n",
    "        self.label_list = label_list\n",
    "        self.Alb = transform\n",
    "    \n",
    "    def get_labels(self):   \n",
    "        return self.label_list  \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        frames = self.get_video(self.video_path_list[index])\n",
    "        if self.label_list is not None:\n",
    "            label = self.label_list[index]\n",
    "            return frames, label\n",
    "        else:\n",
    "            return frames\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.video_path_list)\n",
    "    \n",
    "    def get_video(self, path):\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        for idx in range(50):\n",
    "            if idx%5 == 3:\n",
    "                _, img = cap.read()\n",
    "                img = cv2.resize(img, (CFG['IMG_SIZE'], CFG['IMG_SIZE']))\n",
    "                frames.append(img)\n",
    "        if self.Alb is not None:\n",
    "            frames = aug_video(frames, tfms=self.Alb)\n",
    "        return torch.FloatTensor(np.array(frames)).permute(0, 3, 1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05370681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    model.to(device)\n",
    "    criterion = FocalLoss('multiclass')\n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for videos, labels in tqdm(iter(train_loader)):\n",
    "            videos = videos.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad() \n",
    "            output = model(videos)\n",
    "            loss = criterion(output.logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val F1 : [{_val_score:.5f}]')\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(_val_score)\n",
    "            \n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "            date=datetime.today().strftime(\"%m_%d_%H_%M\")\n",
    "            torch.save(best_model.state_dict(), './'+'all_adam' +'_'+ date + '_best_model.pth')\n",
    "\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d28aae5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sam_train(skf_idx, model, optimizer, train_loader, val_loader, scheduler, device, cls_type):\n",
    "    model.to(device)\n",
    "    criterion = FocalLoss('multiclass')\n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "      \n",
    "        for videos, labels in tqdm(iter(train_loader)):\n",
    "            videos = videos.to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = model(videos)\n",
    "            loss = criterion(output.logits, labels)\n",
    "            loss.backward()\n",
    "#             optimizer.step()\n",
    "            optimizer.first_step(zero_grad=True)\n",
    "\n",
    "            criterion(model(videos).logits, labels).backward()\n",
    "            optimizer.second_step(zero_grad=True)\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "       \n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val F1 : [{_val_score:.5f}]')\n",
    "    \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(_val_score)\n",
    "            \n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "            date=datetime.today().strftime(\"%m_%d_%H_%M\")\n",
    "            torch.save(best_model.state_dict(), './'+cls_type + '_' + str(skf_idx) +'_'+ date + '_best_model.pth')\n",
    "        skf_idx+=1\n",
    "    return best_model,achieve,skf_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51fa21fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    preds, trues = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for videos, labels in tqdm(iter(val_loader)):\n",
    "            videos = videos.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            output = model(videos)\n",
    "            loss = criterion(output.logits, labels)\n",
    "            val_loss.append(loss.item())\n",
    "            \n",
    "            preds += output.logits.argmax(1).detach().cpu().numpy().tolist()\n",
    "            trues += labels.detach().cpu().numpy().tolist()\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "    \n",
    "    _val_score = f1_score(trues, preds, average='macro')\n",
    "    return _val_loss, _val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "795f3b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# cls_type = 'all'\n",
    "# base_optimizer = torch.optim.SGD  # define an optimizer for the \"sharpness-aware\" update\n",
    "# optimizer = SAM(mae_model.parameters(), base_optimizer, lr=CFG[\"LEARNING_RATE\"], momentum=0.1)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3,threshold_mode='abs',min_lr=1e-12, verbose=True)\n",
    "# skf_idx=1\n",
    "# for train_idx,val_idx in skf.split(all_df['video_path'],all_df['label']):\n",
    "#     train_dataset = VideoDataset(all_df['video_path'][train_idx].values, all_df['label'][train_idx].values,transform=Alb)\n",
    "#     val_dataset = VideoDataset(all_df['video_path'][val_idx].values, all_df['label'][val_idx].values, transform=Alb)\n",
    "#     train_loader = DataLoader(train_dataset,sampler=ImbalancedDatasetSampler(train_dataset),shuffle=False,batch_size = CFG['BATCH_SIZE'],  num_workers=4)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], num_workers=4)\n",
    "#     _,achieve,skf_idx= sam_train(skf_idx,mae_model, optimizer, train_loader, val_loader, scheduler, device, cls_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b731a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1079/1079 [03:18<00:00,  5.44it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [00:17<00:00, 15.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.57810] Val Loss : [0.41034] Val F1 : [0.33898]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1079/1079 [03:18<00:00,  5.44it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [00:17<00:00, 15.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.21128] Val Loss : [0.33104] Val F1 : [0.30790]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1079/1079 [03:17<00:00,  5.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [00:17<00:00, 15.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.12210] Val Loss : [0.33924] Val F1 : [0.26983]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1079/1079 [03:17<00:00,  5.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [00:17<00:00, 15.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [0.07852] Val Loss : [0.25827] Val F1 : [0.34705]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1079/1079 [03:17<00:00,  5.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [00:17<00:00, 15.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Train Loss : [0.07304] Val Loss : [0.25743] Val F1 : [0.35562]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1079/1079 [03:17<00:00,  5.47it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [00:17<00:00, 15.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.13194] Val Loss : [0.12368] Val F1 : [0.91068]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1079/1079 [03:17<00:00,  5.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [00:17<00:00, 15.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.05741] Val Loss : [0.10082] Val F1 : [0.92803]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1079/1079 [03:17<00:00,  5.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [00:17<00:00, 15.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.04634] Val Loss : [0.10368] Val F1 : [0.90011]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1079/1079 [03:17<00:00,  5.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [00:17<00:00, 15.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [0.03076] Val Loss : [0.11501] Val F1 : [0.88973]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1079/1079 [03:17<00:00,  5.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [00:17<00:00, 15.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Train Loss : [0.03278] Val Loss : [0.15706] Val F1 : [0.85225]\n",
      "Epoch    10: reducing learning rate of group 0 to 5.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1079/1079 [03:17<00:00,  5.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [00:17<00:00, 15.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.03134] Val Loss : [0.02824] Val F1 : [0.99081]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1079/1079 [03:17<00:00,  5.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [00:17<00:00, 15.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.02126] Val Loss : [0.02521] Val F1 : [0.98999]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1079/1079 [03:17<00:00,  5.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [00:17<00:00, 15.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.01566] Val Loss : [0.02506] Val F1 : [0.99213]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1079/1079 [03:17<00:00,  5.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [00:17<00:00, 15.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [0.01018] Val Loss : [0.04799] Val F1 : [0.97183]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1079/1079 [03:17<00:00,  5.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [00:17<00:00, 15.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Train Loss : [0.00934] Val Loss : [0.04009] Val F1 : [0.97492]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1080/1080 [03:17<00:00,  5.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [00:17<00:00, 15.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.02014] Val Loss : [0.01738] Val F1 : [0.98744]\n",
      "Epoch    16: reducing learning rate of group 0 to 2.5000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1080/1080 [03:17<00:00,  5.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [00:16<00:00, 15.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.00926] Val Loss : [0.00739] Val F1 : [0.99735]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1080/1080 [03:17<00:00,  5.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [00:17<00:00, 15.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.00587] Val Loss : [0.00715] Val F1 : [0.99791]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1080/1080 [03:17<00:00,  5.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [00:17<00:00, 15.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [0.00497] Val Loss : [0.00590] Val F1 : [0.99868]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1080/1080 [03:17<00:00,  5.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [00:17<00:00, 15.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Train Loss : [0.00315] Val Loss : [0.00434] Val F1 : [1.00000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1080/1080 [03:17<00:00,  5.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [00:17<00:00, 15.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.00204] Val Loss : [0.00185] Val F1 : [1.00000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1080/1080 [03:17<00:00,  5.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [00:16<00:00, 15.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.00130] Val Loss : [0.00162] Val F1 : [1.00000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1080/1080 [03:17<00:00,  5.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [00:17<00:00, 15.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.00085] Val Loss : [0.00187] Val F1 : [1.00000]\n",
      "Epoch    23: reducing learning rate of group 0 to 1.2500e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1080/1080 [03:17<00:00,  5.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [00:17<00:00, 15.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [0.00300] Val Loss : [0.00153] Val F1 : [1.00000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1080/1080 [03:17<00:00,  5.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [00:17<00:00, 15.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Train Loss : [0.00077] Val Loss : [0.00082] Val F1 : [1.00000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(params = mae_model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2,threshold_mode='abs',min_lr=1e-10, verbose=True)\n",
    "\n",
    "# if CFG['LOAD_WEIGHT'] == True:\n",
    "#     checkpoint = torch.load('./best_model.pth')\n",
    "#     mae_model.load_state_dict(checkpoint)\n",
    "\n",
    "\n",
    "for train_idx,val_idx in skf.split(all_df['video_path'],all_df['label']):\n",
    "    train_dataset = VideoDataset(all_df['video_path'][train_idx].values, all_df['label'][train_idx].values,transform=Alb)\n",
    "    val_dataset = VideoDataset(all_df['video_path'][val_idx].values, all_df['label'][val_idx].values, transform=Alb)\n",
    "    train_loader = DataLoader(train_dataset,sampler=ImbalancedDatasetSampler(train_dataset),shuffle=False,batch_size = CFG['BATCH_SIZE'],  num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], num_workers=4)\n",
    "    infer_model = train(mae_model, optimizer, train_loader, val_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd895bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = VideoDataset(test_df['video_path'].values,label_list= None, transform=Alb)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False,batch_size = CFG['BATCH_SIZE']*2,  num_workers=4)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5dce525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for videos in tqdm(iter(test_loader)):\n",
    "            videos = videos.to(device)\n",
    "            output = model(videos)\n",
    "#             preds += output.logits.argmax(1).detach().cpu().numpy().tolist()\n",
    "            preds += output.logits.detach().cpu().numpy().tolist()\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d95f675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 450/450 [00:52<00:00,  8.62it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 450/450 [00:52<00:00,  8.51it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 450/450 [00:52<00:00,  8.52it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 450/450 [00:53<00:00,  8.49it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 450/450 [00:52<00:00,  8.50it/s]\n"
     ]
    }
   ],
   "source": [
    "preds_list = []\n",
    "\n",
    "preds=None\n",
    "\n",
    "\n",
    "for idx in range(5):\n",
    "    preds = inference(mae_model, test_loader, device)\n",
    "    preds_list.append(preds)\n",
    "    \n",
    "pred_sum = np.sum(preds_list,axis=0)\n",
    "pred_max = pred_sum.argmax(1).tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11f55d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./data/sample_submission.csv')\n",
    "submit['label'] = pred_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "92bdd37e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1217\n",
       "1      240\n",
       "7      203\n",
       "3       63\n",
       "2       33\n",
       "11      19\n",
       "8       13\n",
       "9        8\n",
       "5        3\n",
       "4        1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b23d178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "date=datetime.today().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "submit.to_csv('./'+date+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7efe081",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
