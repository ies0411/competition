{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1db13f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pseudo label -> randomrain\n",
    "#crash auto label and confidence기반 수작업 -> night\n",
    "# pip uninstall opencv-python-headless==4.5.5.62\n",
    "#pip install opencv-python-headless==4.5.2.52\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed9ba04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torchsampler import ImbalancedDatasetSampler\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "import albumentations as A\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from segmentation_models_pytorch.losses import FocalLoss\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoImageProcessor, AutoConfig\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eee9bfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://github.com/davda54/sam\n",
    "\n",
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                e_w = p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "                self.state[p][\"e_w\"] = e_w\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
    "\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        p.grad.norm(p=2).to(shared_device)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a95a426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f09879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "all_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0de55a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>video_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./train/TRAIN_0000.mp4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./train/TRAIN_0001.mp4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>./train/TRAIN_0002.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>./train/TRAIN_0003.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./train/TRAIN_0004.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>TRAIN_2693</td>\n",
       "      <td>./train/TRAIN_2693.mp4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694</th>\n",
       "      <td>TRAIN_2694</td>\n",
       "      <td>./train/TRAIN_2694.mp4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>TRAIN_2695</td>\n",
       "      <td>./train/TRAIN_2695.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>TRAIN_2696</td>\n",
       "      <td>./train/TRAIN_2696.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>TRAIN_2697</td>\n",
       "      <td>./train/TRAIN_2697.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2698 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sample_id              video_path  label\n",
       "0     TRAIN_0000  ./train/TRAIN_0000.mp4      7\n",
       "1     TRAIN_0001  ./train/TRAIN_0001.mp4      7\n",
       "2     TRAIN_0002  ./train/TRAIN_0002.mp4      0\n",
       "3     TRAIN_0003  ./train/TRAIN_0003.mp4      0\n",
       "4     TRAIN_0004  ./train/TRAIN_0004.mp4      1\n",
       "...          ...                     ...    ...\n",
       "2693  TRAIN_2693  ./train/TRAIN_2693.mp4      3\n",
       "2694  TRAIN_2694  ./train/TRAIN_2694.mp4      5\n",
       "2695  TRAIN_2695  ./train/TRAIN_2695.mp4      0\n",
       "2696  TRAIN_2696  ./train/TRAIN_2696.mp4      0\n",
       "2697  TRAIN_2697  ./train/TRAIN_2697.mp4      0\n",
       "\n",
       "[2698 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "865dbbcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1783\n",
       "1      318\n",
       "7      317\n",
       "3       78\n",
       "2       51\n",
       "9       34\n",
       "11      33\n",
       "8       30\n",
       "5       28\n",
       "4       13\n",
       "12       6\n",
       "10       4\n",
       "6        3\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ef3570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CFG = {\n",
    "    'VIDEO_LENGTH':10, \n",
    "    'IMG_SIZE':250,\n",
    "    'EPOCHS':4,\n",
    "    'LEARNING_RATE':1e-5,\n",
    "    'BATCH_SIZE':2,\n",
    "    'SEED':2023,\n",
    "    'SPLIT':5,\n",
    "    'ROOT':'./data',\n",
    "    'MODEL':'MCG-NJU/videomae-base-short',\n",
    "    'LOAD_WEIGHT':False,\n",
    "    'LOAD_WEIGHT_NAME' :'a',\n",
    "    'VAL_SCORE_THRES' : 0.98,\n",
    "    'NUM_ASB':4,\n",
    "    'DATA_GENERATE_THRES':6.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b42aa213",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits = CFG['SPLIT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee5d0e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['video_path'] = all_df['video_path'].apply(lambda x:CFG['ROOT']+x[1:])\n",
    "test_df['video_path'] = test_df['video_path'].apply(lambda x:CFG['ROOT']+x[1:])\n",
    "\n",
    "crash_df = all_df.copy()\n",
    "crash_df['label'] = crash_df['label'].apply(lambda x: 1 if x != 0 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d72cd202",
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_df = all_df.copy()\n",
    "idx = ego_df[ego_df['label']==0].index\n",
    "ego_df.drop(idx,inplace=True)\n",
    "#0:no 1:yes\n",
    "ego_df['label'] = ego_df['label'].apply(lambda x: 1 if x < 7 else 0)\n",
    "ego_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba66f9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = all_df.copy()\n",
    "idx = weather_df[weather_df['label']==0].index\n",
    "weather_df.drop(idx,inplace=True)\n",
    "#0:normal,1:snow,2:rain\n",
    "weather_df['label'] = weather_df['label'].apply(lambda x: 0 if x==1 or x==2 or x==7 or x==8 else 1 if x==3 or x==4 or x==8 or x==9 else 2)\n",
    "weather_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cd0fd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df = all_df.copy()\n",
    "idx = time_df[time_df['label']==0].index\n",
    "time_df.drop(idx,inplace=True)\n",
    "#0:day 1:night\n",
    "time_df['label'] = time_df['label'].apply(lambda x: 0 if x%2==1 else 1)\n",
    "time_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65e1c289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>video_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./data/train/TRAIN_0000.mp4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./data/train/TRAIN_0001.mp4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>./data/train/TRAIN_0002.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>./data/train/TRAIN_0003.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./data/train/TRAIN_0004.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>TRAIN_2693</td>\n",
       "      <td>./data/train/TRAIN_2693.mp4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694</th>\n",
       "      <td>TRAIN_2694</td>\n",
       "      <td>./data/train/TRAIN_2694.mp4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>TRAIN_2695</td>\n",
       "      <td>./data/train/TRAIN_2695.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>TRAIN_2696</td>\n",
       "      <td>./data/train/TRAIN_2696.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>TRAIN_2697</td>\n",
       "      <td>./data/train/TRAIN_2697.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2698 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sample_id                   video_path  label\n",
       "0     TRAIN_0000  ./data/train/TRAIN_0000.mp4      7\n",
       "1     TRAIN_0001  ./data/train/TRAIN_0001.mp4      7\n",
       "2     TRAIN_0002  ./data/train/TRAIN_0002.mp4      0\n",
       "3     TRAIN_0003  ./data/train/TRAIN_0003.mp4      0\n",
       "4     TRAIN_0004  ./data/train/TRAIN_0004.mp4      1\n",
       "...          ...                          ...    ...\n",
       "2693  TRAIN_2693  ./data/train/TRAIN_2693.mp4      3\n",
       "2694  TRAIN_2694  ./data/train/TRAIN_2694.mp4      5\n",
       "2695  TRAIN_2695  ./data/train/TRAIN_2695.mp4      0\n",
       "2696  TRAIN_2696  ./data/train/TRAIN_2696.mp4      0\n",
       "2697  TRAIN_2697  ./data/train/TRAIN_2697.mp4      0\n",
       "\n",
       "[2698 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b433f3c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>video_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./data/train/TRAIN_0000.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./data/train/TRAIN_0001.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./data/train/TRAIN_0004.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0006</td>\n",
       "      <td>./data/train/TRAIN_0006.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0007</td>\n",
       "      <td>./data/train/TRAIN_0007.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>TRAIN_2685</td>\n",
       "      <td>./data/train/TRAIN_2685.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>TRAIN_2689</td>\n",
       "      <td>./data/train/TRAIN_2689.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>TRAIN_2692</td>\n",
       "      <td>./data/train/TRAIN_2692.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>TRAIN_2693</td>\n",
       "      <td>./data/train/TRAIN_2693.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>TRAIN_2694</td>\n",
       "      <td>./data/train/TRAIN_2694.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>915 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sample_id                   video_path  label\n",
       "0    TRAIN_0000  ./data/train/TRAIN_0000.mp4      0\n",
       "1    TRAIN_0001  ./data/train/TRAIN_0001.mp4      0\n",
       "2    TRAIN_0004  ./data/train/TRAIN_0004.mp4      1\n",
       "3    TRAIN_0006  ./data/train/TRAIN_0006.mp4      1\n",
       "4    TRAIN_0007  ./data/train/TRAIN_0007.mp4      0\n",
       "..          ...                          ...    ...\n",
       "910  TRAIN_2685  ./data/train/TRAIN_2685.mp4      0\n",
       "911  TRAIN_2689  ./data/train/TRAIN_2689.mp4      1\n",
       "912  TRAIN_2692  ./data/train/TRAIN_2692.mp4      0\n",
       "913  TRAIN_2693  ./data/train/TRAIN_2693.mp4      1\n",
       "914  TRAIN_2694  ./data/train/TRAIN_2694.mp4      1\n",
       "\n",
       "[915 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ego_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "660a98ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>video_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./data/train/TRAIN_0000.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./data/train/TRAIN_0001.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>./data/train/TRAIN_0002.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>./data/train/TRAIN_0003.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./data/train/TRAIN_0004.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>TRAIN_2693</td>\n",
       "      <td>./data/train/TRAIN_2693.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694</th>\n",
       "      <td>TRAIN_2694</td>\n",
       "      <td>./data/train/TRAIN_2694.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>TRAIN_2695</td>\n",
       "      <td>./data/train/TRAIN_2695.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>TRAIN_2696</td>\n",
       "      <td>./data/train/TRAIN_2696.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>TRAIN_2697</td>\n",
       "      <td>./data/train/TRAIN_2697.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2698 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sample_id                   video_path  label\n",
       "0     TRAIN_0000  ./data/train/TRAIN_0000.mp4      1\n",
       "1     TRAIN_0001  ./data/train/TRAIN_0001.mp4      1\n",
       "2     TRAIN_0002  ./data/train/TRAIN_0002.mp4      0\n",
       "3     TRAIN_0003  ./data/train/TRAIN_0003.mp4      0\n",
       "4     TRAIN_0004  ./data/train/TRAIN_0004.mp4      1\n",
       "...          ...                          ...    ...\n",
       "2693  TRAIN_2693  ./data/train/TRAIN_2693.mp4      1\n",
       "2694  TRAIN_2694  ./data/train/TRAIN_2694.mp4      1\n",
       "2695  TRAIN_2695  ./data/train/TRAIN_2695.mp4      0\n",
       "2696  TRAIN_2696  ./data/train/TRAIN_2696.mp4      0\n",
       "2697  TRAIN_2697  ./data/train/TRAIN_2697.mp4      0\n",
       "\n",
       "[2698 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crash_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57731127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>video_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./data/train/TRAIN_0000.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./data/train/TRAIN_0001.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./data/train/TRAIN_0004.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0006</td>\n",
       "      <td>./data/train/TRAIN_0006.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0007</td>\n",
       "      <td>./data/train/TRAIN_0007.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>TRAIN_2685</td>\n",
       "      <td>./data/train/TRAIN_2685.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>TRAIN_2689</td>\n",
       "      <td>./data/train/TRAIN_2689.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>TRAIN_2692</td>\n",
       "      <td>./data/train/TRAIN_2692.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>TRAIN_2693</td>\n",
       "      <td>./data/train/TRAIN_2693.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>TRAIN_2694</td>\n",
       "      <td>./data/train/TRAIN_2694.mp4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>915 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sample_id                   video_path  label\n",
       "0    TRAIN_0000  ./data/train/TRAIN_0000.mp4      0\n",
       "1    TRAIN_0001  ./data/train/TRAIN_0001.mp4      0\n",
       "2    TRAIN_0004  ./data/train/TRAIN_0004.mp4      0\n",
       "3    TRAIN_0006  ./data/train/TRAIN_0006.mp4      1\n",
       "4    TRAIN_0007  ./data/train/TRAIN_0007.mp4      0\n",
       "..          ...                          ...    ...\n",
       "910  TRAIN_2685  ./data/train/TRAIN_2685.mp4      0\n",
       "911  TRAIN_2689  ./data/train/TRAIN_2689.mp4      0\n",
       "912  TRAIN_2692  ./data/train/TRAIN_2692.mp4      0\n",
       "913  TRAIN_2693  ./data/train/TRAIN_2693.mp4      1\n",
       "914  TRAIN_2694  ./data/train/TRAIN_2694.mp4      2\n",
       "\n",
       "[915 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bebf1af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>video_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./data/train/TRAIN_0000.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./data/train/TRAIN_0001.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./data/train/TRAIN_0004.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0006</td>\n",
       "      <td>./data/train/TRAIN_0006.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0007</td>\n",
       "      <td>./data/train/TRAIN_0007.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>TRAIN_2685</td>\n",
       "      <td>./data/train/TRAIN_2685.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>TRAIN_2689</td>\n",
       "      <td>./data/train/TRAIN_2689.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>TRAIN_2692</td>\n",
       "      <td>./data/train/TRAIN_2692.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>TRAIN_2693</td>\n",
       "      <td>./data/train/TRAIN_2693.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>TRAIN_2694</td>\n",
       "      <td>./data/train/TRAIN_2694.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>915 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sample_id                   video_path  label\n",
       "0    TRAIN_0000  ./data/train/TRAIN_0000.mp4      0\n",
       "1    TRAIN_0001  ./data/train/TRAIN_0001.mp4      0\n",
       "2    TRAIN_0004  ./data/train/TRAIN_0004.mp4      0\n",
       "3    TRAIN_0006  ./data/train/TRAIN_0006.mp4      0\n",
       "4    TRAIN_0007  ./data/train/TRAIN_0007.mp4      0\n",
       "..          ...                          ...    ...\n",
       "910  TRAIN_2685  ./data/train/TRAIN_2685.mp4      1\n",
       "911  TRAIN_2689  ./data/train/TRAIN_2689.mp4      0\n",
       "912  TRAIN_2692  ./data/train/TRAIN_2692.mp4      0\n",
       "913  TRAIN_2693  ./data/train/TRAIN_2693.mp4      0\n",
       "914  TRAIN_2694  ./data/train/TRAIN_2694.mp4      0\n",
       "\n",
       "[915 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fed98cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58170cb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at MCG-NJU/videomae-base-short were not used when initializing VideoMAEModel: ['decoder.decoder_layers.3.layernorm_before.weight', 'decoder.head.bias', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.0.attention.attention.q_bias', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.1.attention.attention.q_bias', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.norm.weight', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.0.layernorm_before.bias', 'mask_token', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.0.layernorm_after.weight', 'encoder_to_decoder.weight', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.v_bias', 'decoder.decoder_layers.1.attention.attention.v_bias', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.0.attention.attention.v_bias', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.2.attention.attention.q_bias', 'decoder.norm.bias', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.2.attention.attention.v_bias', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.head.weight', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.3.attention.attention.q_bias', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.2.attention.attention.query.weight']\n",
      "- This IS expected if you are initializing VideoMAEModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VideoMAEModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "from transformers import VideoMAEConfig, VideoMAEModel\n",
    "from transformers import AutoImageProcessor, VideoMAEForVideoClassification\n",
    "from transformers import XCLIPVisionModel, XCLIPVisionConfig\n",
    "\n",
    "configuration = VideoMAEConfig()\n",
    "configuration = AutoConfig.from_pretrained(CFG['MODEL'])\n",
    "configuration.num_frames = CFG['VIDEO_LENGTH']\n",
    "configuration.image_size=CFG['IMG_SIZE']\n",
    "configuration.image_size=CFG['IMG_SIZE']\n",
    "configuration.attention_probs_dropout_prob = 0.3\n",
    "configuration.hidden_dropout_prob = 0.3\n",
    "\n",
    "model = AutoModel.from_pretrained(CFG['MODEL'],config= configuration,ignore_mismatched_sizes=True)\n",
    "\n",
    "\n",
    "image_processor_config = AutoImageProcessor.from_pretrained(CFG['MODEL'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e92d9447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoMAEConfig {\n",
       "  \"_name_or_path\": \"MCG-NJU/videomae-base-short\",\n",
       "  \"architectures\": [\n",
       "    \"VideoMAEForPreTraining\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.3,\n",
       "  \"decoder_hidden_size\": 384,\n",
       "  \"decoder_intermediate_size\": 1536,\n",
       "  \"decoder_num_attention_heads\": 6,\n",
       "  \"decoder_num_hidden_layers\": 4,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.3,\n",
       "  \"hidden_size\": 768,\n",
       "  \"image_size\": 250,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"model_type\": \"videomae\",\n",
       "  \"norm_pix_loss\": true,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_frames\": 10,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"patch_size\": 16,\n",
       "  \"qkv_bias\": true,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.26.1\",\n",
       "  \"tubelet_size\": 2,\n",
       "  \"use_mean_pooling\": false\n",
       "}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c403f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.dropout_1 = nn.Dropout(p=0.5)\n",
    "        self.dropout_2 = nn.Dropout(p=0.5)\n",
    "        self.cls_layer_1 = nn.LazyLinear(100)\n",
    "        self.cls_layer_2 = nn.Linear(100,2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.model(x).last_hidden_state.mean(dim=1)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.cls_layer_1(x)\n",
    "#         x = self.dropout_2(x)\n",
    "        x = self.cls_layer_2(x)\n",
    "#         x = torch.nn.functional.log_softmax(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "261c24b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_model = MyModel(model)\n",
    "ego_model = MyModel(model)\n",
    "weather_model = MyModel(model)\n",
    "time_model = MyModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1dafa727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (model): VideoMAEModel(\n",
       "    (embeddings): VideoMAEEmbeddings(\n",
       "      (patch_embeddings): VideoMAEPatchEmbeddings(\n",
       "        (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
       "      )\n",
       "    )\n",
       "    (encoder): VideoMAEEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (6): VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (7): VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (8): VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (9): VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (10): VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (11): VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (dropout_1): Dropout(p=0.5, inplace=False)\n",
       "  (dropout_2): Dropout(p=0.5, inplace=False)\n",
       "  (cls_layer_1): LazyLinear(in_features=0, out_features=100, bias=True)\n",
       "  (cls_layer_2): Linear(in_features=100, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crash_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5f7e174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n"
     ]
    }
   ],
   "source": [
    "for param in crash_model.model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "    \n",
    "for idx,param in enumerate(crash_model.model.parameters()):\n",
    "    print(idx)\n",
    "    if idx>50:\n",
    "        param.requires_grad = True\n",
    "        \n",
    "\n",
    "for param in ego_model.model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for idx,param in enumerate(ego_model.model.parameters()):\n",
    "    if idx>50:\n",
    "        param.requires_grad = True\n",
    "        \n",
    "for param in weather_model.model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for idx,param in enumerate(weather_model.model.parameters()):\n",
    "    if idx>50:\n",
    "        param.requires_grad = True\n",
    "        \n",
    "\n",
    "for param in time_model.model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for idx,param in enumerate(time_model.model.parameters()):\n",
    "    if idx>50:\n",
    "        param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1b0472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "414e3138",
   "metadata": {},
   "outputs": [],
   "source": [
    "Alb = A.Compose([\n",
    "        A.Resize(width=CFG['IMG_SIZE'], height=CFG['IMG_SIZE']),\n",
    "#         A.HorizontalFlip(p=0.5),\n",
    "#         A.OneOf([\n",
    "#             A.Blur(blur_limit=3,p=0.3),\n",
    "#             A.GaussNoise(p=0.3,var_limit=(0, 26)),\n",
    "#             A.Downscale(p=0.3,scale_min=0.7, scale_max=0.99, interpolation=2),\n",
    "# #             A.RandomBrightness(p=0.2, limit=0.05),    \n",
    "#             A.CoarseDropout(p=0.2, max_holes=10, max_height=8, max_width=8, min_holes=5, min_height=2, min_width=2),\n",
    "#         ], p=0.7),\n",
    "#         A.OneOf([\n",
    "#         A.ElasticTransform(p=0.3),\n",
    "#         A.SafeRotate(limit=45,p=0.3),\n",
    "#         ],p=0.7),\n",
    "        \n",
    "        A.Normalize(mean=tuple(image_processor_config.image_mean)\n",
    "                   ,std=tuple(image_processor_config.image_std),p=1)\n",
    "    ], p=1)\n",
    "\n",
    "\n",
    "def aug_video(vid, tfms):\n",
    "    aug_vid = []\n",
    "    for x in vid:\n",
    "        aug_vid.append((tfms(image = np.asarray(x)))['image'])\n",
    "    return torch.from_numpy(np.stack(aug_vid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f3ffee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_path_list, label_list,transform=None):\n",
    "        self.video_path_list = video_path_list\n",
    "        self.label_list = label_list\n",
    "        self.Alb = transform\n",
    "    \n",
    "    def get_labels(self):   \n",
    "        return self.label_list  \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        frames = self.get_video(self.video_path_list[index])\n",
    "        if self.label_list is not None:\n",
    "            label = self.label_list[index]\n",
    "            return frames, label\n",
    "        else:\n",
    "            return frames\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.video_path_list)\n",
    "    \n",
    "    def get_video(self, path):\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        for idx in range(50):\n",
    "            if idx%5 == 3:\n",
    "                _, img = cap.read()\n",
    "#                 img = cv2.resize(img, (CFG['IMG_SIZE'], CFG['IMG_SIZE']))\n",
    "                frames.append(img)\n",
    "        if self.Alb is not None:\n",
    "            frames = aug_video(frames, tfms=self.Alb)\n",
    "        return torch.FloatTensor(np.array(frames)).permute(0, 3, 1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05370681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(skf_idx, model, optimizer, train_loader, val_loader, scheduler, device, cls_type):\n",
    "    model.to(device)\n",
    "    criterion = FocalLoss('multiclass')\n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    achieve = False\n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "      \n",
    "        for videos, labels in tqdm(iter(train_loader)):\n",
    "            videos = videos.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(videos)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "#             optimizer.step()\n",
    "            optimizer.first_step(zero_grad=True)\n",
    "\n",
    "            criterion(model(videos), labels).backward()\n",
    "            optimizer.second_step(zero_grad=True)\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "       \n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val F1 : [{_val_score:.5f}]')\n",
    "        if _val_score > CFG['VAL_SCORE_THRES']:\n",
    "            achieve=True\n",
    "            print(\"archieve score!!\")\n",
    "#             break\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(_val_score)\n",
    "            \n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "            date=datetime.today().strftime(\"%m_%d_%H_%M\")\n",
    "            torch.save(best_model.state_dict(), './'+cls_type + '_' + str(skf_idx) +'_'+ date + '_best_model.pth')\n",
    "        skf_idx+=1\n",
    "    return best_model,achieve,skf_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51fa21fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    preds, trues = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for videos, labels in tqdm(iter(val_loader)):\n",
    "            videos = videos.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            output = model(videos)\n",
    "            \n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "            \n",
    "            preds += output.argmax(1).detach().cpu().numpy().tolist()\n",
    "            trues += labels.detach().cpu().numpy().tolist()\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "    \n",
    "    _val_score = f1_score(trues, preds, average='macro')\n",
    "    return _val_loss, _val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3663b709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_pl = []\n",
    "\n",
    "# crash = {\n",
    "#     'model' : crash_model,\n",
    "#     'cls_type' : 'crash',\n",
    "#     'df' : crash_df\n",
    "# }\n",
    "\n",
    "# ego = {\n",
    "#     'model' : ego_model,\n",
    "#     'cls_type' : 'ego',\n",
    "#     'df' : ego_df\n",
    "# }\n",
    "\n",
    "# weather = {\n",
    "#     'model' : weather_model,\n",
    "#     'cls_type' : 'weather',\n",
    "#     'df' : weather_df\n",
    "# }\n",
    "\n",
    "# time = {\n",
    "#     'model' : time_model,\n",
    "#     'cls_type' : 'time',\n",
    "#     'df' : time_df\n",
    "# }\n",
    "\n",
    "# dict_pl.append(crash)\n",
    "# dict_pl.append(ego)\n",
    "# dict_pl.append(weather)\n",
    "# dict_pl.append(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b731a4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# # if CFG['LOAD_WEIGHT'] == True:\n",
    "# # checkpoint = torch.load('./checkpoint/crash/crash_19_02_24_04_15_best_model.pth')\n",
    "# # crash_model.load_state_dict(checkpoint)\n",
    "\n",
    "# print('crash_model')\n",
    "# apply_df = crash_df.copy()\n",
    "# cls_type = 'crash'\n",
    "# # base_optimizer = torch.optim.SGD  # define an optimizer for the \"sharpness-aware\" update\n",
    "# # optimizer = SAM(crash_model.parameters(), base_optimizer, lr=CFG[\"LEARNING_RATE\"], momentum=0.5)\n",
    "# optimizer = torch.optim.Adam(params = crash_model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3,threshold_mode='abs',min_lr=1e-12, verbose=True)\n",
    "# skf_idx=1\n",
    "# for train_idx,val_idx in skf.split(apply_df['video_path'],apply_df['label']):\n",
    "#     train_dataset = VideoDataset(apply_df['video_path'][train_idx].values, apply_df['label'][train_idx].values,transform=Alb)\n",
    "#     val_dataset = VideoDataset(apply_df['video_path'][val_idx].values, apply_df['label'][val_idx].values, transform=Alb)\n",
    "#     train_loader = DataLoader(train_dataset,sampler=ImbalancedDatasetSampler(train_dataset),shuffle=False,batch_size = CFG['BATCH_SIZE'],  num_workers=4)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], num_workers=4)\n",
    "#     crash_model,achieve,skf_idx= train(skf_idx,crash_model, optimizer, train_loader, val_loader, scheduler, device, cls_type)\n",
    "# #         skf_idx+=1\n",
    "# #     if achieve == True:\n",
    "# #         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29de8360",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ego model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 366/366 [02:21<00:00,  2.58it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:07<00:00, 12.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.36481] Val Loss : [0.35412] Val F1 : [0.31716]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 366/366 [02:22<00:00,  2.58it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:07<00:00, 12.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.35155] Val Loss : [0.34833] Val F1 : [0.31203]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 366/366 [02:21<00:00,  2.58it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:07<00:00, 12.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.34789] Val Loss : [0.34755] Val F1 : [0.35227]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 366/366 [02:21<00:00,  2.58it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:07<00:00, 12.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [0.34978] Val Loss : [0.34677] Val F1 : [0.42307]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 366/366 [02:21<00:00,  2.58it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:07<00:00, 12.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.34830] Val Loss : [0.34943] Val F1 : [0.36119]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 366/366 [02:22<00:00,  2.58it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:07<00:00, 12.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.34894] Val Loss : [0.34743] Val F1 : [0.44325]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 366/366 [02:21<00:00,  2.58it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:07<00:00, 12.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.34873] Val Loss : [0.34892] Val F1 : [0.36869]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 366/366 [02:22<00:00,  2.57it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:07<00:00, 12.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [0.34919] Val Loss : [0.34817] Val F1 : [0.40213]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 366/366 [02:22<00:00,  2.57it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:07<00:00, 12.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.34731] Val Loss : [0.34814] Val F1 : [0.35227]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 366/366 [02:22<00:00,  2.57it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:07<00:00, 12.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.34868] Val Loss : [0.34954] Val F1 : [0.32845]\n",
      "Epoch    10: reducing learning rate of group 0 to 5.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 366/366 [02:22<00:00,  2.58it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:07<00:00, 12.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.34828] Val Loss : [0.34911] Val F1 : [0.32579]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 366/366 [02:22<00:00,  2.57it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:07<00:00, 12.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [0.35149] Val Loss : [0.34888] Val F1 : [0.33678]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 366/366 [02:22<00:00,  2.58it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:07<00:00, 12.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.34851] Val Loss : [0.34836] Val F1 : [0.33400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 366/366 [02:22<00:00,  2.58it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:07<00:00, 12.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.34711] Val Loss : [0.34806] Val F1 : [0.33892]\n",
      "Epoch    14: reducing learning rate of group 0 to 2.5000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 366/366 [02:22<00:00,  2.57it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:07<00:00, 12.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.34887] Val Loss : [0.34825] Val F1 : [0.32839]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 366/366 [02:21<00:00,  2.58it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:07<00:00, 12.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [0.34876] Val Loss : [0.34833] Val F1 : [0.33400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 366/366 [02:22<00:00,  2.58it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:07<00:00, 12.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.34562] Val Loss : [0.34815] Val F1 : [0.37614]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 366/366 [02:22<00:00,  2.58it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:07<00:00, 12.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.35103] Val Loss : [0.34726] Val F1 : [0.38503]\n",
      "Epoch    18: reducing learning rate of group 0 to 1.2500e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 366/366 [02:22<00:00,  2.57it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:07<00:00, 12.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.34932] Val Loss : [0.34729] Val F1 : [0.37573]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 366/366 [02:22<00:00,  2.57it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:07<00:00, 12.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [0.34966] Val Loss : [0.34729] Val F1 : [0.37573]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# checkpoint = torch.load('./checkpoint/ego/ego_18_02_24_13_18_best_model.pth')\n",
    "\n",
    "\n",
    "# ego_model.load_state_dict(checkpoint)\n",
    "\n",
    "# from sam import SAM\n",
    "# !pip install sam\n",
    "base_optimizer = torch.optim.SGD  # define an optimizer for the \"sharpness-aware\" update\n",
    "optimizer = SAM(ego_model.parameters(), base_optimizer, lr=CFG[\"LEARNING_RATE\"], momentum=0.5)\n",
    "print('ego model')\n",
    "apply_df = ego_df.copy()\n",
    "cls_type = 'ego'\n",
    "# optimizer = torch.optim.Adadelta(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "# optimizer = torch.optim.Adam(params = ego_model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda1, lambda2], verbose=True)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3,threshold_mode='abs',min_lr=1e-12, verbose=True)\n",
    "skf_idx=0\n",
    "for train_idx,val_idx in skf.split(apply_df['video_path'],apply_df['label']):\n",
    "    train_dataset = VideoDataset(apply_df['video_path'][train_idx].values, apply_df['label'][train_idx].values,transform=Alb)\n",
    "    val_dataset = VideoDataset(apply_df['video_path'][val_idx].values, apply_df['label'][val_idx].values, transform=Alb)\n",
    "    train_loader = DataLoader(train_dataset,sampler=ImbalancedDatasetSampler(train_dataset),shuffle=False,batch_size = CFG['BATCH_SIZE'],  num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], num_workers=4)\n",
    "    _,achieve,skf_idx = train(skf_idx,ego_model, optimizer, train_loader, val_loader, scheduler, device, cls_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b971f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load('./checkpoint/weather_0_02_22_00_49_best_model.pth')\n",
    "\n",
    "\n",
    "# weather_model.load_state_dict(checkpoint)\n",
    "# print('weather model')\n",
    "# apply_df = weather_df.copy()\n",
    "# cls_type = 'weather'\n",
    "\n",
    "# base_optimizer = torch.optim.SGD  # define an optimizer for the \"sharpness-aware\" update\n",
    "# optimizer = SAM(weather_model.parameters(), base_optimizer, lr=CFG[\"LEARNING_RATE\"], momentum=0.5)\n",
    "# # optimizer = torch.optim.Adam(params = weather_model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3,threshold_mode='abs',min_lr=1e-12, verbose=True)\n",
    "# skf_idx=0\n",
    "# for train_idx,val_idx in skf.split(apply_df['video_path'],apply_df['label']):\n",
    "#     train_dataset = VideoDataset(apply_df['video_path'][train_idx].values, apply_df['label'][train_idx].values,transform=Alb)\n",
    "#     val_dataset = VideoDataset(apply_df['video_path'][val_idx].values, apply_df['label'][val_idx].values, transform=Alb)\n",
    "#     train_loader = DataLoader(train_dataset,sampler=ImbalancedDatasetSampler(train_dataset),shuffle=False,batch_size = CFG['BATCH_SIZE'],  num_workers=4)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], num_workers=4)\n",
    "#     weather_model,achieve,skf_idx = train(skf_idx,weather_model, optimizer, train_loader, val_loader, scheduler, device, cls_type)\n",
    "# #         skf_idx+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c07f60b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checkpoint = torch.load('./checkpoint/time_0_02_22_01_01_best_model.pth')\n",
    "\n",
    "\n",
    "# time_model.load_state_dict(checkpoint)\n",
    "# print('time model')\n",
    "# apply_df = time_df.copy()\n",
    "# cls_type = 'time'\n",
    "\n",
    "# base_optimizer = torch.optim.SGD  # define an optimizer for the \"sharpness-aware\" update\n",
    "# optimizer = SAM(time_model.parameters(), base_optimizer, lr=CFG[\"LEARNING_RATE\"], momentum=0.5)\n",
    "\n",
    "# # optimizer = torch.optim.Adam(params = time_model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3,threshold_mode='abs',min_lr=1e-12, verbose=True)\n",
    "# skf_idx=0\n",
    "# for train_idx,val_idx in skf.split(apply_df['video_path'],apply_df['label']):\n",
    "#     train_dataset = VideoDataset(apply_df['video_path'][train_idx].values, apply_df['label'][train_idx].values,transform=Alb)\n",
    "#     val_dataset = VideoDataset(apply_df['video_path'][val_idx].values, apply_df['label'][val_idx].values, transform=Alb)\n",
    "#     train_loader = DataLoader(train_dataset,sampler=ImbalancedDatasetSampler(train_dataset),shuffle=False,batch_size = CFG['BATCH_SIZE'],  num_workers=4)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], num_workers=4)\n",
    "#     time_model,achieve,skf_idx = train(skf_idx,time_model, optimizer, train_loader, val_loader, scheduler, device, cls_type)\n",
    "# #         skf_idx+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd895bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = VideoDataset(test_df['video_path'].values,label_list= None, transform=Alb)\n",
    "# test_loader = DataLoader(test_dataset, shuffle=False,batch_size = CFG['BATCH_SIZE']*2,  num_workers=4)\n",
    "   \n",
    "train_test_dataset = VideoDataset(all_df['video_path'].values,label_list= None, transform=Alb)\n",
    "train_test_loader = DataLoader(train_test_dataset, shuffle=False,batch_size = CFG['BATCH_SIZE'],  num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d95f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for videos in tqdm(iter(test_loader)):\n",
    "            videos = videos.to(device)\n",
    "            output = model(videos)\n",
    "#             preds += output.logits.argmax(1).detach().cpu().numpy().tolist()\n",
    "            preds += output.logits.detach().cpu().numpy().tolist()\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dcde3e00",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './checkpoint/crash/crash_13_02_22_15_11_best_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m crash_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./checkpoint/crash/crash_13_02_22_15_11_best_model.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m crash_model\u001b[38;5;241m.\u001b[39mload_state_dict(crash_checkpoint)\n\u001b[1;32m      4\u001b[0m ego_checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./checkpoint/ego/ego_19_02_22_16_23_best_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/competition/lib/python3.8/site-packages/torch/serialization.py:594\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    592\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 594\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    596\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    598\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    599\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/envs/competition/lib/python3.8/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/envs/competition/lib/python3.8/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './checkpoint/crash/crash_13_02_22_15_11_best_model.pth'"
     ]
    }
   ],
   "source": [
    "crash_checkpoint = torch.load('./checkpoint/crash/crash_13_02_22_15_11_best_model.pth')\n",
    "crash_model.load_state_dict(crash_checkpoint)\n",
    "\n",
    "ego_checkpoint = torch.load('./checkpoint/ego/ego_19_02_22_16_23_best_model.pth')\n",
    "ego_model.load_state_dict(ego_checkpoint)\n",
    "\n",
    "weather_checkpoint = torch.load('./checkpoint/weather/weather_18_02_22_17_12_best_model.pth')\n",
    "weather_model.load_state_dict(weather_checkpoint)\n",
    "\n",
    "time_checkpoint = torch.load('./checkpoint/time/time_0_02_22_21_23_best_model.pth')\n",
    "time_model.load_state_dict(time_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7001f7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noncrash_dataset = VideoDataset(noncrash_df['video_path'].values,label_list= None, transform=Alb)\n",
    "# noncrash_loader = DataLoader(noncrash_dataset, shuffle=False,batch_size = CFG['BATCH_SIZE'],  num_workers=4)\n",
    "   \n",
    "# g_time_preds = inference(time_model, noncrash_loader, device)\n",
    "# g_ego_preds = inference(ego_model, noncrash_loader, device)\n",
    "# g_weahter_preds = inference(weather_model, noncrash_loader, device)\n",
    "\n",
    "# idx = weather_g_df[weather_g_df['label']==0].index\n",
    "# weather_g_df.drop(idx,inplace=True)\n",
    "# print(len(weather_g_df[weather_g_df['label']==2]))\n",
    "# weather_g_df.to_csv('./weather_g_df.csv', index=False)\n",
    "\n",
    "# time_g_df = generate_label(g_time_preds,noncrash_df)\n",
    "# time_g_df.to_csv('./time_g_df.csv', index=False)\n",
    "\n",
    "# ego_g_df = generate_label(ego_preds,noncrash_df)\n",
    "# ego_g_df.to_csv('./ego_g_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fb5949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_label(pred,ref_df):\n",
    "    generate_data = pd.DataFrame(pred,columns = ['one','two'])\n",
    "    generate_data_index = generate_data[abs(generate_data['one']-generate_data['two'])>CFG['DATA_GENERATE_THRES']].index.values.tolist()\n",
    "    generate_data_list = generate_data[abs(generate_data['one']-generate_data['two'])>CFG['DATA_GENERATE_THRES']].values.tolist()\n",
    "    label = np.array(generate_data_list).argmax(1)\n",
    "    path=ref_df['video_path'][generate_data_index].values.tolist()\n",
    "    g_df = pd.DataFrame(columns = ['video_path','label'])\n",
    "    g_df['video_path'] = path\n",
    "    g_df['label'] = label\n",
    "    return g_df\n",
    "# df.max(axis = 1, numeric_only = True)\n",
    "def generate_weather_label(pred,ref_df):\n",
    "    generate_data = pd.DataFrame(pred,columns = ['one','two','three'])\n",
    "    generate_data_index = generate_data[generate_data.max(axis=1,numeric_only=True)>2].index.values.tolist()\n",
    "    generate_data_list = generate_data[generate_data.max(axis=1,numeric_only=True)>2].values.tolist()\n",
    "\n",
    "    label = np.array(generate_data_list).argmax(1)\n",
    "    path=ref_df['video_path'][generate_data_index].values.tolist()\n",
    "    g_df = pd.DataFrame(columns = ['video_path','label'])\n",
    "    g_df['video_path'] = path\n",
    "    g_df['label'] = label\n",
    "    return g_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f55d35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_preds_list = []\n",
    "crash_preds_list=[]\n",
    "ego_preds_list=[]\n",
    "weather_preds_list=[]\n",
    "time_preds_list=[]\n",
    "crash_preds=None\n",
    "ego_preds=None\n",
    "weather_preds=None\n",
    "time_preds=None\n",
    "\n",
    "\n",
    "for idx in range(CFG['NUM_ASB']):\n",
    "    crash_preds = inference(crash_model, test_loader, device)\n",
    "    crash_preds_list.append(crash_preds)\n",
    "    \n",
    "crash_pred_sum = np.sum(crash_preds_list,axis=0)\n",
    "crash_max = crash_pred_sum.argmax(1).tolist()\n",
    "\n",
    "for idx in range(CFG['NUM_ASB']):\n",
    "    ego_preds = inference(ego_model, test_loader, device)\n",
    "    ego_preds_list.append(ego_preds)\n",
    "    \n",
    "ego_pred_sum = np.sum(ego_preds_list,axis=0)\n",
    "ego_max = ego_pred_sum.argmax(1).tolist()\n",
    "    \n",
    "for idx in range(CFG['NUM_ASB']):\n",
    "    weather_preds = inference(weather_model, test_loader, device)\n",
    "    weather_preds_list.append(weather_preds)\n",
    "\n",
    "weather_pred_sum = np.sum(weather_preds_list,axis=0)\n",
    "weather_max = weather_pred_sum.argmax(1).tolist()\n",
    "    \n",
    "for idx in range(CFG['NUM_ASB']):\n",
    "    time_preds = inference(time_model, test_loader, device)\n",
    "    time_preds_list.append(time_preds)\n",
    "\n",
    "time_pred_sum = np.sum(time_preds_list,axis=0)\n",
    "time_max = time_pred_sum.argmax(1).tolist()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bdd37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crash_preds_df = pd.DataFrame(crash_preds,columns=['crash'])\n",
    "# ego_preds_df = pd.DataFrame(ego_preds,columns=['ego'])\n",
    "# weather_preds_df = pd.DataFrame(weather_preds,columns=['weather'])\n",
    "# time_preds_df = pd.DataFrame(time_preds,columns=['time'])\n",
    "\n",
    "preds=[]\n",
    "\n",
    "print(len(crash_preds))\n",
    "print(len(ego_preds))\n",
    "print(len(weather_preds))\n",
    "print(len(time_preds))\n",
    "\n",
    "for idx,crash in enumerate(crash_max):\n",
    "    ego = ego_max[idx]\n",
    "    weather = weather_max[idx]\n",
    "    time = time_max[idx]\n",
    "    if crash == 0:\n",
    "        preds.append(0)\n",
    "    else:\n",
    "        if ego==0:\n",
    "            if weather==0:\n",
    "                if time == 0:\n",
    "                    preds.append(7)\n",
    "                else:\n",
    "                    preds.append(8)\n",
    "            elif weather==1:\n",
    "                if time == 0:\n",
    "                    preds.append(9)\n",
    "                else:\n",
    "                    preds.append(10)\n",
    "            else:\n",
    "                if time == 0:\n",
    "                    preds.append(11)\n",
    "                else:\n",
    "                    preds.append(12)\n",
    "                    \n",
    "        else:\n",
    "            if weather==0:\n",
    "                if time == 0:\n",
    "                    preds.append(1)\n",
    "                else:\n",
    "                    preds.append(2)\n",
    "            elif weather==1:\n",
    "                if time == 0:\n",
    "                    preds.append(3)\n",
    "                else:\n",
    "                    preds.append(4)\n",
    "            else:\n",
    "                if time == 0:\n",
    "                    preds.append(5)\n",
    "                else:\n",
    "                    preds.append(6)\n",
    "                140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39d5d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # all_df['label'].values.tolist()\n",
    "# gt = crash_df['label'].values.tolist()\n",
    "# f1_score(gt,crash_max,average='macro')\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bf40f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# index = (np.array(preds) != np.array(gt))\n",
    "# len(index==True)\n",
    "eff = np.where(np.array(preds) != np.array(gt))\n",
    "eff[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b85f91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gt[1758])\n",
    "print(preds[1758])\n",
    "#weather.., crash.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb0231",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# path = all_df['video_path'][649]\n",
    "path = all_df['video_path'][343]\n",
    "frames=[]\n",
    "cap = cv2.VideoCapture(path)\n",
    "for idx in range(50):\n",
    "    if idx%5 == 3:\n",
    "        _, img = cap.read()\n",
    "        frames.append(img)\n",
    "visualize(frames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac56625",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = all_df['label'].values.tolist()\n",
    "f1_score(gt, preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610ac0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./data/sample_submission.csv')\n",
    "# comp = pd.read_csv('./submit/2023_02_21_10_44_04.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9741e2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit['label'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c079b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comp['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beda436d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submit['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2374bb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "date=datetime.today().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "submit.to_csv('./'+date+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de756792",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
