{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1db13f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchsampler\n",
    "#https://github.com/ufoym/imbalanced-dataset-sampler\n",
    "\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     train_dataset,\n",
    "#     sampler=ImbalancedDatasetSampler(train_dataset),\n",
    "#     batch_size=args.batch_size,\n",
    "#     **kwargs\n",
    "# )\n",
    "#https://www.kaggle.com/competitions/dfl-bundesliga-data-shootout/discussion/360236\n",
    "#model kinetic version\n",
    "#label seperate\n",
    "#video augmentation\n",
    "#imbalance data\n",
    "#focal loss\n",
    "#https://github.com/HHTseng/video-classification\n",
    "#https://huggingface.co/models?other=video-classification\n",
    "#앙상블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfb75f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed9ba04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torchsampler import ImbalancedDatasetSampler\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "import albumentations as A\n",
    "\n",
    "# from einops import rearrange\n",
    "# from decord import VideoReader\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from segmentation_models_pytorch.losses import FocalLoss\n",
    "# from transformers import AutoModel, AutoImageProcessor, AutoConfig\n",
    "# from skmultilearn.model_selection import iterative_train_test_split\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorchvideo.transforms.transforms_factory import create_video_transform\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoModel, AutoImageProcessor, AutoConfig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a95a426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f09879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "all_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0de55a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>video_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./train/TRAIN_0000.mp4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./train/TRAIN_0001.mp4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>./train/TRAIN_0002.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>./train/TRAIN_0003.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./train/TRAIN_0004.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>TRAIN_2693</td>\n",
       "      <td>./train/TRAIN_2693.mp4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694</th>\n",
       "      <td>TRAIN_2694</td>\n",
       "      <td>./train/TRAIN_2694.mp4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>TRAIN_2695</td>\n",
       "      <td>./train/TRAIN_2695.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>TRAIN_2696</td>\n",
       "      <td>./train/TRAIN_2696.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>TRAIN_2697</td>\n",
       "      <td>./train/TRAIN_2697.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2698 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sample_id              video_path  label\n",
       "0     TRAIN_0000  ./train/TRAIN_0000.mp4      7\n",
       "1     TRAIN_0001  ./train/TRAIN_0001.mp4      7\n",
       "2     TRAIN_0002  ./train/TRAIN_0002.mp4      0\n",
       "3     TRAIN_0003  ./train/TRAIN_0003.mp4      0\n",
       "4     TRAIN_0004  ./train/TRAIN_0004.mp4      1\n",
       "...          ...                     ...    ...\n",
       "2693  TRAIN_2693  ./train/TRAIN_2693.mp4      3\n",
       "2694  TRAIN_2694  ./train/TRAIN_2694.mp4      5\n",
       "2695  TRAIN_2695  ./train/TRAIN_2695.mp4      0\n",
       "2696  TRAIN_2696  ./train/TRAIN_2696.mp4      0\n",
       "2697  TRAIN_2697  ./train/TRAIN_2697.mp4      0\n",
       "\n",
       "[2698 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ef3570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CFG = {\n",
    "    'VIDEO_LENGTH':40, # 10프레임 * 5초\n",
    "    'IMG_SIZE':115,\n",
    "    'EPOCHS':30,\n",
    "    'LEARNING_RATE':3e-5,\n",
    "    'BATCH_SIZE':1,\n",
    "    'SEED':2023,\n",
    "    'SPLIT':5,\n",
    "    'ROOT':'./data',\n",
    "    'MODEL':\"facebook/timesformer-base-finetuned-k400\"\n",
    "    \n",
    "#     'MCG-NJU/videomae-base-finetuned-ssv2'\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b42aa213",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits = CFG['SPLIT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12bdff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee5d0e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['video_path'] = all_df['video_path'].apply(lambda x:CFG['ROOT']+x[1:])\n",
    "test_df['video_path'] = test_df['video_path'].apply(lambda x:CFG['ROOT']+x[1:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65e1c289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>video_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./data/train/TRAIN_0000.mp4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./data/train/TRAIN_0001.mp4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>./data/train/TRAIN_0002.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>./data/train/TRAIN_0003.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./data/train/TRAIN_0004.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>TRAIN_2693</td>\n",
       "      <td>./data/train/TRAIN_2693.mp4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694</th>\n",
       "      <td>TRAIN_2694</td>\n",
       "      <td>./data/train/TRAIN_2694.mp4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>TRAIN_2695</td>\n",
       "      <td>./data/train/TRAIN_2695.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>TRAIN_2696</td>\n",
       "      <td>./data/train/TRAIN_2696.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>TRAIN_2697</td>\n",
       "      <td>./data/train/TRAIN_2697.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2698 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sample_id                   video_path  label\n",
       "0     TRAIN_0000  ./data/train/TRAIN_0000.mp4      7\n",
       "1     TRAIN_0001  ./data/train/TRAIN_0001.mp4      7\n",
       "2     TRAIN_0002  ./data/train/TRAIN_0002.mp4      0\n",
       "3     TRAIN_0003  ./data/train/TRAIN_0003.mp4      0\n",
       "4     TRAIN_0004  ./data/train/TRAIN_0004.mp4      1\n",
       "...          ...                          ...    ...\n",
       "2693  TRAIN_2693  ./data/train/TRAIN_2693.mp4      3\n",
       "2694  TRAIN_2694  ./data/train/TRAIN_2694.mp4      5\n",
       "2695  TRAIN_2695  ./data/train/TRAIN_2695.mp4      0\n",
       "2696  TRAIN_2696  ./data/train/TRAIN_2696.mp4      0\n",
       "2697  TRAIN_2697  ./data/train/TRAIN_2697.mp4      0\n",
       "\n",
       "[2698 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b433f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_df = all_df.copy()\n",
    "crash_df['label'] = crash_df['label'].apply(lambda x: 1 if x != 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "660a98ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>video_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./data/train/TRAIN_0000.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./data/train/TRAIN_0001.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>./data/train/TRAIN_0002.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>./data/train/TRAIN_0003.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./data/train/TRAIN_0004.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>TRAIN_2693</td>\n",
       "      <td>./data/train/TRAIN_2693.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694</th>\n",
       "      <td>TRAIN_2694</td>\n",
       "      <td>./data/train/TRAIN_2694.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>TRAIN_2695</td>\n",
       "      <td>./data/train/TRAIN_2695.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>TRAIN_2696</td>\n",
       "      <td>./data/train/TRAIN_2696.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>TRAIN_2697</td>\n",
       "      <td>./data/train/TRAIN_2697.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2698 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sample_id                   video_path  label\n",
       "0     TRAIN_0000  ./data/train/TRAIN_0000.mp4      1\n",
       "1     TRAIN_0001  ./data/train/TRAIN_0001.mp4      1\n",
       "2     TRAIN_0002  ./data/train/TRAIN_0002.mp4      0\n",
       "3     TRAIN_0003  ./data/train/TRAIN_0003.mp4      0\n",
       "4     TRAIN_0004  ./data/train/TRAIN_0004.mp4      1\n",
       "...          ...                          ...    ...\n",
       "2693  TRAIN_2693  ./data/train/TRAIN_2693.mp4      1\n",
       "2694  TRAIN_2694  ./data/train/TRAIN_2694.mp4      1\n",
       "2695  TRAIN_2695  ./data/train/TRAIN_2695.mp4      0\n",
       "2696  TRAIN_2696  ./data/train/TRAIN_2696.mp4      0\n",
       "2697  TRAIN_2697  ./data/train/TRAIN_2697.mp4      0\n",
       "\n",
       "[2698 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crash_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e2dd568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train_idx,test_idx in skf.split(crash_df['video_path'],crash_df['label']):\n",
    "#     print(train_idx)\n",
    "#     print(\"==\")\n",
    "#     print(test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f497d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# train_df, val_df = train_test_split(\n",
    "#     all_df, test_size=0.2, stratify=all_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fed98cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58170cb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/timesformer-base-finetuned-k400 were not used when initializing TimesformerModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing TimesformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TimesformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TimesformerModel were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
      "- timesformer.embeddings.position_embeddings: found shape torch.Size([1, 197, 768]) in the checkpoint and torch.Size([1, 50, 768]) in the model instantiated\n",
      "- timesformer.embeddings.time_embeddings: found shape torch.Size([1, 8, 768]) in the checkpoint and torch.Size([1, 40, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import VideoMAEConfig, VideoMAEModel\n",
    "from transformers import AutoImageProcessor, VideoMAEForVideoClassification\n",
    "\n",
    "image_processor_config = AutoImageProcessor.from_pretrained(CFG['MODEL'])\n",
    "# configuration = VideoMAEConfig()\n",
    "configuration = AutoConfig.from_pretrained(CFG['MODEL'])\n",
    "\n",
    "# configuration.image_size=CFG['IMG_SIZE']\n",
    "# configuration.num_frames = CFG['VIDEO_LENGTH']\n",
    "# image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "# AutoModel\n",
    "# model = VideoMAEModel.from_pretrained(CFG['MODEL'],config=configuration,ignore_mismatched_sizes=True)\n",
    "# crash_model = VideoMAEForVideoClassification.from_pretrained(CFG['MODEL'],config=configuration,ignore_mismatched_sizes=True)\n",
    "configuration.num_frames=CFG['VIDEO_LENGTH']\n",
    "configuration.image_size=CFG['IMG_SIZE']\n",
    "configuration.id2label = {0:'no crash',1:'crash'}\n",
    "configuration.label2id = {'no crash':0,'crash':1}\n",
    "crash_model = AutoModel.from_pretrained(CFG['MODEL'],config=configuration,ignore_mismatched_sizes=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3336aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimesformerConfig {\n",
       "  \"_name_or_path\": \"facebook/timesformer-base-finetuned-k400\",\n",
       "  \"architectures\": [\n",
       "    \"TimesformerForVideoClassification\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"attention_type\": \"divided_space_time\",\n",
       "  \"drop_path_rate\": 0,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"no crash\",\n",
       "    \"1\": \"crash\"\n",
       "  },\n",
       "  \"image_size\": 115,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"crash\": 1,\n",
       "    \"no crash\": 0\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-06,\n",
       "  \"model_type\": \"timesformer\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_frames\": 40,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"patch_size\": 16,\n",
       "  \"qkv_bias\": true,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.26.1\"\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crash_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e5c84fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_processor_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4df59751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #https://torchvideo.readthedocs.io/en/latest/transforms.html#examples\n",
    "# train_transform = create_video_transform(\n",
    "#     mode='train',\n",
    "#     num_samples=model.config.num_frames,\n",
    "#     video_mean = tuple(image_processor_config.image_mean),\n",
    "#     video_std = tuple(image_processor_config.image_std),\n",
    "#     crop_size = tuple(image_processor_config.crop_size.values())\n",
    "# )\n",
    "\n",
    "# val_transform = create_video_transform(\n",
    "#     mode='val',\n",
    "#     num_samples=model.config.num_frames,\n",
    "#     video_mean = tuple(image_processor_config.image_mean),\n",
    "#     video_std = tuple(image_processor_config.image_std),\n",
    "#     crop_size = tuple(image_processor_config.crop_size.values())\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "414e3138",
   "metadata": {},
   "outputs": [],
   "source": [
    "Alb = A.Compose([\n",
    "        A.Resize(width=CFG['IMG_SIZE'], height=CFG['IMG_SIZE']),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.ShiftScaleRotate(rotate_limit=15, p=0.3),\n",
    "#         A.ChannelDropout(p=0.1),\n",
    "#         A.RandomRain(p=0.1),\n",
    "#         A.GridDistortion(p=0.3),\n",
    "        A.Normalize()\n",
    "    ], p=1)\n",
    "\n",
    "\n",
    "def aug_video(vid, tfms):\n",
    "#     seed = random.randint(0,99999)\n",
    "    aug_vid = []\n",
    "    for x in vid:\n",
    "#         random.seed(seed)\n",
    "        aug_vid.append((tfms(image = np.asarray(x)))['image'])\n",
    "    return torch.from_numpy(np.stack(aug_vid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f3ffee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_path_list, label_list,transform=None):\n",
    "        self.video_path_list = video_path_list\n",
    "        self.label_list = label_list\n",
    "#         self.transform=transform\n",
    "        self.Alb = transform\n",
    "    \n",
    "    def get_labels(self):   \n",
    "        return self.label_list  \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        frames = self.get_video(self.video_path_list[index])\n",
    "        \n",
    "#         vr = VideoReader(self.video_path_list[index])\n",
    "#         video = torch.from_numpy(vr.get_batch(range(CFG['VIDEO_LENGTH'])).asnumpy())\n",
    "#         video = rearrange(video, 't h w c -> c t h w')\n",
    "        if self.label_list is not None:\n",
    "#             if self.transform:\n",
    "#                 frames = transform(frames)\n",
    "            label = self.label_list[index]\n",
    "            return frames, label\n",
    "        else:\n",
    "#             if self.transform:\n",
    "#                 frames = transform(frames)\n",
    "            return frames\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.video_path_list)\n",
    "    \n",
    "    def get_video(self, path):\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        for _ in range(5,45):\n",
    "            _, img = cap.read()\n",
    "            img = cv2.resize(img, (CFG['IMG_SIZE'], CFG['IMG_SIZE']))\n",
    "#             img = img / 255.\n",
    "            frames.append(img)\n",
    "        if self.Alb is not None:\n",
    "            frames = aug_video(frames, tfms=self.Alb)\n",
    "#         8, 224, 224, 3\n",
    "        return torch.FloatTensor(np.array(frames)).permute(0, 3, 1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d90e7112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a477b553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "class CrashClsModel(nn.Module):\n",
    "    def __init__(self,pretrained_model):\n",
    "        super().__init__()\n",
    "        self.model = pretrained_model\n",
    "        self.classifier = nn.LazyLinear(2)\n",
    "    def forward(self, x):\n",
    "#         print(x.size())\n",
    "        batch_size = x.size(0)\n",
    "        x = self.model(x).last_hidden_state.mean(dim=1)\n",
    "        x_out = self.classifier(x)\n",
    "#         x = x[0].view(batch_size,-1)\n",
    "#         x = self.classifier(x)\n",
    "#         print(x.size())\n",
    "#         print(x.size())\n",
    "#         print(x)\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05370681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    model.to(device)\n",
    "#     criterion = nn.CrossEntropyLoss().to(device)\n",
    "#     criterion = nn.BCELoss().to(device)\n",
    "    criterion = FocalLoss('multiclass')\n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for videos, labels in tqdm(iter(train_loader)):\n",
    "            videos = videos.to(device)\n",
    "            labels = labels.to(device)\n",
    "#             print(labels)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(videos)\n",
    "#             print(output.armax(-1).item())\n",
    "            \n",
    "#             output = F.softmax(output,dim=1)[:1]\n",
    "#             print(output)\n",
    "#             print(\"==\")\n",
    "#             print(labels)\n",
    "#             print(output)\n",
    "            loss = criterion(output, labels)\n",
    "           \n",
    "#             print(output.logits)\n",
    "#             print(labels)\n",
    "#             print(loss)\n",
    "#             loss =FocalLoss(gamma=0)(output,labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "#             print(loss.item())\n",
    "            train_loss.append(loss.item())\n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val F1 : [{_val_score:.5f}]')\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(_val_score)\n",
    "            \n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "            torch.save(best_model.state_dict(), './'+str(epoch)+'_best_model.pth')\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51fa21fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    preds, trues = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for videos, labels in tqdm(iter(val_loader)):\n",
    "            videos = videos.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            output = model(videos)\n",
    "            \n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "            \n",
    "            preds += output.argmax(1).detach().cpu().numpy().tolist()\n",
    "            trues += labels.detach().cpu().numpy().tolist()\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "    \n",
    "    _val_score = f1_score(trues, preds, average='macro')\n",
    "    return _val_loss, _val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c10c25a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train_idx,test_idx in skf.split(crash_df['video_path'],crash_df['label']):\n",
    "#     print(crash_df['video_path'][train_idx].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b731a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/server-003/anaconda3/envs/competition/lib/python3.8/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 2158/2158 [07:31<00:00,  4.78it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 540/540 [00:33<00:00, 16.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.11593] Val Loss : [0.42433] Val F1 : [0.26560]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 2158/2158 [07:31<00:00,  4.78it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 540/540 [00:33<00:00, 16.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.04214] Val Loss : [0.45003] Val F1 : [0.25939]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 2158/2158 [07:31<00:00,  4.78it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 540/540 [00:33<00:00, 16.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.03420] Val Loss : [0.37307] Val F1 : [0.40447]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 2158/2158 [07:32<00:00,  4.77it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 540/540 [00:33<00:00, 16.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [0.02478] Val Loss : [0.35302] Val F1 : [0.48769]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 2158/2158 [07:31<00:00,  4.78it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 540/540 [00:33<00:00, 16.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Train Loss : [0.02443] Val Loss : [0.41397] Val F1 : [0.43866]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 2158/2158 [07:32<00:00,  4.77it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 540/540 [00:33<00:00, 16.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6], Train Loss : [0.01966] Val Loss : [0.38489] Val F1 : [0.29585]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 2158/2158 [07:32<00:00,  4.77it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 540/540 [00:33<00:00, 16.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7], Train Loss : [0.01803] Val Loss : [0.54248] Val F1 : [0.25939]\n",
      "Epoch     7: reducing learning rate of group 0 to 1.5000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 25%|███████████████████████▏                                                                   | 550/2158 [01:55<05:37,  4.77it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset,shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,batch_size \u001b[38;5;241m=\u001b[39m CFG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBATCH_SIZE\u001b[39m\u001b[38;5;124m'\u001b[39m],  num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m     13\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size \u001b[38;5;241m=\u001b[39m CFG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBATCH_SIZE\u001b[39m\u001b[38;5;124m'\u001b[39m], num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m infer_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcrash_cls_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 32\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_loader, val_loader, scheduler, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m             loss \u001b[38;5;241m=\u001b[39m criterion(output, labels)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#             print(output.logits)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#             print(labels)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#             print(loss)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#             loss =FocalLoss(gamma=0)(output,labels)\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m             \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m             optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#             print(loss.item())\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/competition/lib/python3.8/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/competition/lib/python3.8/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# crash_train_df, val_df = train_test_split(\n",
    "#     crash_df, test_size=0.2, stratify=crash_df['label'])\n",
    "crash_cls_model = CrashClsModel(crash_model)\n",
    "optimizer = torch.optim.Adam(params = crash_cls_model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2,threshold_mode='abs',min_lr=1e-10, verbose=True)\n",
    "\n",
    "#  sampler=ImbalancedDatasetSampler(train_dataset),\n",
    "for train_idx,val_idx in skf.split(crash_df['video_path'],crash_df['label']):\n",
    "    train_dataset = VideoDataset(crash_df['video_path'][train_idx].values, crash_df['label'][train_idx].values,transform=Alb)\n",
    "    val_dataset = VideoDataset(crash_df['video_path'][val_idx].values, crash_df['label'][val_idx].values, transform=Alb)\n",
    "    train_loader = DataLoader(train_dataset,shuffle=False,batch_size = CFG['BATCH_SIZE'],  num_workers=12)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], num_workers=12)\n",
    "    infer_model = train(crash_cls_model, optimizer, train_loader, val_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773f1a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3e8cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_model = CrashClsModel(model).to(device)\n",
    "\n",
    "# optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2,threshold_mode='abs',min_lr=1e-10, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935c8047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer_model = train(video_model, optimizer, train_loader, val_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dce525b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d95f675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f55d35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bdd37e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
