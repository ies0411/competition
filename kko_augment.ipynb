{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "017c340e-df82-4ac2-93c7-30b6c702393f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install accelerate\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8422c08-d7b2-4141-978c-67d61f281288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
    "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast, AdamW, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset # 데이터를 모델에 사용할 수 있도록 정리해 주는 라이브러리\n",
    "\n",
    "# CUDA 사용 가능 여부 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "454ea39b-d055-40fd-8801-1a7ae6ce99a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df5d7480-9580-4233-ade0-8055854d943e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('./data/train.csv')\n",
    "data_df = data_df.drop(['id','category'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8628e701-61d6-4e17-b325-af188faa4688",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'LR' : 5e-6, # Learning Rate\n",
    "    'EPOCHS' : 10, # 학습 Epoch\n",
    "    'BATCH_SIZE' : 1,\n",
    "    # 'AUG_RATIO' : 0.15,\n",
    "    'AUG_PROB' : 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51c70a4a-56b9-4a30-bc9b-bc985f9f2943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f035c565484ea8a59cc3f98432858d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#   'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',  # or float32 version: revision=KoGPT6B-ryan1.5b\n",
    "#   eos_token='</s>'\n",
    "# )\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./kakao_6_9_epoch/\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./kakao_6_9_epoch/\", eos_token='</s>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "078d4623-e880-49cb-9c05-4bb9b60d2086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class HansolDataset(Dataset):\n",
    "    def __init__(self, data_df, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data_df = None\n",
    "        self.sentence_list = []\n",
    "        self._augment_question(data_df)\n",
    "        self._generate_data()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.sentence_list[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentence_list)\n",
    "\n",
    "    def _augment_question(self, data_df):\n",
    "        for _, row in data_df.iterrows():\n",
    "            new_row = {}\n",
    "            random_number = random.choice(range(0, len(data_df)))\n",
    "            new_row['질문_1'] = row['질문_1'] + ' ' + data_df['질문_1'][random_number]\n",
    "            new_row['질문_2'] = row['질문_2'] + ' ' + data_df['질문_2'][random_number]\n",
    "            new_row['답변_1'] = row['답변_1'] + ' ' + data_df['답변_1'][random_number]\n",
    "            new_row['답변_2'] = row['답변_2'] + ' ' + data_df['답변_2'][random_number]\n",
    "            new_row['답변_3'] = row['답변_3'] + ' ' + data_df['답변_3'][random_number]\n",
    "            new_row['답변_4'] = row['답변_4'] + ' ' + data_df['답변_4'][random_number]\n",
    "            new_row['답변_5'] = row['답변_5'] + ' ' + data_df['답변_5'][random_number]\n",
    "            df = pd.DataFrame(new_row,  index = [0])\n",
    "            data_df = pd.concat([data_df, df], ignore_index=True)\n",
    "        self.data_df = data_df.copy(deep=True)\n",
    "        \n",
    "    def _generate_data(self):\n",
    "        for _, row in self.data_df.iterrows():\n",
    "            for q_col in ['질문_1', '질문_2']:\n",
    "                for a_col in ['답변_1', '답변_2', '답변_3', '답변_4', '답변_5']:\n",
    "                    # 질문과 답변 쌍을 </s> token으로 연결\n",
    "                    input_text = row[q_col] + tokenizer.eos_token + row[a_col] + tokenizer.eos_token\n",
    "                    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "                    self.sentence_list.append(input_ids)\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "470f0e29-df0d-4939-85b7-464cbe72bb23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTJForCausalLM(\n",
       "  (transformer): GPTJModel(\n",
       "    (wte): Embedding(64512, 4096)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-27): 28 x GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=64512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afdb8f56-1a0d-4bc6-a536-9a6efb9fe1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model.named_children():\n",
    "    if name == \"transformer\":\n",
    "        for name, sub_module in module.named_children():\n",
    "            # print(name)\n",
    "            if name == \"wte\":\n",
    "                for param in sub_module.parameters():\n",
    "                    param.requires_grad = False\n",
    "            elif name == \"drop\":\n",
    "                for param in sub_module.parameters():\n",
    "                    param.requires_grad = False\n",
    "            elif name == \"h\":\n",
    "                # print(name)\n",
    "                 for name, layer_module in sub_module.named_children():\n",
    "                     if int(name) < 22:\n",
    "                        for param in layer_module.parameters():\n",
    "                            param.requires_grad = False\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7a4fc6-7b62-4bc4-b549-84894f929628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1 - Avg Loss: 0.2052:  40%|███▌     | 5175/12880 [32:51<51:29,  2.49it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 3 - Avg Loss: 0.1245:  43%|███▉     | 5591/12880 [35:40<44:28,  2.73it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 3 - Avg Loss: 0.1221:  56%|█████    | 7179/12880 [45:49<32:10,  2.95it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 5 - Avg Loss: 0.1069:  67%|██████   | 8662/12880 [54:52<22:56,  3.07it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 5 - Avg Loss: 0.1047:  80%|████▊ | 10347/12880 [1:05:37<19:30,  2.16it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 7 - Avg Loss: 0.1119:  19%|█▎     | 2404/12880 [15:13<1:03:43,  2.74it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "model.to(device) # 모델을 GPU단으로 이동\n",
    "\n",
    "# 모델 학습 설정\n",
    "optimizer = AdamW(model.parameters(), lr=CFG['LR'])\n",
    "model.train()\n",
    "\n",
    "# 모델 학습\n",
    "for epoch in range(CFG['EPOCHS']):\n",
    "    total_loss = 0\n",
    "    dataset = HansolDataset(data_df = data_df, tokenizer=tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True) # 미니 배치 형태로 데이터 갖추기\n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for batch_idx, batch in progress_bar:\n",
    "        # 데이터를 GPU단으로 이동\n",
    "        # batch.unsqueeze(0)\n",
    "        batch = batch[0]\n",
    "        batch = batch.to(device)\n",
    "        outputs = model(batch, labels=batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 진행률 표시줄에 평균 손실 업데이트\n",
    "        progress_bar.set_description(f\"Epoch {epoch+1} - Avg Loss: {total_loss / (batch_idx+1):.4f}\")\n",
    "\n",
    "    # 에폭의 평균 손실을 출력\n",
    "    print(f\"Epoch {epoch+1}/{CFG['EPOCHS']}, Average Loss: {total_loss / len(dataloader)}\")\n",
    "    model.save_pretrained(f\"./kko_aug2_{epoch}_epoch\")\n",
    "    tokenizer.save_pretrained(f\"./kko_aug2_{epoch}_epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f256a94-29be-46d3-9aa5-401b672910c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74a9d8b-b49f-4773-92ed-a286daaf5e33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1cff3f-d53f-4f79-bfdc-9c015be9d73a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
