{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ad712ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install easyocr\n",
    "#!git clone https://github.com/clovaai/deep-text-recognition-benchmark.git\n",
    "# !pip install natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ec85d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : augmentation, auto tune , or just use easyorc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59257f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./deep-text-recognition-benchmark/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7def816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model import Model\n",
    "from dataset import hierarchical_dataset, AlignCollate, Batch_Balanced_Dataset\n",
    "from utils import (\n",
    "    CTCLabelConverter,\n",
    "    CTCLabelConverterForBaiduWarpctc,\n",
    "    AttnLabelConverter,\n",
    "    Averager,\n",
    ")\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch\n",
    "import argparse\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "from sklearn.metrics import *\n",
    "\n",
    "# from test import validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a142245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision.models import resnet18, resnet34, resnet50, resnet101\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "414d1830",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    'cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f51d9be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_HEIGHT_SIZE': 64,\n",
    "    'IMG_WIDTH_SIZE': 256,\n",
    "    'EPOCHS': 50,\n",
    "    'LEARNING_RATE': 1e-3,\n",
    "    'BATCH_SIZE': 180,\n",
    "    'NUM_WORKERS': 12,  # 본인의 GPU, CPU 환경에 맞게 설정\n",
    "    'SEED': 41,\n",
    "    'INPUT_CHANNEL': 3,\n",
    "    'HIDDEN_N': 64\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c797eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "seed_everything(CFG['SEED'])  # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c024ebcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd2c3ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제공된 학습데이터 중 1글자 샘플들의 단어사전이 학습/테스트 데이터의 모든 글자를 담고 있으므로 학습 데이터로 우선 배치\n",
    "df['len'] = df['label'].str.len()\n",
    "train_v1 = df[df['len'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "015e8c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제공된 학습데이터 중 2글자 이상의 샘플들에 대해서 단어길이를 고려하여 Train (80%) / Validation (20%) 분할\n",
    "df = df[df['len'] > 1]\n",
    "train_v2, val, _, _ = train_test_split(\n",
    "    df, df['len'], test_size=0.2, random_state=CFG['SEED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8daae7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66251 10637\n"
     ]
    }
   ],
   "source": [
    "# 학습 데이터로 우선 배치한 1글자 샘플들과 분할된 2글자 이상의 학습 샘플을 concat하여 최종 학습 데이터로 사용\n",
    "train = pd.concat([train_v1, train_v2])\n",
    "print(len(train), len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ddb6e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2349\n"
     ]
    }
   ],
   "source": [
    "# 학습 데이터로부터 단어 사전(Vocabulary) 구축\n",
    "train_gt = [gt for gt in train['label']]\n",
    "train_gt = \"\".join(train_gt)\n",
    "letters = sorted(list(set(list(train_gt))))\n",
    "print(len(letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd39db44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gt = [gt for gt in train['label']]\n",
    "train_gt = \"\".join(train_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dfb680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = sorted(list(set(list(train_gt))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65ff2129",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = [\"-\"] + letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52ad7701",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx2char = {k: v for k, v in enumerate(vocabulary, start=0)}\n",
    "char2idx = {v: k for k, v in idx2char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64a23ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2350\n"
     ]
    }
   ],
   "source": [
    "vocabulary = [\"-\"] + letters\n",
    "print(len(vocabulary))\n",
    "idx2char = {k: v for k, v in enumerate(vocabulary, start=0)}\n",
    "char2idx = {v: k for k, v in idx2char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2d4acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_path_list, label_list, train_mode=True):\n",
    "        self.img_path_list = img_path_list\n",
    "        self.label_list = label_list\n",
    "        self.train_mode = train_mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_path_list[index]).convert('RGB')\n",
    "\n",
    "        if self.train_mode:\n",
    "            image = self.train_transform(image)\n",
    "        else:\n",
    "            image = self.test_transform(image)\n",
    "\n",
    "        if self.label_list is not None:\n",
    "            text = self.label_list[index]\n",
    "            return image, text\n",
    "        else:\n",
    "            return image\n",
    "\n",
    "    # Image Augmentation\n",
    "    def train_transform(self, image):\n",
    "        transform_ops = transforms.Compose([\n",
    "            transforms.Resize((CFG['IMG_HEIGHT_SIZE'], CFG['IMG_WIDTH_SIZE'])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                 std=(0.229, 0.224, 0.225))\n",
    "        ])\n",
    "        return transform_ops(image)\n",
    "\n",
    "    def test_transform(self, image):\n",
    "        transform_ops = transforms.Compose([\n",
    "            transforms.Resize((CFG['IMG_HEIGHT_SIZE'], CFG['IMG_WIDTH_SIZE'])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                 std=(0.229, 0.224, 0.225))\n",
    "        ])\n",
    "        return transform_ops(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1355496",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train['img_path'].values, train['label'].values)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True, num_workers=CFG['NUM_WORKERS'])\n",
    "\n",
    "val_dataset = CustomDataset(val['img_path'].values, val['label'].values)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True, num_workers=CFG['NUM_WORKERS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90da6d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_batch, text_batch = iter(train_loader).next()\n",
    "# print(image_batch.size(), text_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4d00106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "from modules.transformation import TPS_SpatialTransformerNetwork\n",
    "from modules.feature_extraction import VGG_FeatureExtractor, RCNN_FeatureExtractor, ResNet_FeatureExtractor\n",
    "from modules.sequence_modeling import BidirectionalLSTM\n",
    "from modules.prediction import Attention\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, num_class=len(char2idx)):\n",
    "        super(Model, self).__init__()\n",
    "        rnn_hidden_size = 256\n",
    "        self.Transformation = TPS_SpatialTransformerNetwork(\n",
    "            F=20, I_size=(CFG['IMG_HEIGHT_SIZE'], CFG['IMG_WIDTH_SIZE']), I_r_size=(CFG['IMG_HEIGHT_SIZE'], CFG['IMG_WIDTH_SIZE']), I_channel_num=CFG['INPUT_CHANNEL'])\n",
    "\n",
    "        \"\"\" FeatureExtraction \"\"\"\n",
    "\n",
    "        resnet = resnet34(pretrained=True)\n",
    "        # CNN Feature Extract\n",
    "        resnet_modules = list(resnet.children())[:-3]\n",
    "        self.feature_extract = nn.Sequential(\n",
    "            *resnet_modules,\n",
    "            nn.Conv2d(256, 256, kernel_size=(3, 6), stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.linear1 = nn.Linear(1024, rnn_hidden_size)\n",
    "\n",
    "#         self.FeatureExtraction = ResNet_FeatureExtractor(\n",
    "#             CFG['INPUT_CHANNEL'], CFG['HIDDEN_N'])\n",
    "#         self.FeatureExtraction_output = CFG['HIDDEN_N']\n",
    "#         self.AdaptiveAvgPool = nn.AdaptiveAvgPool2d(\n",
    "#             (None, 1))  # Transform final (imgH/16-1) -> 1\n",
    "\n",
    "        self.rnn = nn.RNN(input_size=rnn_hidden_size,\n",
    "                          hidden_size=rnn_hidden_size,\n",
    "                          bidirectional=True,\n",
    "                          batch_first=True)\n",
    "        self.linear2 = nn.Linear(rnn_hidden_size, num_class)\n",
    "\n",
    "        \"\"\" Sequence modeling\"\"\"\n",
    "        self.SequenceModeling = nn.Sequential(\n",
    "            #             BidirectionalLSTM(self.FeatureExtraction_output,\n",
    "            #                               rnn_hidden_size, rnn_hidden_size),\n",
    "            BidirectionalLSTM(rnn_hidden_size, rnn_hidden_size, rnn_hidden_size))\n",
    "        self.SequenceModeling_output = CFG['HIDDEN_N']\n",
    "\n",
    "        \"\"\" Prediction \"\"\"\n",
    "        self.Prediction = nn.Linear(self.SequenceModeling_output, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Transformation stage \"\"\"\n",
    "        input = self.Transformation(x)\n",
    "\n",
    "        \"\"\" Feature extraction stage \"\"\"\n",
    "        visual_feature = self.feature_extract(input)\n",
    "        visual_feature = visual_feature.permute(0, 3, 1, 2)\n",
    "\n",
    "        ###\n",
    "        batch_size = visual_feature.size(0)\n",
    "        T = visual_feature.size(1)\n",
    "        # [batch_size, T==width, num_features==channels*height]\n",
    "        visual_feature = visual_feature.view(batch_size, T, -1)\n",
    "        visual_feature = self.linear1(visual_feature)\n",
    "\n",
    "\n",
    "#         visual_feature = self.AdaptiveAvgPool(\n",
    "#             visual_feature.permute(0, 3, 1, 2))  # [b, c, h, w] -> [b, w, c, h]\n",
    "#         visual_feature = visual_feature.squeeze(3)\n",
    "\n",
    "        \"\"\" Sequence modeling stage \"\"\"\n",
    "        contextual_feature = self.SequenceModeling(visual_feature)\n",
    "\n",
    "        \"\"\" Prediction stage \"\"\"\n",
    "\n",
    "        output = self.linear2(contextual_feature)\n",
    "        # [T==10, batch_size, num_classes==num_features]\n",
    "        prediction = output.permute(1, 0, 2)\n",
    "\n",
    "#         prediction = self.Prediction(contextual_feature.contiguous())\n",
    "#         prediction = prediction.permute(1, 0, 2)\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "467c87ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CTCLoss(blank=0)  # idx 0 : '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39b4cd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text_batch(text_batch):\n",
    "    text_batch_targets_lens = [len(text) for text in text_batch]\n",
    "    text_batch_targets_lens = torch.IntTensor(text_batch_targets_lens)\n",
    "\n",
    "    text_batch_concat = \"\".join(text_batch)\n",
    "    text_batch_targets = [char2idx[c] for c in text_batch_concat]\n",
    "    text_batch_targets = torch.IntTensor(text_batch_targets)\n",
    "\n",
    "    return text_batch_targets, text_batch_targets_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e1f3350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(text_batch, text_batch_logits):\n",
    "    \"\"\"\n",
    "    text_batch: list of strings of length equal to batch size\n",
    "    text_batch_logits: Tensor of size([T, batch_size, num_classes])\n",
    "    \"\"\"\n",
    "    text_batch_logps = F.log_softmax(\n",
    "        text_batch_logits, 2)  # [T, batch_size, num_classes]\n",
    "    text_batch_logps_lens = torch.full(size=(text_batch_logps.size(1),),\n",
    "                                       fill_value=text_batch_logps.size(0),\n",
    "                                       dtype=torch.int32).to(device)  # [batch_size]\n",
    "\n",
    "    text_batch_targets, text_batch_targets_lens = encode_text_batch(text_batch)\n",
    "    loss = criterion(text_batch_logps, text_batch_targets,\n",
    "                     text_batch_logps_lens, text_batch_targets_lens)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1abe84b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(text_batch_logits):\n",
    "    text_batch_tokens = F.softmax(\n",
    "        text_batch_logits, 2).argmax(2)  # [T, batch_size]\n",
    "    text_batch_tokens = text_batch_tokens.numpy().T  # [batch_size, T]\n",
    "\n",
    "    text_batch_tokens_new = []\n",
    "    for text_tokens in text_batch_tokens:\n",
    "        text = [idx2char[idx] for idx in text_tokens]\n",
    "        text = \"\".join(text)\n",
    "        text_batch_tokens_new.append(text)\n",
    "\n",
    "    return text_batch_tokens_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63a5e78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d6d3e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    model.to(device)\n",
    "\n",
    "    best_loss = 999999\n",
    "    best_model = None\n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for image_batch, text_batch in tqdm(iter(train_loader)):\n",
    "            image_batch = image_batch.to(device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             text_batch_logits = model(image_batch)\n",
    "#             loss = compute_loss(text_batch, text_batch_logits)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             train_loss.append(loss.item())\n",
    "\n",
    "#         _train_loss = np.mean(train_loss)\n",
    "        _val_loss = validation(model, val_loader, device)\n",
    "        print(\n",
    "            f'Epoch : [{epoch}] Train CTC Loss : [{_train_loss:.5f}] Val CTC Loss : [{_val_loss:.5f}]')\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(_val_loss)\n",
    "\n",
    "        if best_loss > _val_loss:\n",
    "            best_loss = _val_loss\n",
    "            best_model = model\n",
    "            torch.save(model.state_dict(), './model_2_best.pth')\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d7ff03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     preds = []\n",
    "#     with torch.no_grad():\n",
    "#         for image_batch in tqdm(iter(test_loader)):\n",
    "#             image_batch = image_batch.to(device)\n",
    "\n",
    "#             text_batch_logits = model(image_batch)\n",
    "\n",
    "#             text_batch_pred = decode_predictions(text_batch_logits.cpu())\n",
    "\n",
    "#             preds.extend(text_batch_pred)\n",
    "#     return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0015d35c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2093683311.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_822366/2093683311.py\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    _val_loss = np.mean(val_loss)\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def validation(model, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    preds = []\n",
    "    labels = []\n",
    "    acc_cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for image_batch, text_batch in tqdm(iter(val_loader)):\n",
    "            image_batch = image_batch.to(device)\n",
    "\n",
    "            text_batch_logits = model(image_batch)\n",
    "            loss = compute_loss(text_batch, text_batch_logits)\n",
    "            text_batch_pred = decode_predictions(text_batch_logits.cpu())\n",
    "            val_loss.append(loss.item())\n",
    "            preds.append(text_batch_pred)\n",
    "            labels.append(list(text_batch))\n",
    "\n",
    "    _val_loss = np.mean(val_loss)\n",
    "    batch_num = len(preds)\n",
    "    for idx in range(batch_num):\n",
    "        if pred[idx] == labels[idx]:\n",
    "            acc_cnt += 1.0\n",
    "    acc = float(acc_cnt/batch_num)\n",
    "    print(f'ACC : {acc:.5f}')\n",
    "\n",
    "    return _val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9e59da35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8eb9204058342ad9e703b8a325f2a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/415 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_822366/3647942540.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m infer_model = train(model, optimizer, train_loader,\n\u001b[0;32m---> 19\u001b[0;31m                     val_loader, scheduler, device)\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_822366/280722161.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_loader, val_loader, scheduler, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mimage_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#             optimizer.zero_grad()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model = RecognitionModel()\n",
    "\n",
    "# checkpoint = torch.load('./model_2_best.pth')\n",
    "model = Model(len(char2idx))\n",
    "# model.load_state_dict(checkpoint)\n",
    "# model.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "# model.eval()\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=model.parameters(), lr=CFG[\"LEARNING_RATE\"])\n",
    "\n",
    "# optimizer = torch.optim.Adadelta(\n",
    "#     params=model.parameters(), lr=CFG[\"LEARNING_RATE\"], rho=0.95, eps=1e-8)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=2, threshold_mode='abs', min_lr=1e-8, verbose=True)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader,\n",
    "                    val_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532948a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './model_2_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26ad908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4a4b2269",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test['img_path'].values, None)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=CFG['NUM_WORKERS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0d2c38b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for image_batch in tqdm(iter(test_loader)):\n",
    "            image_batch = image_batch.to(device)\n",
    "\n",
    "            text_batch_logits = model(image_batch)\n",
    "\n",
    "            text_batch_pred = decode_predictions(text_batch_logits.cpu())\n",
    "\n",
    "            preds.extend(text_batch_pred)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9559491e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a43d2dd465640a081f9db36b2b3fbda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/464 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = inference(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf85dd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 별 추론결과를 독립적으로 후처리\n",
    "def remove_duplicates(text):\n",
    "    if len(text) > 1:\n",
    "        letters = [text[0]] + [letter for idx,\n",
    "                               letter in enumerate(text[1:], start=1) if text[idx] != text[idx-1]]\n",
    "    elif len(text) == 1:\n",
    "        letters = [text[0]]\n",
    "    else:\n",
    "        return \"\"\n",
    "    return \"\".join(letters)\n",
    "\n",
    "\n",
    "def correct_prediction(word):\n",
    "    parts = word.split(\"-\")\n",
    "    parts = [remove_duplicates(part) for part in parts]\n",
    "    corrected_word = \"\".join(parts)\n",
    "    return corrected_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b4155e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit['label'] = predictions\n",
    "submit['label'] = submit['label'].apply(correct_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "661f7ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('./submission_rev.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848a71b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
