{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35d516bb-b5e0-43b4-9cfb-0091798caf0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install git+https://github.com/yuchenlin/LLM-Blender.git\n",
    "# !pip install \"pydantic<2\"\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "# !pip install pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52f6df33-af56-41b5-a023-98330676e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0199ae5e-4f53-4b06-813a-a5ca68e61b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
    "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast, AdamW, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "# CUDA 사용 가능 여부 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf9e2fc2-b52d-4551-a782-7b386d25f5ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a40ccca3b86a434e83f9f75a4fb4ca02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"beomi/polyglot-ko-12.8b-safetensors\"  # safetensors 컨버팅된 레포\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,  eos_token='</s>')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "                                             # quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b90abc8-2699-450f-92aa-4c5ac4f19bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_int8_training\n",
    "# # lora_config = LoraConfig(r=8,\n",
    "# #                          lora_alpha=32,\n",
    "# #                          target_modules=['q_proj','v_proj'],\n",
    "# #                          lora_dropout=0.1,\n",
    "# #                          # inference_mode=False,\n",
    "# #                          bias=\"none\",\n",
    "# #                          task_type=TaskType.CAUSAL_LM)\n",
    "# lora_config = LoraConfig(\n",
    "#     r=8, \n",
    "#     lora_alpha=32, \n",
    "#     target_modules=[\"query_key_value\"], \n",
    "#     lora_dropout=0.05, \n",
    "#     bias=\"none\", \n",
    "#     task_type=\"CAUSAL_LM\"\n",
    "# )\n",
    "\n",
    "# model.gradient_checkpointing_enable()\n",
    "\n",
    "# model = prepare_model_for_int8_training(model)\n",
    "# perf_model = get_peft_model(model, lora_config)\n",
    "\n",
    "\n",
    "\n",
    "from peft import get_peft_model, PromptTuningConfig, TaskType, PromptTuningInit, PeftModel, PeftConfig\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    model,    # 프롬프트 튜닝에 사용될 기본 모델\n",
    "    \"./alpha_lora_3_epoch/\",     # Peft 모델이 저장된 위치\n",
    "    is_trainable = False \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67c3dbf7-c1cf-44d6-bd59-8bafadbf7596",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"./data/test.csv\")\n",
    "# test_df\n",
    "# test_df[\"질문\"].iloc[107]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "166357bc-d5b0-4f94-b803-d08a69ba24c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '또한, 원목사이딩을 사용하는 것에 어떤 단점이 있을까요?',\n",
       " 11: '또한, 부실 시공으로 인해 타공하자가 발생할 가능성이 있나요?',\n",
       " 16: '그리고 유성페인트를 사용하는 것에 대한 부작용이 있을까요?',\n",
       " 17: '그리고 반점이 생긴지 1년 이내인 하자에 대해 어떤 보수작업을 해야 하나요?',\n",
       " 20: '또한, 피스 하자가 무엇인지 자세히 설명해 주실 수 있나요?',\n",
       " 22: '또한, 겨울에 도배를 할 때 특별히 신경써야 할 사항이 있을까요?',\n",
       " 23: '또한, 페인트가 남으면 어디에 보관하는 게 좋을까요?',\n",
       " 24: '또한, 유광 자기질 타일은 얼마나 오랫동안 사용할 수 있을까요?',\n",
       " 25: '그리고 스탠드조명을 선택할 때 주의할 점이 있을까요?',\n",
       " 26: '그리고 불량 도배지를 사용할 경우 도배지가 얼마나 오랫동안 버틸까요?',\n",
       " 27: '그리고 실크벽지의 교체 주기는 얼마나인가요?',\n",
       " 28: '그리고 철골구조의 장점은 무엇인가요?',\n",
       " 29: '그리고 조적식 구조란 무엇인가요?',\n",
       " 33: '또한, 액체방수공사는 무엇을 하는 것인가요?',\n",
       " 35: '그리고 장판이 남을 때 어떻게 처리해야 하나요?',\n",
       " 38: '또한, 콘크리트 벽에 구멍을 뚫는 방법에는 어떤 도구나 기술을 사용해야 하나요?',\n",
       " 39: '그리고 인테리어에 가장 많이 사용되는 도배재료는 무엇인가요?',\n",
       " 42: '그리고 복도나 협소한 공간을 확장시키기 위해 가장 효과적인 방법이 무엇일까요?',\n",
       " 48: '또한, 배관공사 시 통기구를 설치해야 하는 이유가 무엇인가요?',\n",
       " 49: '또한, 규산질계 침투성 도포 방수공사는 어떤 방식으로 이루어지나요?',\n",
       " 50: '또한, 내부와 외부 온도의 큰 차이로 인해 곰팡이 발생이 빨라지나요?',\n",
       " 54: '그리고 기둥-보 구조 방식은 무엇을 의미하나요?',\n",
       " 56: '그리고 새집증후군을 예방하는 데 가장 효과적인 방법은 무엇인가요?',\n",
       " 59: '또한, 옥상 방수용 탄성 에멀전 페인트를 사용하는 장점은 무엇인가요?',\n",
       " 63: '그리고 MSDS(Material Safety Data Sheet)는 무엇을 포함하고 있나요?',\n",
       " 66: '또한, 건조시간이 충분하지 않으면 도배지가 꼬일 수 있는 이유가 무엇인가요?',\n",
       " 70: '그리고 이를 해결하기 위해 어떤 방법을 사용할 수 있나요?',\n",
       " 74: '또한, 공간을 넓게 보이도록 인테리어를 꾸미는 방법은 뭐가 있을까요?',\n",
       " 76: '그리고 아파트 도배 평수를 어떻게 계산해야 하나요?',\n",
       " 77: '또한, 구조적 결함 때문에 석고수정이 발생할 가능성이 있는가요?',\n",
       " 81: '그리고 부적절한 설치로 인해 제품의 품질에 영향을 미칠 수 있을까요?',\n",
       " 83: '그리고 마감재의 하자를 판단하는 데 어떤 방법을 사용해야 할까요?',\n",
       " 87: '그리고 중목구조 방식에 대해 좀 더 자세히 설명해 주실 수 있을까요?',\n",
       " 91: '그리고 벽돌구조란 무엇인가요?',\n",
       " 92: '그리고 기둥-보 구조 방식은 무엇을 의미하나요?',\n",
       " 95: '그리고 인테리어에서 바닥재를 선택할 때 고려해야 할 중요한 요소는 무엇인가요?',\n",
       " 105: '또한, 도료와 벽지 중에서 어떤 것을 선택하는 것이 더 나은 선택일까요?',\n",
       " 106: '또한, 징크판넬의 단점에는 어떤 것들이 있을까요?',\n",
       " 109: '그리고 높은 습도로 인해 도배지 패턴이 이어지지 않을 수 있는 이유가 무엇일까요?',\n",
       " 112: '그리고 라돈을 측정하는 데 가장 적합한 지점은 어디인가요?',\n",
       " 113: '그리고 분말 소화기는 어떤 용도로 사용되는 건가요?',\n",
       " 114: '그리고 면진구조는 어떤 건물 구조나 시스템을 의미하나요?',\n",
       " 120: '또한, 벽에 구멍을 막는 가장 효과적인 방법은 무엇인가요?',\n",
       " 124: '그리고 내진구조란 무엇인가요?',\n",
       " 125: '그리고 아파트 도배 평수를 어떻게 계산해야 하나요?',\n",
       " 129: '그리고 오리지널징크의 장점에는 무엇이 있나요?'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "appedix_dict_1 = {}\n",
    "for idx, row in test_df.iterrows():\n",
    "    for q_col in ['질문','id']:\n",
    "        if q_col == '질문':\n",
    "            if row[q_col].count('?') >=2:\n",
    "                appendix = row[q_col].split('?')[1] + \"?\"\n",
    "                appendix = appendix.strip()\n",
    "                row[q_col] = row[q_col].split('?')[0] + \"?\"\n",
    "                row[q_col] = row[q_col].strip()\n",
    "                appedix_dict_1[idx] = appendix\n",
    "            else:\n",
    "                row[q_col] = row[q_col].strip()\n",
    "appedix_dict_1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "192018a8-bfdf-4236-9856-654c7a643781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{6: '합지벽지의 어떤 단점이 있나요?',\n",
       " 15: '페인트 상도재의 역할과 사용 방법에 대해 알려주세요.',\n",
       " 47: '습도가 높을 때 곰팡이가 어떻게 발생하는지 자세히 알고 싶습니다.',\n",
       " 55: '도배지가 남으면 어떻게 처리해야 하나요?',\n",
       " 64: '소화기 종류에는 어떤 것들이 있는지 알려주세요.',\n",
       " 86: '토목이 무엇인지 설명해줘.',\n",
       " 93: '셀룰로오스의 단점에 대해 설명해주세요.',\n",
       " 94: '반려동물을 위한 바닥재에는 어떤 종류가 있는지 알려주세요.',\n",
       " 98: '밀풀 사용 시 주의할 점은 무엇인가요?',\n",
       " 99: '통나무구조 방식은 어떤 건물에 사용되는 건축 구조 방식인가요?',\n",
       " 108: '펫테리어가 무엇인지 자세히 알려주세요.',\n",
       " 119: '철근콘크리트 구조에 대해 좀 더 자세히 알려주세요.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "appedix_dict_2 = {}\n",
    "for idx, row in test_df.iterrows():\n",
    "    for q_col in ['질문','id']:\n",
    "        if q_col == '질문':\n",
    "            if row[q_col].count('그리고') > 0 and row[q_col].count('이를') == 0:\n",
    "                appendix = row[q_col].split('그리고')[1]\n",
    "                appendix = appendix.strip()\n",
    "                row[q_col] = row[q_col].split('그리고')[0]\n",
    "                row[q_col] = row[q_col].strip()\n",
    "                appedix_dict_2[idx] = appendix\n",
    "            else:\n",
    "                row[q_col] = row[q_col].strip()\n",
    "appedix_dict_2     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "298eff93-aeae-4f88-85ce-8bffac9ff17e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: ', 압출법 단열판을 사용하는 것의 장점은 무엇인가요?또한',\n",
       " 32: ', 세라믹 타일을 사용할 때 고려해야 할 단점은 무엇인가요?또한',\n",
       " 45: ', 낡은 목재 가구의 흠집을 숨기는 방법을 알려주세요.또한',\n",
       " 52: ', 속건형 유성 발수제의 사용 목적과 효과에 대해 알려주세요.또한',\n",
       " 72: ', 강화마루의 장점은 무엇입니까?또한',\n",
       " 89: ', 아이소핑크의 장점은 무엇인가요?또한'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appedix_dict_3 = {}\n",
    "for idx, row in test_df.iterrows():\n",
    "    for q_col in ['질문','id']:\n",
    "        if q_col == '질문':\n",
    "            if row[q_col].count('또한') > 0:\n",
    "                appendix = row[q_col].split('또한')[1] + \"또한\"\n",
    "                appendix = appendix.strip()\n",
    "                row[q_col] = row[q_col].split('또한')[0] + \"또한\"\n",
    "                row[q_col] = row[q_col].strip()\n",
    "                appedix_dict_3[idx] = appendix\n",
    "            else:\n",
    "                row[q_col] = row[q_col].strip()\n",
    "\n",
    "appedix_dict_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0822e01e-87d1-4f44-af57-ab845d559c3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 300.00 MiB (GPU 0; 47.54 GiB total capacity; 46.50 GiB already allocated; 242.25 MiB free; 46.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpeft_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m peft_model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 797 (5 times)]\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 300.00 MiB (GPU 0; 47.54 GiB total capacity; 46.50 GiB already allocated; 242.25 MiB free; 46.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "peft_model.to(device)\n",
    "peft_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11ad5b43-c236-4b87-88e5-a4e14b800d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/130 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  0%|                                                  | 0/130 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 33\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# 답변 생성\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# output_sequences = model.generate(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m#     # eos_token_id=tokenizer.eos_token_id\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     output_sequences \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# min_length=20,\u001b[39;49;00m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# temperature=1.2,\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# top_k=500,\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# top_p=1.2,\u001b[39;49;00m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# repetition_penalty=1.5,\u001b[39;49;00m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# do_sample=True,\u001b[39;49;00m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# num_beams=18,\u001b[39;49;00m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# no_repeat_ngram_size=5,\u001b[39;49;00m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# num_return_sequences=1,\u001b[39;49;00m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m generated_sequence \u001b[38;5;129;01min\u001b[39;00m output_sequences:\n\u001b[1;32m     49\u001b[0m     full_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(generated_sequence, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1513\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1497\u001b[0m         input_ids,\n\u001b[1;32m   1498\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1510\u001b[0m     )\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1514\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1518\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1519\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1527\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:2350\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2347\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2349\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2350\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2351\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2353\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2354\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2358\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:1034\u001b[0m, in \u001b[0;36mGPTNeoXForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, inputs_embeds, head_mask, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;124;03mpast_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\u001b[39;00m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;124;03m    Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;124;03m>>> prediction_logits = outputs.logits\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1034\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpt_neox\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1047\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1048\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_out(hidden_states)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:925\u001b[0m, in \u001b[0;36mGPTNeoXModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    914\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    915\u001b[0m         layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    916\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    922\u001b[0m         output_attentions,\n\u001b[1;32m    923\u001b[0m     )\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 925\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:690\u001b[0m, in \u001b[0;36mGPTNeoXLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, head_mask, use_cache, layer_past, output_attentions)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    682\u001b[0m     hidden_states: Optional[torch\u001b[38;5;241m.\u001b[39mFloatTensor],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    688\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    689\u001b[0m ):\n\u001b[0;32m--> 690\u001b[0m     attention_layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m attention_layer_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: attn_output, present, (attn_weights)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_dropout(attn_output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:177\u001b[0m, in \u001b[0;36mGPTNeoXAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, head_mask, layer_past, use_cache, output_attentions, padding_mask)\u001b[0m\n\u001b[1;32m    172\u001b[0m has_layer_past \u001b[38;5;241m=\u001b[39m layer_past \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# Compute QKV\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Attention heads [batch, seq_len, hidden_size]\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m#   --> [batch, seq_len, (np * 3 * head_size)]\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_key_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# [batch, seq_len, (num_heads * 3 * head_size)]\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m#   --> [batch, seq_len, num_heads, 3 * head_size]\u001b[39;00m\n\u001b[1;32m    181\u001b[0m new_qkv_shape \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_heads, \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_size)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/peft/tuners/lora/layer.py:306\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_layer(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 306\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m active_adapter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapters:\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m active_adapter \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_A\u001b[38;5;241m.\u001b[39mkeys():\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "\n",
    "# Inference를 위한 test.csv 파일 로드\n",
    "# test = pd.read_csv('./data/test.csv')\n",
    "\n",
    "# test.csv의 '질문'에 대한 '답변'을 저장할 리스트\n",
    "file_path = \"result_alpha.txt\"\n",
    "total_preds = []\n",
    "# '질문' 컬럼의 각 질문에 대해 답변 생성\n",
    "# for idx,model in enumerate(model_list):\n",
    "preds = []\n",
    "idx = 0\n",
    "for test_question in tqdm(test_df['질문']):\n",
    "    # 입력 텍스트를 토큰화하고 모델 입력 형태로 변환\n",
    "    \n",
    "    input_ids = tokenizer.encode(test_question + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # 답변 생성\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # output_sequences = model.generate(\n",
    "        #     input_ids=input_ids.to(device),\n",
    "        #     max_length=2000,\n",
    "        #     min_length=20,\n",
    "        #     temperature=1.5,\n",
    "        #     top_k=120,\n",
    "        #     top_p=1.5,\n",
    "        #     repetition_penalty=1.0,\n",
    "        #     do_sample=True,\n",
    "        #     num_beams=120,\n",
    "        #     # no_repeat_ngram_size=3,\n",
    "        #     # num_return_sequences=1,\n",
    "        #     # eos_token_id=tokenizer.eos_token_id\n",
    "        # )\n",
    "        output_sequences = model.generate(\n",
    "            input_ids=input_ids.to(device),\n",
    "            max_length=800,\n",
    "            # min_length=20,\n",
    "            # temperature=1.2,\n",
    "            # top_k=500,\n",
    "            # top_p=1.2,\n",
    "            # repetition_penalty=1.5,\n",
    "            # do_sample=True,\n",
    "            # num_beams=18,\n",
    "            # no_repeat_ngram_size=5,\n",
    "            # num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    for generated_sequence in output_sequences:\n",
    "        full_text = tokenizer.decode(generated_sequence, skip_special_tokens=False)\n",
    "        print(full_text)\n",
    "        answer_start = full_text.find(tokenizer.eos_token) \n",
    "        anser_end = full_text.rfind(tokenizer.eos_token) \n",
    "        answer_only = full_text[answer_start+4:anser_end].strip()\n",
    "        answer_only = answer_only.replace('\\n', ' ')\n",
    "        print(answer_only)\n",
    "        preds.append(answer_only)\n",
    "        answer_only = answer_only+'\\n'\n",
    "    if idx==0:\n",
    "        with open(file_path, \"w\") as file:\n",
    "            file.write(answer_only)\n",
    "    else:\n",
    "        with open(file_path, \"a\") as file:\n",
    "            file.write(answer_only)\n",
    "    idx+=1\n",
    "\n",
    "\n",
    "    # with open(csv_file_path, mode='w') as csv_file:\n",
    "    #     csv_writer = csv.writer(csv_file)\n",
    "    #     csv_writer.writerow(answer_only)\n",
    "# break\n",
    "# total_preds.append(preds)\n",
    "# model = model.cpu()\n",
    "# torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5219c545-c1eb-4302-82fe-1e0e5b067b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                    | 0/46 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  2%|███▋                                                                                                                                                                        | 1/46 [00:03<02:54,  3.88s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원목사이딩의 단점은 가격대가 높을 뿐 아니라 관리가 어렵고 습기에 민감하여 뒤틀림, 부서짐, 수축/팽창이 발생할 수 있다는 점입니다. 또한 특히 곰팡이와 곤충에 노출될 경우 훼손될 가능성이 높다는 점도 감안해야 합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███████▍                                                                                                                                                                    | 2/46 [00:11<04:17,  5.84s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "타공불량은 부실한 시공으로 인해 발생할 수 있습니다. 예를 들어, 적절하지 않은 도구로 작업하거나 타공 부위의 크기를 정확히 측정하지 않고 작업하면 타공불량이 발생할 수 있으며, 이는 부실한 시공으로 이어질 수 있습니다. 따라서 시공자가 부실한 시공을 했거나 잘못된 도구로 작업하는 것을 방지하기 위해 제조사의 권장사항을 준수하고, 적절한 도구와 기술을 사용하여 작업하는 것이 중요합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|███████████▏                                                                                                                                                                | 3/46 [00:16<04:11,  5.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유성페인트의 단점은 주로 두 가지입니다. 첫 번째로, 유성페인트는 특히 습기가 많은 환경에서 쉽게 노출될 경우가 있어, 노출된 부분에서는 용해될 수 있다는 점이 있습니다. 두 번째로, 이러한 페인트는 대기 중에 방출되면 미세먼지나 오존 등의 오염물질을 생성할 수 있다는 것이 단점으로 꼽힙니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|██████████████▉                                                                                                                                                             | 4/46 [00:24<04:38,  6.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "반점이 생긴지 2년 이상된 경우, 벽지를 재시공하는 것이 가장 바람직한 보수작업 방법입니다. 이렇게 하면 기존의 반점이나 얼룩을 제거하고 벽지를 새로 시공하여 완전히 교체할 수 있습니다. 이 경우 전문가의 도움을 받아 작업하는 것이 좋습니다. 예를 들어, 전문 도배업체나 장인들의 도움을 받아 새로운 벽지 시공을 진행할 수 있습니다. 부분적으로 벽지를 교체하거나 전체적으로 벽지를 교체할지에 대한 판단은 전문가와 상의하여 결정하는 것이 중요합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|██████████████████▋                                                                                                                                                         | 5/46 [00:29<04:02,  5.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "피스 하자는 건물의 외벽에 사용되는 도장재료를 말합니다. 이러한 페인트는 건축물의 외관을 아름답게 꾸며주는 데에 기여하며, 때문에 건축문 주위에 많이 사용됩니다. 이를 통해 건물의 외관을 보호하고, 색상을 연출하여 시각적인 효과를 극대화합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|██████████████████████▍                                                                                                                                                     | 6/46 [00:42<05:30,  8.26s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "겨울철 도배는 몇 가지 주의해야 할 사항이 있습니다. 1. 온도 관리: 도배 작업 중에는 실내 온도가 일정하게 유지되어야 합니다. 특히 도배풀이 얼어버리는 것을 막기 위해 보일러나 전열기를 사용하여 온도를 적절하게 유지해야 합니다. 2. 과열 방지: 추운 날씨로 실내 온도를 높이기 위해 전열기를 오랜 시간 사용하는 것은 도배지에 해로울 수 있습니다. 지나치게 고열에 노출되면 도배지의 색상이 변하거나 이음매가 벌어질 수 있습니다. 3. 환기 주의: 도배 시공 직후 집안의 냄새를 제거하려는 용도로 환기를 하는 것은 좋지만, 춥고 건조한 겨울에는 도배지가 고르게 건조되지 않아 터질 수 있습니다. 이를 방지하기 위해 2일 정도는 창문을 닫아 놓는 것이 좋습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|██████████████████████████▏                                                                                                                                                 | 7/46 [00:47<04:43,  7.26s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "페인트는 일반적으로 내구성이 뛰어나기 때문에 오랜 기간 동안 사용할 수 있는 것이 장점입니다. 또한, 페인트는 환경 친화적이고 다양한 재료로 만들어져 있어 건강에 해를 끼치지 않는 것이 또 다른 장점입니다. 따라서 페인트는 실내 및 실외 장소에서 널리 사용되고 있습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████████████████████▉                                                                                                                                              | 8/46 [00:48<03:19,  5.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유광 자기질 타일의 기대수명은 50년입니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████████████████████████▋                                                                                                                                          | 9/46 [00:54<03:20,  5.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스탠드 조명은 공간을 차지하지 않아 공간 활용이 좋고, 부드러운 분위기를 연출할 수 있다는 장점이 있습니다. 또한, 스탠드 조명은 소품의 역할을 하기 때문에 인테리어에 다양한 활용이 가능합니다. 그러나 작은 공간에는 부적합하며, 불안정하게 설치할 경우 파손의 위험이 있습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|█████████████████████████████████████▏                                                                                                                                     | 10/46 [00:57<02:51,  4.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "도배지의 내구성이 떨어질 경우 사용 중에 찢어질 가능성이 높아집니다. 따라서 품질이 낮은 도배지를 사용하면 찢어지는 문제가 발생할 수 있으니, 품질이 검증된 제품을 선택하는 것이 좋습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|████████████████████████████████████████▉                                                                                                                                  | 11/46 [01:01<02:36,  4.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실크벽지의 교체주기는 일반적으로 5년에서 7년 사이입니다. 그러나 사용 환경 및 유지 관리 상태에 따라 차이가 있을 수 있습니다. 주기적인 청소와 유지보수를 통해 실크벽지의 수명을 연장시킬 수 있으니 주기적인 관리가 중요합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|████████████████████████████████████████████▌                                                                                                                              | 12/46 [01:03<02:04,  3.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "철골구조는 건물의 외벽에는 그다지 하중이 걸리지 않기 때문에 고층 건물의 건축이 가능한 것이 장점입니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|████████████████████████████████████████████████▎                                                                                                                          | 13/46 [01:08<02:13,  4.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "조적식 구조는 벽돌이나 콘크리트 블록을 모르타르로 쌓아 만드는 건축 구조를 말합니다. 주택과 같은 소규모 건물에 주로 사용되며 내습, 내구성이 우수하고 내화구조로서 안전성이 뛰어나며 경제적입니다. 그러나 수평한 외력에 상대적으로 취약하며 단일 벽체로는 수평 및 수직적 제한이 있을 수 있습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████████████████████████████████                                                                                                                       | 14/46 [01:12<02:12,  4.15s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "액체방수공사란 콘크리트, 모르타르 등의 표면에 액체 형태의 방수제를 도포하거나 침투시키고 방수제를 혼합한 모르타르를 덧발라 침투를 막는 공법입니다. 이를 통해 건물 내부로의 수분 유입을 방지하여 건물의 내구성과 수명을 유지하는 데 도움이 됩니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████████████████████████████████████████████▊                                                                                                                   | 15/46 [01:17<02:17,  4.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "장판 처리방법은 생활폐기물 스티커를 구매한 뒤, 지역의 구청 또는 주민센터에서 배출하면 됩니다. 다만, 장판의 크기에 따라 비용이 달라지므로 사전에 크기를 확인하는 것이 중요합니다. 장판을 정확히 배출하는 것은 친환경적인 방법이며, 지역 사회의 깨끗한 환경을 유지하는 데 기여하는 좋은 방법입니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███████████████████████████████████████████████████████████▍                                                                                                               | 16/46 [01:26<02:56,  5.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "콘크리트에 구멍을 뚫으려면 먼저 안전한 안경과 마스크를 착용해야 합니다. 그리고 코어드릴이나 해머드릴과 같은 강력한 전동 툴을 사용하여 구멍을 뚫어야 합니다. 이떄 사용하는 드릴 비트는 콘크리트의 강도와 종류에 따라 다르며, 일반적으로 고강도나 중강도의 드릴 비트를 사용합니다. 또한 구멍을 뚫기 전에 콘크리트 구멍을 뚫어도 괜찮은지 확인하기 위해 전기선, 수도관, 다른 배관 및 구조물이 없는지 확인하는 것이 중요합니다. 보호장비를 착용하고 안전 수칙을 준수하여 구멍을 뚫어 주세요.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███████████████████████████████████████████████████████████████▏                                                                                                           | 17/46 [01:29<02:22,  4.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인테리어에서 사용되는 바닥재로는 주로 대리석, 목재, 대나무, 종이 등이 사용됩니다. 이러한 바닥재들은 각각의 특성에 따라 고르는 방법이 다를 수 있습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|██████████████████████████████████████████████████████████████████▉                                                                                                        | 18/46 [01:36<02:39,  5.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "복도나 협소한 공간에서도 쾌적한 인테리어를 구성하는 데에는 몇 가지 방법이 있습니다. 먼저, 큰 거울을 사용하여 공간을 확장시킬 수 있습니다. 거울은 시각적인 확장 효과를 주어 공간을 넓어 보이게 만들어줍니다. 또한, 밝은 컬러의 벽지나 인테리어 소품을 활용하여 공간을 밝고 개방적으로 만들 수 있습니다. 마지막으로, 슬림한 디자인의 가구를 선택하여 협소한 공간에 적합한 가구를 활용하여 쾌적한 분위기를 조성할 수 있습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|██████████████████████████████████████████████████████████████████████▋                                                                                                    | 19/46 [01:40<02:13,  4.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "배관공사 시 통기구란 배관 내부에서 공기와 물의 순환이 원활하게 이루어지도록 하고, 온도와 압력을 조절하는 역할을 합니다. 또한 배관의 수명을 연장하고 유지보수 비용을 줄이는 데 도움이 됩니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|██████████████████████████████████████████████████████████████████████████▎                                                                                                | 20/46 [01:40<01:35,  3.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "규산질계 침투성이 뭐야?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|██████████████████████████████████████████████████████████████████████████████                                                                                             | 21/46 [01:48<02:01,  4.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "내부와 외부의 온도의 큰 차이는 곰팡이 발생을 촉진시키는 요인이 됩니다. 이는 온도의 큰 차이에 의해 도배지에 결로가 발생하여 곰팡이가 발생할 수 있기 때문입니다. 따라서 온도 관리가 중요하며, 온도차를 줄이기 위해 냉열기를 가동하는 것이 도움이 될 수 있습니다. 또한, 제습기를 가동하여 실내 습도를 관리하는 것도 중요합니다. 만약 이러한 조치에도 불구하고 곰팡이가 발생한다면, 전문가의 도움을 받아 곰팡이 제거와 취급하는 것이 좋습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|█████████████████████████████████████████████████████████████████████████████████▊                                                                                         | 22/46 [01:54<02:06,  5.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기둥의 장점은 건물의 외벽에 하중이 적게 걸리기 때문에 고층 건물의 건축이 용이하다는 점입니다. 또한 철골구조는 높은 강도를 가지고 있어 안정성이 뛰어나며, 시공성이 우수하여 건설 기간을 단축할 수 있는 장점이 있습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████████████████████████████████████████▌                                                                                     | 23/46 [02:02<02:15,  5.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "새집증후군을 예방할 수 있는 방법은 목재 가구나 건축 자재에 포름알데히드 등 유해 물질이 사용되지 않도록 주의해야 합니다. 또한 실내 흡연을 피하고, 휘발성 유기 화합물을 함유한 제품 및 방향제를 사용하지 않아야 합니다. 실내에 공기 정화 식물을 두고, 자주 환기를 시켜 공기를 맑고 건강하게 유지하는 것이 중요합니다. 또한 실내 습도를 적정하게 유지하고 정기적으로 청소를 하면 더욱 효과적일 것입니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████████████████████████████████████████████████████████████████████████████████████████▏                                                                                 | 24/46 [02:09<02:17,  6.26s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "옥상 방수용 탄성 에멜전 페인트의 장점은 건조 후 이음매 없이 연속적으로 형성된 도막이 뛰어난 내수성을 가지고 있어 건물의 충격이나 수축과 팽창에도 도막이 갈라지거나 손상되지 않으며, 침수가 장기간 반복되는 옥외폭로 조건에서도 소지 부착성과 내구성을 유지할 수 있는 것입니다. 또한, 이 제품은 탄성과 인장강도를 보유하여 방수성을 높일 뿐만 아니라 건축물의 옥상을 보호하는 데 효과적입니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|████████████████████████████████████████████████████████████████████████████████████████████▉                                                                              | 25/46 [02:23<03:00,  8.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSDS는 주로 물질안전보건자료로 사용되며, 화학 물질의 잠재적 위험에 대한 정보와 안전한 취급, 보관, 폐기에 대한 지침을 포함하는 문서입니다. MSDS는 작업장 내 화학물질에 대한 유해한 노출로부터 근로자를 보호하기 위해 산업안전보건공단, 환경공단, 미국의 OSHA(산업안전보건국)와 같은 규제 기관에서 요구하고 있습니다. 또한, MSDS는 화학물질의 유해성, 위험성, 응급조치 요령, 취급 방법 등을 설명하며, 사업주는 MSDS를 통해 취급하는 화학물질에 대한 정보를 얻어 관리하며, 근로자는 직업병이나 재해사고로부터 자신을 예방할 수 있습니다. 제조사는 산업안전보건법에 따라 MSDS를 작성하여야 하며, 각 국가마다 MSDS 양식은 다르지만, 최근에는 전세계가 동일한 양식과 체계의 MSDS를 사용하고 있습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                          | 26/46 [02:24<02:10,  6.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "건조시간이 충분하다면 도배 후 완전 건조되기 전까지 꼬임이 발생할 수 있습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                      | 27/46 [02:28<01:44,  5.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "벽지를 선택할 때는 공간의 크기, 조명 상태, 가구의 색상 등을 고려해야 합니다. 또한, 벽지의 패턴과 질감도 전체적인 디자인에 맞게 조화롭게 선택하는 것이 중요합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                   | 28/46 [02:41<02:20,  7.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "넓은 공간을 연출하기 위해서는 몇 가지 포인트를 고려해야 합니다. 먼저, 큰 거울을 설치하여 공간을 확장하는 것이 중요합니다. 또한, 밝은 컬러의 벽지나 가구를 선택하여 공간을 밝고 개방적으로 만들 수 있습니다. 마지막으로, 조명과 색상을 효율적으로 활용하여 공간을 밝하고 개방적으로 만들어보세요. 예를 들어, 간접 조명과 밝은 조명을 활용하여 공간의 분위기를 조절하고, 이중선반을 활용하여 수납 공간을 효과적으로 확보할 수 있습니다. 또한, 공간을 넓어 보이게 하는 데코타일을 활용하는 것도 좋은 방법입니다. 데코타일은 저렴한 가격과 간편한 시공뿐만 아니라 우수한 내구성과 내수성을 갖추고 있어 많은 사람들에게 선호되고 있습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                               | 29/46 [02:42<01:38,  5.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아파트 도배평수는 일반적으로 분양 평수*2.5로 계산합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                           | 30/46 [02:49<01:40,  6.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "석고수정은 건물의 구조적 결함으로 인해 발생할 수 있습니다. 건물 구조적 결함은 건물의 설계 또는 시공 단계에서 발생한 결함으로, 예를 들어 지지체의 불균형, 부재의 미반영 등이 해당됩니다. 이러한 결함으로 인해 석고수정이 용이하게 발생할 수 있으며, 이에 대한 대책으로는 제습기를 가동하고 환기를 통해 실내 적정 습도를 유지하는 것이 중요합니다. 또한, 전문가의 도움을 받아 보수 작업을 진행하는 것이 좋습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                       | 31/46 [02:53<01:22,  5.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "면 불량은 주로 시공자가 부실 설치를 했거나 잘못된 공구와 재료를 사용했을 때 발생할 수 있습니다. 이러한 경우에는 면 불량이 발생할 가능성이 높으므로 시공자로부터 연락을 받는 것이 좋습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                    | 32/46 [03:03<01:37,  6.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "마감재의 하자 여부를 판단하기 위해서는 몇 가지 요소를 고려해야 합니다. 첫째로는 설계도서와의 일치 여부가 있습니다. 마감재가 설계도서에 명시된 내용과 일치하는지 확인해야 합니다. 둘째로는 기능상의 문제, 예를 들어 마감재가 쉽게 파손되거나 사용상의 문제가 있는지를 확인해야 합니다. 그 다음으로는 미관상의 문제 여부도 중요합니다. 마감재의 외관이나 마감재들 간의 조화가 보존되었는지 확인해야 합니다공극이 발생하는 원인 - 책임소재 - 해결방법에 대해 설명드리겠습니다. 1. 공간 내 높은 습도 원인 : 높은 습기는 도배지의 끝 부분이 들뜰 수 있어?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                | 33/46 [03:09<01:24,  6.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중목구조는 건물의 지탱을 하기 위해 건축물의 틀과 지지대로 사용되는 양식을 말합니다. 이 구조 방식은 기본적으로 기중-보 방식과 유사하지만, 구조용 목재로 큰 각재를 사용하여 건축하는 것이 특징입니다. 중목구조는 건물 전체에 일정한 하중을 전달하면서 내구성과 안정성을 확보하는 데에 사용됩니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                            | 34/46 [03:14<01:12,  6.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "벽돌은 건물의 벽체나 기초를 벽돌과 모르타르로 쌓아 만드는 건축 방식을 말합니다. 이는 조적식 구조의 기본이며, 블록, 석재, 타일 등을 섞어 사용하기도 합니다. 이러한 구조법은 주로 담장, 굴뚝, 그리고 기타 다양한 구조물을 축조하는 데 활용됩니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                         | 35/46 [03:20<01:08,  6.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기둥의 장점은 건물의 외벽에 하중이 적게 걸리기 때문에 고층 건물의 건축이 용이하다는 점입니다. 또한 철골구조는 높은 강도를 가지고 있어 안정성이 뛰어나며, 시공성이 우수하여 건설 기간을 단축할 수 있는 장점이 있습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                     | 36/46 [03:26<01:00,  6.07s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "바닥재를 선택할 경우 가장 중요한 것은 사용 공간의 특성을 고려하는 것입니다. 예를 들어, 주방이나 욕실과 같은 습기가 많은 공간에는 습기에 강한 소재를 선택해야 합니다. 또한, 바닥재의 색상과 디자인은 기존 가구와 잘 어울리는지, 그리고 청소와 유지보수가 쉬운지도 고려해야 합니다. 따라서 바닥재를 선택하면 공간을 효율적으로 활용할 수 있습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                 | 37/46 [03:30<00:49,  5.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "도배지는 패턴과 텍스처를 추가할 때 좋고, 도료는 단색이며 부드러운 마감을 원할 때 유용합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                             | 38/46 [03:36<00:43,  5.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "징크판넬의 가격이 다른 소재에 비해 비교적 높은 편이며, 시공이 잘못될 경우 쉽게 녹슬 수 있다는 점이 있습니다. 이러한 단점을 고려하여 적절한 유지보수와 정기적인 관리가 필요합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                          | 39/46 [03:42<00:40,  5.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "높은 습도로 인해 석고보드의 패턴이 이어지지 이는 않은 경우, 이음메 부분이 들뜰 수 있습니다. 이는 도배 작업의 품질을 떨어지게 만들 수 있습니다. 따라서 습도를 관리하고 적절한 환기를 유지하는 것이 중요합니다. 또한, 온도와 습도의 변화에 따라 도배지의 패턴이 달라질 수 있으므로 이러한 요소들을 고려하여 도배 작업을 진행하는 것이 좋습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                      | 40/46 [03:48<00:35,  5.91s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라돈은 공동주택의 실내공기질 측정지점으로 알려져 있습니다. 공동주택의 총 세대수가 100세대일 때는 3세대를 측정하고, 이후 100세대가 증가할 때마다 1세대씩 추가하여 최대 12세대까지 시료를 채취합니다. 이는 공동주택의 구조와 크기를 고려하여 대표성 있는 측정값을 얻기 위한 것입니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                  | 41/46 [03:53<00:28,  5.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분말 소화기는 탄산수소 나트륨 분말을 사용하여 불을 질식시키는 불연성 고압가스에 의해 소화약제를 방출하는 소화기입니다. 주로 기체 화재에 사용되며, 높은 화재 위험이 있는 곳에 설치되어 있습니다. 독특한 작동 방식으로 인해 일반적인 불화재 소화기보다 불연소 화재를 진압하는 효과적인 소화기입니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏              | 42/46 [03:55<00:17,  4.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "면진구조란 면진 장치를 사용해 지반과 건물을 분리시키는 방법을 의미합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊           | 43/46 [03:56<00:10,  3.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "구멍을 메우기 위해 합성 먼지 또는 석고를 사용하고, 필요하면 패팅을 적용합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌       | 44/46 [04:02<00:08,  4.23s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "내진구조는 건물의 구조물에 충격이 발생했을 때 붕괴되지 않도록 설계된 구조를 말합니다. 내진구조는 지진 발생 시 건물의 붕괴나 큰 피해를 최소화하기 위해 설계된 시스템으로, 건물의 기초나 건물 구조물에 충격을 완화시키는 장치 등을 설치하여 지진으로 인한 피해를 최소화합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 45/46 [04:03<00:03,  3.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아파트 도배평수는 일반적으로 분양 평수*2.5로 계산합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46/46 [04:08<00:00,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오리지널징크는 다른 징크에 비해 수명이 길고 다양한 패턴과 디자인이 가능하며 친환경적이고 금속 부식에 대한 내식성이 뛰어나 유지보수가 쉽다는 장점이 있습니다. 또한 오리지널징크는 가벼우면서도 매우 견고하며 낮은 유지보수 비용과 높은 재활용 가능성을 갖고 있습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = \"result_alpha_appendix_1_r2.txt\"\n",
    "\n",
    "\n",
    "for idx, test_question in tqdm(appedix_dict_1.items()):\n",
    "    # 입력 텍스트를 토큰화하고 모델 입력 형태로 변환\n",
    "    \n",
    "    input_ids = tokenizer.encode(test_question + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # 답변 생성\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # output_sequences = model.generate(\n",
    "        #     input_ids=input_ids.to(device),\n",
    "        #     max_length=2000,\n",
    "        #     min_length=20,\n",
    "        #     temperature=1.5,\n",
    "        #     top_k=120,\n",
    "        #     top_p=1.5,\n",
    "        #     repetition_penalty=1.0,\n",
    "        #     do_sample=True,\n",
    "        #     num_beams=120,\n",
    "        #     # no_repeat_ngram_size=3,\n",
    "        #     # num_return_sequences=1,\n",
    "        #     # eos_token_id=tokenizer.eos_token_id\n",
    "        # )\n",
    "        output_sequences = model.generate(\n",
    "            input_ids=input_ids.to(device),\n",
    "            max_length=800,\n",
    "            min_length=20,\n",
    "            temperature=0.8,\n",
    "            top_k=500,\n",
    "            top_p=0.8,\n",
    "            repetition_penalty=1.5,\n",
    "            do_sample=True,\n",
    "            num_beams=18,\n",
    "            no_repeat_ngram_size=5,\n",
    "            # num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "\n",
    "    for generated_sequence in output_sequences:\n",
    "        full_text = tokenizer.decode(generated_sequence, skip_special_tokens=False)\n",
    "        print(full_text)\n",
    "        answer_start = full_text.find(tokenizer.eos_token) \n",
    "        anser_end = full_text.rfind(tokenizer.eos_token) \n",
    "        answer_only = full_text[answer_start+4:anser_end].strip()\n",
    "        answer_only = answer_only.replace('\\n', ' ')\n",
    "        preds.append(answer_only)\n",
    "        print(answer_only)\n",
    "        answer_only = str(idx)+answer_only+'\\n'\n",
    "    if idx==0:\n",
    "        with open(file_path, \"w\") as file:\n",
    "            file.write(answer_only)\n",
    "    else:\n",
    "        with open(file_path, \"a\") as file:\n",
    "            file.write(answer_only)\n",
    "    idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16f13b91-1b26-4860-9e71-acfc62cc4f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                    | 0/12 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  8%|██████████████▎                                                                                                                                                             | 1/12 [00:01<00:21,  1.96s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "합지벽지는 가격이 저렴하고 이음새 겹침으로 시공이 쉬우며 실크벽지보다 친환경인 점이 장점입니다.\n",
      "합지벽지는 가격이 저렴하고 이음새 겹침으로 시공이 쉬우며 실크벽지보다 친환경인 점이 장점입니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|████████████████████████████▋                                                                                                                                               | 2/12 [00:07<00:39,  3.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "페인트 상도기는 최종 마감으로 칠하는 페인트로, 영어로는 Top Coating이라고 합니다. 상도재는 하도재와 주도재의 제품을 보호하는 역할을 담당하며, 외부 표면에 노출될 때의 열 및 기후에 강하며 내구성이 높은 특징을 가지고 있습니다. 이러한 장점 때문에 가격이 다소 비쌉니다.\n",
      "페인트 상도기는 최종 마감으로 칠하는 페인트로, 영어로는 Top Coating이라고 합니다. 상도재는 하도재와 주도재의 제품을 보호하는 역할을 담당하며, 외부 표면에 노출될 때의 열 및 기후에 강하며 내구성이 높은 특징을 가지고 있습니다. 이러한 장점 때문에 가격이 다소 비쌉니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████████████████████████████████████████                                                                                                                                 | 3/12 [00:08<00:26,  2.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네, 습도가 높으면 환경이 곰팡이에게 이상적이므로 곰팡이가 쉽게 발생할 수 있습니다.\n",
      "네, 습도가 높으면 환경이 곰팡이에게 이상적이므로 곰팡이가 쉽게 발생할 수 있습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|█████████████████████████████████████████████████████████▎                                                                                                                  | 4/12 [00:14<00:32,  4.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "도배지를 처리하는 방법 중 하나는 폐기물 스티커를 부착하는 것입니다. 폐기물 스티커는 동사무소, 편의점, 일반 슈퍼 등에서 구매할 수 있습니다. 또한, 아파트의 경우에는 경비실에 비치되어 있는 경우가 있으니 해당 시설의 담당자나 관리사무실에 문의하여 얻을 수도 있습니다.\n",
      "도배지를 처리하는 방법 중 하나는 폐기물 스티커를 부착하는 것입니다. 폐기물 스티커는 동사무소, 편의점, 일반 슈퍼 등에서 구매할 수 있습니다. 또한, 아파트의 경우에는 경비실에 비치되어 있는 경우가 있으니 해당 시설의 담당자나 관리사무실에 문의하여 얻을 수도 있습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|███████████████████████████████████████████████████████████████████████▋                                                                                                    | 5/12 [00:17<00:24,  3.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "소화기의 주요 종류로는 ABC 소화기, 분말 소화기, 이산화탄소 소화기가 있습니다. 또한 물 소화기와 거품 소화기 등도 있으며, 화재 종류에 따라 다양한 소화기가 사용됩니다.\n",
      "소화기의 주요 종류로는 ABC 소화기, 분말 소화기, 이산화탄소 소화기가 있습니다. 또한 물 소화기와 거품 소화기 등도 있으며, 화재 종류에 따라 다양한 소화기가 사용됩니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████████████████████████████████████████████                                                                                      | 6/12 [00:24<00:29,  4.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토목은 목재, 철재, 토석 등을 활용하여 도로, 교량, 항만, 제방, 철도, 건물, 상하수도 등을 건설하는 공사를 의미합니다. 이와 같은 공사를 수행하는 전문가들을 토목기술자라고 하며, 토목은 인프라를 구축하고 유지하는 데 중요한 역할을 합니다. 토목 기술은 안전하고 효율적인 건설을 위해 세밀한 계획과 전문적인 기술이 필요합니다. 또한, 환경 보호 및 지속가능한 발전을 고려하여 현대적이고 친환경적인 건설을 지향하고 있습니다.\n",
      "토목은 목재, 철재, 토석 등을 활용하여 도로, 교량, 항만, 제방, 철도, 건물, 상하수도 등을 건설하는 공사를 의미합니다. 이와 같은 공사를 수행하는 전문가들을 토목기술자라고 하며, 토목은 인프라를 구축하고 유지하는 데 중요한 역할을 합니다. 토목 기술은 안전하고 효율적인 건설을 위해 세밀한 계획과 전문적인 기술이 필요합니다. 또한, 환경 보호 및 지속가능한 발전을 고려하여 현대적이고 친환경적인 건설을 지향하고 있습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                       | 7/12 [00:25<00:18,  3.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "셀룰로오스는 정밀한 시공능력이 전제된다는 단점이 있습니다.\n",
      "셀룰로오스는 정밀한 시공능력이 전제된다는 단점이 있습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                         | 8/12 [00:30<00:16,  4.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "반려동물을 위한바닥재로는 미끄럼 방지 기능이 있는 바닥재가 적합합니다. 반려동물은 뛰어다니는 습성이 있기 때문에 미끄러질 경우 다칠 위험이 있고 다리 관절에 무리가 갈 수 있습니다. 또한, 바닥재를 바꾸기 어려운 상황이라면 카펫이나 러그를 깔아두는 것도 좋은 대안이 될 수 있습니다.\n",
      "반려동물을 위한바닥재로는 미끄럼 방지 기능이 있는 바닥재가 적합합니다. 반려동물은 뛰어다니는 습성이 있기 때문에 미끄러질 경우 다칠 위험이 있고 다리 관절에 무리가 갈 수 있습니다. 또한, 바닥재를 바꾸기 어려운 상황이라면 카펫이나 러그를 깔아두는 것도 좋은 대안이 될 수 있습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                           | 9/12 [00:35<00:12,  4.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "밀풀은 가루 형태의 도배재료로, 시공 전에 물과 혼합하여 사용합니다. 이 제품은 밀풀과는 달리 곰팡이 발생이 적고 무게가 가벼운 것이 장점이며, 하지만 접착력이 상대적으로 낮다는 단점이 있습니다. 이러한 특징을 감안하여 시공 전에 적합한 환경과 접착제를 선택하는 것이 중요합니다.\n",
      "밀풀은 가루 형태의 도배재료로, 시공 전에 물과 혼합하여 사용합니다. 이 제품은 밀풀과는 달리 곰팡이 발생이 적고 무게가 가벼운 것이 장점이며, 하지만 접착력이 상대적으로 낮다는 단점이 있습니다. 이러한 특징을 감안하여 시공 전에 적합한 환경과 접착제를 선택하는 것이 중요합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                            | 10/12 [00:43<00:10,  5.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "통나무구조 방식이란 주로 원형이나 각형의 내력벽으로 구성되며, 나머지 바닥이나 지붕은 2x4 경량 목구조와 유사한 방식으로 조립됩니다. 이 구조는 벽체가 통나무를 쌓는 형태이기 때문에 1층 높이에서 7~12cm 정도의 침하가 장기적으로 발생할 수 있으므로, 창문틀 등의 개구부에는 침하를 고려한 디테일을 고려해야 합니다. 통나무구조 방식은 자연적이고 따뜻한 분위기를 연출해내기 때문에 주택 및 휴양시설에서 선호되는 구조 방식 중 하나입니다.\n",
      "통나무구조 방식이란 주로 원형이나 각형의 내력벽으로 구성되며, 나머지 바닥이나 지붕은 2x4 경량 목구조와 유사한 방식으로 조립됩니다. 이 구조는 벽체가 통나무를 쌓는 형태이기 때문에 1층 높이에서 7~12cm 정도의 침하가 장기적으로 발생할 수 있으므로, 창문틀 등의 개구부에는 침하를 고려한 디테일을 고려해야 합니다. 통나무구조 방식은 자연적이고 따뜻한 분위기를 연출해내기 때문에 주택 및 휴양시설에서 선호되는 구조 방식 중 하나입니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊              | 11/12 [00:50<00:05,  5.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "펫테리어란 ‘반려동물(pet)’과 ‘인테리어(interior)’의 합성어로, 반려동물과의 생활을 고려한 편의성을 높이면서도 주거 공간의 인테리어를 고려하는 스타일을 말합니다. 즉, 반려동물과 관련된 편의시설과 인테리어 소품을 활용하여 쾌적하고 안락한 공간을 조성하는 것이 펫테리어의 핵심입니다. 이를 통해 주거 환경을 반려동물과 함께하는 가족들에게 쾌적한 공간을 제공하는 것이 목표입니다.\n",
      "펫테리어란 ‘반려동물(pet)’과 ‘인테리어(interior)’의 합성어로, 반려동물과의 생활을 고려한 편의성을 높이면서도 주거 공간의 인테리어를 고려하는 스타일을 말합니다. 즉, 반려동물과 관련된 편의시설과 인테리어 소품을 활용하여 쾌적하고 안락한 공간을 조성하는 것이 펫테리어의 핵심입니다. 이를 통해 주거 환경을 반려동물과 함께하는 가족들에게 쾌적한 공간을 제공하는 것이 목표입니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:56<00:00,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "철근콘크리트구조는 콘크리트의 압축 강도를 활용하면서도 인장 응력에 대한 취약점을 보완하기 위해 철근을 사용하는 구조입니다. 일반적으로 도심 지역에서 많이 사용되며, 내구성, 내화 및 내진 성능이 뛰어나지만, 철근의 보강으로 인해 구조물의 중량이 증가하여 고층 건물에서는 아래층의 보나 기둥 단면적이 커져야 하는 문제가 있습니다.\n",
      "철근콘크리트구조는 콘크리트의 압축 강도를 활용하면서도 인장 응력에 대한 취약점을 보완하기 위해 철근을 사용하는 구조입니다. 일반적으로 도심 지역에서 많이 사용되며, 내구성, 내화 및 내진 성능이 뛰어나지만, 철근의 보강으로 인해 구조물의 중량이 증가하여 고층 건물에서는 아래층의 보나 기둥 단면적이 커져야 하는 문제가 있습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = \"result_alpha_appendix_2_r2.txt\"\n",
    "\n",
    "\n",
    "for idx, test_question in tqdm(appedix_dict_2.items()):\n",
    "    # 입력 텍스트를 토큰화하고 모델 입력 형태로 변환\n",
    "    \n",
    "    input_ids = tokenizer.encode(test_question + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # 답변 생성\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # output_sequences = model.generate(\n",
    "        #     input_ids=input_ids.to(device),\n",
    "        #     max_length=2000,\n",
    "        #     min_length=20,\n",
    "        #     temperature=1.5,\n",
    "        #     top_k=120,\n",
    "        #     top_p=1.5,\n",
    "        #     repetition_penalty=1.0,\n",
    "        #     do_sample=True,\n",
    "        #     num_beams=120,\n",
    "        #     # no_repeat_ngram_size=3,\n",
    "        #     # num_return_sequences=1,\n",
    "        #     # eos_token_id=tokenizer.eos_token_id\n",
    "        # )\n",
    "        output_sequences = model.generate(\n",
    "            input_ids=input_ids.to(device),\n",
    "            max_length=800,\n",
    "            min_length=20,\n",
    "            temperature=1.2,\n",
    "            top_k=500,\n",
    "            top_p=1.2,\n",
    "            repetition_penalty=1.5,\n",
    "            do_sample=True,\n",
    "            num_beams=18,\n",
    "            no_repeat_ngram_size=5,\n",
    "            # num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "\n",
    "    for generated_sequence in output_sequences:\n",
    "        full_text = tokenizer.decode(generated_sequence, skip_special_tokens=False)\n",
    "        print(full_text)\n",
    "        answer_start = full_text.find(tokenizer.eos_token) \n",
    "        anser_end = full_text.rfind(tokenizer.eos_token) \n",
    "        answer_only = full_text[answer_start+4:anser_end].strip()\n",
    "        answer_only = answer_only.replace('\\n', ' ')\n",
    "        preds.append(answer_only)\n",
    "        print(answer_only)\n",
    "        print(answer_only)\n",
    "        answer_only = str(idx)+answer_only+'\\n'\n",
    "    if idx==0:\n",
    "        with open(file_path, \"w\") as file:\n",
    "            file.write(answer_only)\n",
    "    else:\n",
    "        with open(file_path, \"a\") as file:\n",
    "            file.write(answer_only)\n",
    "    idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "063d2123-28e5-4302-a192-86b00735ab3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                     | 0/6 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 17%|████████████████████████████▊                                                                                                                                                | 1/6 [00:05<00:25,  5.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "압출법단열판은 습기에 강하고 곰팡이 및 세균 증식을 막을 수 있는데, 이는 건강한 실내 환경을 유지하는 데 도움을 줍니다. 또한, 열전도율이 가장 낮아서 효율적인 단열 효과를 제공하며 시공이 간편하여 공사 기간을 단축하는 장점이 있습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|█████████████████████████████████████████████████████████▋                                                                                                                   | 2/6 [00:10<00:20,  5.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "세라믹 타일의 단점으로는 시공이 어려워 하자가 발생할 확률이 높고, 숙련된 기술자가 시공을 맡아야 하므로 가격과 시공비가 상대적으로 높다는 점이 있습니다. 또한 일반 타일보다 슬립리스 처리가 부족하여 미끄러질 수 있는 위험이 있을 수 있습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████████████████████████████████████████████▌                                                                                      | 3/6 [00:15<00:15,  5.13s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "흠집을 목재 펜스틱으로 채우고 흠집을 완만한 흠집 채우기 바로 전에 샌드지로 문질러 조각을 지나치게 여기지 않는 것이 중요합니다. 도마 위에서 펜스틱이 식으면, 나무 왁스를 사용하여 마감하세요. 이러한 마무리 작업은 가구에 쉽게 흠집이 들어가거나 노출되는 부분이 자주 있는 상황에서 효과적입니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                         | 4/6 [00:17<00:07,  3.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "속건형 유성 발수제로 도장된 표면에 도장된 색상이 변하지 않도록 하는 방법이 있을까요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                            | 5/6 [00:22<00:04,  4.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:30003 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "강화마루는 원목 무늬 필름지가 하드코팅 처리된 바닥재를 의미합니다. 일반적으로 평당 7~12 만원의 가격대를 가지고 있습니다. 이 바닥재는 온도나 습도에 의한 변색이 거의 없고, 강도가 뛰어나 수명이 긴 장점이 있습니다. 하지만, 필름지로 코팅처리되어 있기 때문에 질감이나 보행감이 조금 떨어진다는 단점이 있습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:30<00:00,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아이소핑크의 단점은 주로 비용 측면에서 언급됩니다. 이 외에도 아이소핑크는 단열재 중에서 열전도율이 가장 낮아 에너지 절약 효과가 크며, 습기에 강하고 곰팡이 및 세균 증식을 막아주는 특징이 있습니다. 또한 시공이 간편하여 공사 기간과 비용을 절약할 수 있는 장점이 있습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = \"result_alpha_appendix_3_r2.txt\"\n",
    "\n",
    "\n",
    "for idx, test_question in tqdm(appedix_dict_3.items()):\n",
    "    # 입력 텍스트를 토큰화하고 모델 입력 형태로 변환\n",
    "    \n",
    "    input_ids = tokenizer.encode(test_question + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # 답변 생성\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # output_sequences = model.generate(\n",
    "        #     input_ids=input_ids.to(device),\n",
    "        #     max_length=2000,\n",
    "        #     min_length=20,\n",
    "        #     temperature=1.5,\n",
    "        #     top_k=120,\n",
    "        #     top_p=1.5,\n",
    "        #     repetition_penalty=1.0,\n",
    "        #     do_sample=True,\n",
    "        #     num_beams=120,\n",
    "        #     # no_repeat_ngram_size=3,\n",
    "        #     # num_return_sequences=1,\n",
    "        #     # eos_token_id=tokenizer.eos_token_id\n",
    "        # )\n",
    "        output_sequences = model.generate(\n",
    "            input_ids=input_ids.to(device),\n",
    "            max_length=800,\n",
    "            min_length=20,\n",
    "            temperature=1.2,\n",
    "            top_k=500,\n",
    "            top_p=1.2,\n",
    "            repetition_penalty=1.5,\n",
    "            do_sample=True,\n",
    "            num_beams=18,\n",
    "            no_repeat_ngram_size=5,\n",
    "            # num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "\n",
    "    for generated_sequence in output_sequences:\n",
    "        full_text = tokenizer.decode(generated_sequence, skip_special_tokens=False)\n",
    "        print(full_text)\n",
    "        answer_start = full_text.find(tokenizer.eos_token) \n",
    "        anser_end = full_text.rfind(tokenizer.eos_token) \n",
    "        answer_only = full_text[answer_start+4:anser_end].strip()\n",
    "        answer_only = answer_only.replace('\\n', ' ')\n",
    "        preds.append(answer_only)\n",
    "        print(answer_only)\n",
    "        answer_only = str(idx)+answer_only+'\\n'\n",
    "    if idx==0:\n",
    "        with open(file_path, \"w\") as file:\n",
    "            file.write(answer_only)\n",
    "    else:\n",
    "        with open(file_path, \"a\") as file:\n",
    "            file.write(answer_only)\n",
    "    idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e2a3e6d-b53b-4dc3-9bc5-f9466245b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# source_prefix = \"<|source|>\"\n",
    "# cand1_prefix = \"<|candidate1|>\"\n",
    "# cand2_prefix = \"<|candidate2|>\"\n",
    "# inputs = [\"hello!\", \"I love you!\"]\n",
    "# candidates_A = [\"hi!\", \"I hate you!\"]\n",
    "# candidates_B = [\"f**k off!\", \"I love you, too!\"]\n",
    "# def tokenize_pair(sources:List[str], candidate1s:List[str], candidate2s:List[str], source_max_length=1224, candidate_max_length=412):\n",
    "#     ids = []\n",
    "#     assert len(sources) == len(candidate1s) == len(candidate2s)\n",
    "#     max_length = source_max_length + 2 * candidate_max_length\n",
    "#     for i in range(len(sources)):\n",
    "#         source_ids = tokenizer.encode(source_prefix + sources[i], max_length=source_max_length, truncation=True)\n",
    "#         candidate_max_length = (max_length - len(source_ids)) // 2\n",
    "#         candidate1_ids = tokenizer.encode(cand1_prefix + candidate1s[i], max_length=candidate_max_length, truncation=True)\n",
    "#         candidate2_ids = tokenizer.encode(cand2_prefix + candidate2s[i], max_length=candidate_max_length, truncation=True)\n",
    "#         ids.append(source_ids + candidate1_ids + candidate2_ids)\n",
    "#     encodings = tokenizer.pad({\"input_ids\": ids}, return_tensors=\"pt\", padding=\"max_length\", max_length=max_length)\n",
    "#     return encodings\n",
    "\n",
    "# encodings = tokenize_pair(inputs, candidates_A, candidates_B)\n",
    "# encodings = {k:v.to(pairrm.device) for k,v in encodings.items()}\n",
    "# outputs = pairrm(**encodings)\n",
    "# logits = outputs.logits.tolist()\n",
    "# comparison_results = outputs.logits > 0\n",
    "# print(logits)\n",
    "# # [1.9003021717071533, -1.2547134160995483]\n",
    "# print(comparison_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeb3ca3-79db-4997-96b5-a0918f677309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2125879a-50b4-43e8-9542-e987e6e656c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
