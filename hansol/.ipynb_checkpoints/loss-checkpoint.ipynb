{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "629f009e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip install sentence_transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4599b3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentation?\n",
    "# EDA?\n",
    "# 전처리 : KoNLPy Okt \n",
    "# multi-loss\n",
    "# fine-tune\n",
    "# ensenble\n",
    "#0501 BERT 분류 KLUE/bert-base subword tokenizer 로 토큰화 하기\n",
    "#https://wikidocs.net/198955"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "183e48b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer \u001b[38;5;66;03m# SentenceTransformer Version 2.2.2\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Embedding Vector 추출에 활용할 모델(distiluse-base-multilingual-cased-v1) 불러오기\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistiluse-base-multilingual-cased-v1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer # SentenceTransformer Version 2.2.2\n",
    "\n",
    "# Embedding Vector 추출에 활용할 모델(distiluse-base-multilingual-cased-v1) 불러오기\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e54bc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 예시\n",
    "preds = [\n",
    "    \"이번 경진대회는 질의 응답 처리를 수행하는 AI 모델을 개발해야합니다.\",\n",
    "    \"데이콘은 플랫폼입니다.\"\n",
    "]\n",
    "\n",
    "gts = [\n",
    "    \"이번 경진대회의 주제는 도배 하자 질의 응답 AI 모델 개발입니다.\",\n",
    "    \"데이콘은 국내 최대의 AI 경진대회 플랫폼입니다.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cc308b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플에 대한 Cosine Similarity 산식\n",
    "def cosine_similarity(a, b):\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b) if norm_a != 0 and norm_b != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f22965f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 :  이번 경진대회는 질의 응답 처리를 수행하는 AI 모델을 개발해야합니다.\n",
      "정답 :  이번 경진대회의 주제는 도배 하자 질의 응답 AI 모델 개발입니다.\n",
      "Cosine Similarity Score :  0.8344025\n",
      "--------------------\n",
      "예측 :  데이콘은 플랫폼입니다.\n",
      "정답 :  데이콘은 국내 최대의 AI 경진대회 플랫폼입니다.\n",
      "Cosine Similarity Score :  0.62814224\n",
      "--------------------\n",
      "전체 샘플의 Cosine Similarity Score 평균 :  0.73127234\n"
     ]
    }
   ],
   "source": [
    "sample_scores = []\n",
    "for pred, gt in zip(preds, gts):\n",
    "    # 생성된 답변 내용을 512 Embedding Vector로 변환\n",
    "    pred_embed = model.encode(pred)\n",
    "    gt_embed = model.encode(gt)\n",
    "    \n",
    "    sample_score = cosine_similarity(gt_embed, pred_embed)\n",
    "    # Cosine Similarity Score가 0보다 작으면 0으로 간주\n",
    "    sample_score = max(sample_score, 0)\n",
    "    print('예측 : ', pred)\n",
    "    print('정답 : ', gt)\n",
    "    print('Cosine Similarity Score : ', sample_score)\n",
    "    print('-'*20)\n",
    "    sample_scores.append(sample_score)\n",
    "print('전체 샘플의 Cosine Similarity Score 평균 : ', np.mean(sample_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14572b1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/server-003/anaconda3/envs/pcdet/lib/python3.8/site-packages/tqdm-4.64.1-py3.8.egg (from transformers) (4.64.1)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2023.12.25-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (777 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m777.0/777.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.19,>=0.14\n",
      "  Using cached tokenizers-0.15.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Using cached safetensors-0.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3\n",
      "  Using cached huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/server-003/anaconda3/envs/pcdet/lib/python3.8/site-packages/packaging-23.0-py3.8.egg (from transformers) (23.0)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/server-003/anaconda3/envs/pcdet/lib/python3.8/site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: requests in /home/server-003/anaconda3/envs/pcdet/lib/python3.8/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/server-003/anaconda3/envs/pcdet/lib/python3.8/site-packages/PyYAML-6.0-py3.8-linux-x86_64.egg (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/server-003/anaconda3/envs/pcdet/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.4.0)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Using cached fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/server-003/anaconda3/envs/pcdet/lib/python3.8/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/server-003/anaconda3/envs/pcdet/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/server-003/anaconda3/envs/pcdet/lib/python3.8/site-packages (from requests->transformers) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/server-003/anaconda3/envs/pcdet/lib/python3.8/site-packages (from requests->transformers) (1.26.14)\n",
      "Installing collected packages: safetensors, regex, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.13.1 fsspec-2023.12.2 huggingface-hub-0.20.3 regex-2023.12.25 safetensors-0.4.2 tokenizers-0.15.1 transformers-4.37.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7d9974e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast, AdamW\n",
    "from tqdm import tqdm\n",
    "# CUDA 사용 가능 여부 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea2eaebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "644it [00:00, 792.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "data = pd.read_csv('./data/train.csv')\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('skt/kogpt2-base-v2', eos_token='</s>')\n",
    "\n",
    "# 데이터 포맷팅 및 토크나이징\n",
    "formatted_data = []\n",
    "for _, row in tqdm(data.iterrows()):\n",
    "    for q_col in ['질문_1', '질문_2']:\n",
    "        for a_col in ['답변_1', '답변_2', '답변_3', '답변_4', '답변_5']:\n",
    "            # 질문과 답변 쌍을 </s> token으로 연결\n",
    "            input_text = row[q_col] + tokenizer.eos_token + row[a_col]\n",
    "            input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "            formatted_data.append(input_ids)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc9643d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/server-003/anaconda3/envs/pcdet/lib/python3.8/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1 - Avg Loss: 2.8441: 100%|████████████████████████████████| 6440/6440 [05:51<00:00, 18.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Average Loss: 2.8441213505423586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Avg Loss: 1.7493: 100%|████████████████████████████████| 6440/6440 [05:54<00:00, 18.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Average Loss: 1.7493475869645059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Avg Loss: 1.1250: 100%|████████████████████████████████| 6440/6440 [05:53<00:00, 18.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Average Loss: 1.1250453975028503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Avg Loss: 0.7636: 100%|████████████████████████████████| 6440/6440 [05:54<00:00, 18.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Average Loss: 0.7636089072001647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Avg Loss: 0.5407: 100%|████████████████████████████████| 6440/6440 [05:55<00:00, 18.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Average Loss: 0.5406938915331819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Avg Loss: 0.4007: 100%|████████████████████████████████| 6440/6440 [05:53<00:00, 18.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Average Loss: 0.40066774682899625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Avg Loss: 0.3166: 100%|████████████████████████████████| 6440/6440 [05:57<00:00, 18.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Average Loss: 0.31661730327741794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Avg Loss: 0.2644: 100%|████████████████████████████████| 6440/6440 [05:58<00:00, 17.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Average Loss: 0.2644009937318215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Avg Loss: 0.2237:  36%|███████████▍                    | 2295/6440 [02:05<03:46, 18.26it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 26\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     29\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/pcdet/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pcdet/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pcdet/lib/python3.8/site-packages/transformers/optimization.py:486\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    484\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta1))\n\u001b[1;32m    485\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m--> 486\u001b[0m denom \u001b[38;5;241m=\u001b[39m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m step_size \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrect_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m]:  \u001b[38;5;66;03m# No bias correction for Bert\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 모델 로드\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "model.to(device) # 모델을 GPU단으로 이동\n",
    "\n",
    "# 모델 학습 하이퍼파라미터(Hyperparameter) 세팅\n",
    "# 실제 필요에 따라 조정하세요.\n",
    "CFG = {\n",
    "    'LR' : 2e-5, # Learning Rate\n",
    "    'EPOCHS' : 10, # 학습 Epoch\n",
    "}\n",
    "\n",
    "# 모델 학습 설정\n",
    "optimizer = AdamW(model.parameters(), lr=CFG['LR'])\n",
    "model.train()\n",
    "\n",
    "# 모델 학습\n",
    "for epoch in range(CFG['EPOCHS']):\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(enumerate(formatted_data), total=len(formatted_data))\n",
    "    for batch_idx, batch in progress_bar:\n",
    "        # 데이터를 GPU단으로 이동\n",
    "        batch = batch.to(device)\n",
    "        outputs = model(batch, labels=batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 진행률 표시줄에 평균 손실 업데이트\n",
    "        progress_bar.set_description(f\"Epoch {epoch+1} - Avg Loss: {total_loss / (batch_idx+1):.4f}\")\n",
    "\n",
    "    # 에폭의 평균 손실을 출력\n",
    "    print(f\"Epoch {epoch+1}/{CFG['EPOCHS']}, Average Loss: {total_loss / len(formatted_data)}\")\n",
    "\n",
    "# 모델 저장\n",
    "model.save_pretrained(\"./hansoldeco-kogpt2\")\n",
    "tokenizer.save_pretrained(\"./hansoldeco-kogpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634c1b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 Fine-tuned 모델과 토크나이저 불러오기\n",
    "model_dir = \"./hansoldeco-kogpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_dir)\n",
    "model.to(device)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_dir)\n",
    "\n",
    "# Inference를 위한 test.csv 파일 로드\n",
    "test = pd.read_csv('./test.csv')\n",
    "\n",
    "# test.csv의 '질문'에 대한 '답변'을 저장할 리스트\n",
    "preds = []\n",
    "\n",
    "# '질문' 컬럼의 각 질문에 대해 답변 생성\n",
    "for test_question in tqdm(test['질문']):\n",
    "    # 입력 텍스트를 토큰화하고 모델 입력 형태로 변환\n",
    "    input_ids = tokenizer.encode(test_question + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # 답변 생성\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=input_ids.to(device),\n",
    "        max_length=300,\n",
    "        temperature=0.9,\n",
    "        top_k=1,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.2,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "\n",
    "    # 생성된 텍스트(답변) 저장\n",
    "    for generated_sequence in output_sequences:\n",
    "        full_text = tokenizer.decode(generated_sequence, skip_special_tokens=False)\n",
    "        # 질문과 답변의 사이를 나타내는 eos_token (</s>)를 찾아, 이후부터 출력\n",
    "        answer_start = full_text.find(tokenizer.eos_token) + len(tokenizer.eos_token)\n",
    "        answer_only = full_text[answer_start:].strip()\n",
    "        answer_only = answer_only.replace('\\n', ' ')\n",
    "        preds.append(answer_only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4453e1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 데이터셋의 모든 질의에 대한 답변으로부터 512 차원의 Embedding Vector 추출\n",
    "# 평가를 위한 Embedding Vector 추출에 활용하는 모델은 'distiluse-base-multilingual-cased-v1' 이므로 반드시 확인해주세요.\n",
    "from sentence_transformers import SentenceTransformer # SentenceTransformer Version 2.2.2\n",
    "\n",
    "# Embedding Vector 추출에 활용할 모델(distiluse-base-multilingual-cased-v1) 불러오기\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
    "\n",
    "# 생성한 모든 응답(답변)으로부터 Embedding Vector 추출\n",
    "pred_embeddings = model.encode(preds)\n",
    "pred_embeddings.shape\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "# 제출 양식 파일(sample_submission.csv)을 활용하여 Embedding Vector로 변환한 결과를 삽입\n",
    "submit.iloc[:,1:] = pred_embeddings\n",
    "submit.head()\n",
    "# 리더보드 제출을 위한 csv파일 생성\n",
    "submit.to_csv('./baseline_submit.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07a603e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
