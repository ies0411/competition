{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40b23663-6d1f-4ab4-afab-db77038cb2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/haven-jeon/PyKoSpacing.git\n",
    "# !pip install git+https://github.com/ssut/py-hanspell.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35d516bb-b5e0-43b4-9cfb-0091798caf0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install accelerate\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0199ae5e-4f53-4b06-813a-a5ca68e61b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-11 04:22:42.536300: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-11 04:22:43.450375: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-02-11 04:22:43.450474: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-02-11 04:22:43.450485: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.8/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast, AdamW, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from transformers import Trainer\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as tr # 이미지 전처리 기능들을 제공하는 라이브러리\n",
    "from torch.utils.data import DataLoader, Dataset # 데이터를 모델에 사용할 수 있도록 정리해 주는 라이브러리\n",
    "\n",
    "# CUDA 사용 가능 여부 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "478167df-5cc7-45f0-88f5-466112964dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'LR' : 1e-5, # Learning Rate\n",
    "    'EPOCHS' : 50, # 학습 Epoch\n",
    "    'BATCH_SIZE' : 1,\n",
    "    'AUG_RATIO' : 0.15,\n",
    "    'AUG_PROB' : 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82c5270e-167f-4baf-9b5f-07349f83c9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('./data/train.csv')\n",
    "# test_data = pd.read_csv(\"./data/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a510b1fb-3edf-49e9-afd1-ff003e10119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BERT_Augmentation():\n",
    "    def __init__(self):\n",
    "        self.model_name = 'monologg/koelectra-base-v3-generator'\n",
    "        self.model = transformers.AutoModelForMaskedLM.from_pretrained(self.model_name).to(device)\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.unmasker = transformers.pipeline(\"fill-mask\", model=self.model, tokenizer=self.tokenizer)\n",
    "        random.seed(42)\n",
    "    def random_masking_replacement(self, sentence, ratio=0.15):\n",
    "        \"\"\"Masking random eojeol of the sentence, and recover them using PLM.\n",
    "\n",
    "        Args:\n",
    "            sentence (str): Source sentence\n",
    "            ratio (int): Ratio of masking\n",
    "\n",
    "        Returns:\n",
    "          str: Recovered sentence\n",
    "        \"\"\"\n",
    "        \n",
    "        span = int(round(len(sentence.split()) * ratio))\n",
    "        \n",
    "        # 품질 유지를 위해, 문장의 어절 수가 4 이하라면 원문장을 그대로 리턴합니다.\n",
    "        if len(sentence.split()) <= 4:\n",
    "            return sentence\n",
    "\n",
    "        mask = self.tokenizer.mask_token\n",
    "        unmasker = self.unmasker\n",
    "\n",
    "        unmask_sentence = sentence\n",
    "        # 처음과 끝 부분을 [MASK]로 치환 후 추론할 때의 품질이 좋지 않음.\n",
    "        random_idx = random.randint(1, len(unmask_sentence.split()) - span)\n",
    "        \n",
    "        unmask_sentence = unmask_sentence.split()\n",
    "        # del unmask_sentence[random_idx:random_idx+span]\n",
    "        cache = []\n",
    "        for _ in range(span):\n",
    "            # 처음과 끝 부분을 [MASK]로 치환 후 추론할 때의 품질이 좋지 않음.\n",
    "            while cache and random_idx in cache:\n",
    "                random_idx = random.randint(1, len(unmask_sentence) - 2)\n",
    "            cache.append(random_idx)\n",
    "            unmask_sentence[random_idx] = mask\n",
    "            unmask_sentence = unmasker(\" \".join(unmask_sentence))[0]['sequence']\n",
    "            unmask_sentence = unmask_sentence.split()\n",
    "        unmask_sentence = \" \".join(unmask_sentence)\n",
    "        unmask_sentence = unmask_sentence.replace(\"  \", \" \")\n",
    "\n",
    "        return unmask_sentence.strip()\n",
    "\n",
    "    def random_masking_insertion(self, sentence, ratio=0.15):\n",
    "        \n",
    "        span = int(round(len(sentence.split()) * ratio))\n",
    "        mask = self.tokenizer.mask_token\n",
    "        unmasker = self.unmasker\n",
    "        \n",
    "        # Recover\n",
    "        unmask_sentence = sentence\n",
    "        \n",
    "        for _ in range(span):\n",
    "            unmask_sentence = unmask_sentence.split()\n",
    "            random_idx = random.randint(0, len(unmask_sentence)-1)\n",
    "            unmask_sentence.insert(random_idx, mask)\n",
    "            unmask_sentence = unmasker(\" \".join(unmask_sentence))[0]['sequence']\n",
    "\n",
    "        unmask_sentence = unmask_sentence.replace(\"  \", \" \")\n",
    "\n",
    "        return unmask_sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9e28afa-63b8-4ccc-8ea7-aebea5707b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# BERT_aug = BERT_Augmentation()\n",
    "\n",
    "# RATIO=0.15\n",
    "# for _, row in tqdm(data_df.iterrows()):\n",
    "#     for q_col in ['질문_1', '질문_2']:\n",
    "#         # for a_col in ['답변_1', '답변_2', '답변_3', '답변_4', '답변_5']:\n",
    "#             # 질문과 답변 쌍을 </s> token으로 연결\n",
    "#         print(row[q_col])\n",
    "#         random_masking_replacement = BERT_aug.random_masking_replacement\n",
    "#         random_masking_insertion = BERT_aug.random_masking_insertion\n",
    "#         print(random_masking_replacement(row[q_col], CFG['AUG_RATIO']))\n",
    "#         print(random_masking_insertion(row[q_col], CFG['AUG_RATIO']))\n",
    "#         print(\"==\")\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b837c2a5-2989-4b4b-9615-c784b6887a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Generate a random boolean with 50% probability\n",
    "# result = random.choice([True, False])\n",
    "class HansolDataset(Dataset):\n",
    "    def __init__(self, data_pd, tokenizer, ratio = CFG['AUG_RATIO']):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data_pd = data_pd\n",
    "        self.formatted_data = []\n",
    "        self.sentence_list = []\n",
    "        self.ratio = ratio\n",
    "        self._generate_data(data_pd)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            sentence = self.sentence_list[index]\n",
    "            question = self._augment_question(sentence[\"q\"]) if random.random() < CFG['AUG_PROB'] else sentence[\"q\"]\n",
    "            answer = self._augment_question(sentence[\"a\"]) if random.random() < CFG['AUG_PROB'] else sentence[\"a\"]\n",
    "            input_text = question + tokenizer.eos_token + answer + tokenizer.eos_token\n",
    "            input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "        except:\n",
    "            return self.__getitem__(random.randint(0, len(self.sentence_list) - 1))\n",
    "        return input_ids\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.sentence_list)\n",
    "\n",
    "    def _augment_question(self, origin_question):\n",
    "        BERT_aug = BERT_Augmentation()\n",
    "        random_masking_replacement = BERT_aug.random_masking_replacement\n",
    "        random_masking_insertion = BERT_aug.random_masking_insertion\n",
    "        return random_masking_replacement(origin_question, self.ratio) if random.choice([True, False]) else random_masking_insertion(origin_question, self.ratio)\n",
    "\n",
    "    def _generate_data(self, data):\n",
    "        for _, row in tqdm(data.iterrows()):\n",
    "            for q_col in ['질문_1', '질문_2']:\n",
    "                for a_col in ['답변_1', '답변_2', '답변_3', '답변_4', '답변_5']:\n",
    "                    sentence_dict = {}\n",
    "                    sentence_dict[\"q\"]=row[q_col]\n",
    "                    sentence_dict[\"a\"]=row[a_col]\n",
    "                    self.sentence_list.append(sentence_dict)\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfc52484-7bc9-4170-adef-a6c56225cb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained('skt/kogpt2-base-v2', eos_token='</s>')\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('./skt_5_2_epoch/', eos_token='</s>')\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./skt_5_2_epoch/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef0db07c-0291-44d8-bfad-ef5deeabb3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "644it [00:00, 8780.14it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = HansolDataset(data_pd = data_df,tokenizer=tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True) # 미니 배치 형태로 데이터 갖추기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9c0b384-2b7a-4d96-baa3-a3b5421758c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, module in model.named_children():\n",
    "#     if name == \"gpt_neox\":\n",
    "#         for name, sub_module in module.named_children():\n",
    "#             # print(name)\n",
    "#             if name == \"embed_in\":\n",
    "#                 for param in sub_module.parameters():\n",
    "#                     param.requires_grad = False\n",
    "#             elif name == \"emb_dropout\":\n",
    "#                 for param in sub_module.parameters():\n",
    "#                     param.requires_grad = False\n",
    "#             elif name == \"layers\":\n",
    "#                 # print(name)\n",
    "#                  for name, layer_module in sub_module.named_children():\n",
    "#                      if int(name) < 26:\n",
    "#                         for param in layer_module.parameters():\n",
    "#                             param.requires_grad = False\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72cd1c3f-f27c-43a0-bd9e-c216ff8a51c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1 - Avg Loss: 0.6650:  33%|████████████████████████████████████████████▉                                                                                          | 2141/6440 [1:08:33<4:17:48,  3.60s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (605 > 512). Running this sequence through the model will result in indexing errors\n",
      "Epoch 1 - Avg Loss: 0.6276:  55%|██████████████████████████████████████████████████████████████████████████▊                                                            | 3568/6440 [2:01:47<1:21:32,  1.70s/it]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 2 - Avg Loss: 0.3856:  51%|████████████████████████████████████████████████████████████████████▉                                                                  | 3288/6440 [2:11:58<2:04:32,  2.37s/it]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 5 - Avg Loss: 0.2057:  20%|███████████████████████████                                                                                                           | 1298/6440 [9:57:15<39:26:02, 27.61s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">23</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">20 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>optimizer.step()                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">21 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>optimizer.zero_grad()                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">22 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>23 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>total_loss += loss.item()                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">24 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">25 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># 진행률 표시줄에 평균 손실 업데이트</span>                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">26 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>progress_bar.set_description(<span style=\"color: #808000; text-decoration-color: #808000\">f\"Epoch {</span>epoch+<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span><span style=\"color: #808000; text-decoration-color: #808000\">} - Avg Loss: {</span>total_loss<span style=\"color: #808080; text-decoration-color: #808080\"> </span>/<span style=\"color: #808080; text-decoration-color: #808080\"> </span>(batch_    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m23\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m20 \u001b[0m\u001b[2m│   │   \u001b[0moptimizer.step()                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[2m│   │   \u001b[0moptimizer.zero_grad()                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m22 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m23 \u001b[2m│   │   \u001b[0mtotal_loss += loss.item()                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m24 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m25 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# 진행률 표시줄에 평균 손실 업데이트\u001b[0m                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m26 \u001b[0m\u001b[2m│   │   \u001b[0mprogress_bar.set_description(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mEpoch \u001b[0m\u001b[33m{\u001b[0mepoch+\u001b[94m1\u001b[0m\u001b[33m}\u001b[0m\u001b[33m - Avg Loss: \u001b[0m\u001b[33m{\u001b[0mtotal_loss\u001b[90m \u001b[0m/\u001b[90m \u001b[0m(batch_    \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model.to(device) # 모델을 GPU단으로 이동\n",
    "\n",
    "# 모델 학습 설정\n",
    "optimizer = AdamW(model.parameters(), lr=CFG['LR'])\n",
    "model.train()\n",
    "\n",
    "# 모델 학습\n",
    "for epoch in range(CFG['EPOCHS']):\n",
    "    total_loss = 0\n",
    "    # for batch in tqdm(dataloader, desc=\"Processing batches\", unit=\"batch\"):\n",
    "\n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for batch_idx, batch in progress_bar:\n",
    "        # 데이터를 GPU단으로 이동\n",
    "        batch = batch.to(device)\n",
    "        outputs = model(batch, labels=batch)\n",
    "        loss = outputs.loss\n",
    "        # loss_2 = LossFunction(outputs,batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 진행률 표시줄에 평균 손실 업데이트\n",
    "        progress_bar.set_description(f\"Epoch {epoch+1} - Avg Loss: {total_loss / (batch_idx+1):.4f}\")\n",
    "\n",
    "    # 에폭의 평균 손실을 출력\n",
    "    print(f\"Epoch {epoch+1}/{CFG['EPOCHS']}, Average Loss: {total_loss / len(dataloader)}\")\n",
    "    model.save_pretrained(f\"./skt_augment_{epoch}_epoch\")\n",
    "    tokenizer.save_pretrained(f\"./skt_augment_{epoch}_epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1370575-3554-4b60-88a3-1555a01c401b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdda8b13-3206-430c-8d08-eaa2e52c210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ㅣ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69c2c6e-8f27-4d97-89f8-6bee72424a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
